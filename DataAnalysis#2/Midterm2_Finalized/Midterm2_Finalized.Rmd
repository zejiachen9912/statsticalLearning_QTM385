
---
title: "Midterm Data Exercise #2 - Classification"
author: "Harry Chen, Francis Lin, Mike Lin"
date: "March 28th, 2022"
output:
  prettydoc::html_pretty:
    df_print: kable
    theme: leonids
    highlight: github
    toc: no
    toc_depth: 2
    toc_float:
      collapsed: no
urlcolor: blue
---


```{r, include=FALSE}
library(ggplot2)
library(data.table)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
```

```{r, message=F}
# loading the data
rm(list=ls(all=TRUE))
library(data.table)
train_x <- fread("MNISTTrainX.csv")
train_y <- fread("MNISTTrainY.csv")
validate_x <- fread("MNISTValidationX.csv")
validate_y <- fread("MNISTValidationY.csv")
test_x_rand <- fread("MNISTTestXRand.csv")
test_y_rand <- fread("MNISTTestYRand.csv")
```

A review of classification methods that we've covered in this course:

1. Discriminative Methods
  + Logistic Regression
  + Multinomial Logistic Regression
  + Shrinkage Approaches to Logistic Regression (LASSO and Ridge)
  + Generalized Additive Logistic Regression
    
2. Generative Methods
  + Bayes' Theorem for Discrete Features
  + Linear and Quadratic Disciminant Analysis
  + Naive Bayes classifiers (with a mixture of discrete and continuous margins $\pm$ kernel density estimates)
    
3. Geometric Methods
  + Support Vector Classifiers
  + Support Vector Machines with Kernelized Features (Polynomial and RBF)
  
4. Flexible Classifiers
  + KNN and DANN
  + Classification Trees (Bagged Trees, Random Forests, Probability Forests)
  + Boosted Trees (AdaBoost via GBM, XGBoost)
  + Stacked Classifiers
  
***

## Data Set Overview

This assignment revolves around a standard in the classification literature - the MNIST handwritten digits database.  The MNIST data set is a collection of 70,000 handwritten digits taken from handwritten zip codes on letters sent via the USPS.  A single instance (or observation) is a $28 \times 28$ pixel image of a handwritten digit.  Each of the 784 features associated with each image is a **color value** associated with a single pixel.  Color has been coded on a grayscale with 0 corresponding to white (a "paper" pixel), 256 corresponding to black (a "pencil" pixel), and all values in between corresponding to varying levels of gray in accordance with the direction of the grayscale.  In reality, all pixels are either paper or pencil, but various aliasing algorithms and rounding algorithms needed to handle pixels that take on both paper and pencil values lead some pixels to fall at a gray level.

To make analysis possible, the MNIST data set processes the images to ensure that the images are centered - the center of pencil mass is shifted to be around pixel 14,14 - and *deskewed* - the pencil pixels are shiften to be as vertical as possible.  The original deskewing algorithm used for the MNIST data set was not very effective, though, so many images are still skewed in one direction or another.

Each image is associated with a label of 0 through 9 coded by a series of human coders.  There has been significant quality control on this data set that ensures that all of the labels are correct!

MNIST is typically used to benchmark classification algorithms.  The data set is of good size for many state-of-the-art classification methods and presents a somewhat difficult classification problem with 10 categories.  The 10 class problem has been applied to many different algorithms to varying levels of success.  The [original website for MNIST from Yann LeCun, Corinna Cortes, and Christopher J.C. Burges](http://yann.lecun.com/exdb/mnist/) contains error rates on a common test set of 10,000 instances for a number of different classification approaches.  For example, over the 10 class problem using multinomial logistic regression (annoyingly referred to as a 1-layer neural network) has a 12% error rate on the test set.  On the other hand, a committee of 35 convolutional neural nets was able to misclassify only 23 of the test digits!

We'll be using a modified version of this data set to run a series of classification models for different classification tasks.  Your final task will be to build a classification model for the full 10 class problem!

## Modifying the MNIST Data Set

The original data set contains 70,000 instances of labeled $28 \times 28$ pixels.  Since we have neither the computational power nor the time to deal with data of this magnitude, I've made some convenience alterations to the data set.  Here, I'll outline exactly what I did.

First, to reduce the number of pixels for each image, I reduced the resolution of the images from 784 pixels to 196 pixels.  To do this, I created 196 $2 \times 2$ pixel groups and averaged the color values across the four included pixels.  As seen in the figure below, this drastically reduced the dimensionality of the predictor space at the cost of a little less clarity in the resulting pixel images.

Second, to further reduce the number of predictors, I removed full rows and columns of pixels that are part of the paper bounding box (e.g. all paper pixels) in more than 99% of images.  This resulted in removing any pixel in columns 1 and 14 and in rows 1 and 14.  This reduced the number of pixels in each image to 144.

Finally, I added just a little random noise to any paper pixels (pixels with colors values less than 5) to inject a little variance into any mostly paper coumns of pixels.  This will matter very little for classification accuracy but will allow methods that require inversion of a covariance matrix (logistic regression, QDA, etc.) to run without any additional processing needed.  These final two steps have little effect on the images compared to the $14 \times 14$ pixel images.

## Working with the MNIST Data

Using the $12 \times 12$ images, I've created six data sets for you to work with:

  1. `MNISTTrainX.csv` and `MNISTTrainY.csv` which contain 25,000 images or image labels.  For each of the ten digits, there are 2,500 observations.  I've split the images and the labels into separate files to make plotting the digit images as easy as possible.
  
  2. `MNISTValidationX.csv` and `MNISTValidationY.csv` which contain 15,000 images or image labels.  For each of the ten digits, there are 1,500 observations.  These data sets can be used to measure out of sample predictive accuracy for the classification methods.
  
  3. `MNISTTestXRand.csv` and `MNISTTestYRand.csv` which contain 10,000 images.  There are no labels for this data set.  These 10,000 observations (all ten handwritten digits are represented at least 200 times in this test set) will act as a hidden test set that will be used to measure the accuracy of your approaches on my held out test set. `MNISTTestYRand.csv` is an "empty" .csv (all the labels are zeros) that includes the image row corresponding to the test set and a column that can be used to store your final digit prediction for the corresponding row in `MNISTTestX.csv`.  Your final deliverable will include three different sets of predictions for this data set.
  
Each of the data sets that include pixel values are organized in a consistent way.  Each image/row is associated with 144 predictors - each of the 144 pixels in the image.  The pixels are *vectorized* from their matrix form by row - the first feature is row 1 column 1, the second is row 1 column 2, the 12th feature is row 1 column 12, the 13th feature is row 2 column 1, the 25th features is row 3 column 1, etc.  I've provided feature names in each data set to assist in this interpretation.

The reason that this organization is so important is that you will frequently want to convert between the vector of pixels and the actual 2D image (a matrix).  We can easily convert the row of 144 pixels to a 12 by 12 matrix by passing the vector to a matrix **by row**.  Let $x$ be a vector, then this can be achieved easily in R by using `matrix(nrow = 12, ncol = 12, x, byrow = TRUE)`.  This logic can be applied in any scenario where we receive a vector of quantities related to each pixel (coefficients from logistic regression, for example).

Since it is difficult to look at the numbers and know exactly what we're working with, we need a consistent plotting method.  The easiest way to plot the MNIST data is to use the `image()` function in R.  For your convenience, you can use the `plot_digit()` function below:

```{r, echo=TRUE, fig.align = 'center', eval = TRUE}
plot_digit <- function(x, bw = FALSE,...){
  if(sqrt(length(x)) != round(sqrt(length(x)))){
    stop(print("Not a square image! Something is wrong here."))
  }
  n <- sqrt(length(x))
  if(bw == TRUE){
    x <- as.numeric(x > 50)*256
  }
  par(pty = "s")
  image(matrix(as.matrix(x), nrow = n)[,n:1], col = gray(12:1 / 12), ...)
}
```

`plot_digit()` takes two argument: 1. `x` - a vector of squared integer length that is organized in row-column order (like our MNIST data) and 2. `bw` - a logical (TRUE/FALSE) that tells the function to plot the full grayscale **or** approximately round each pixel to either be white (paper) or black (pencil).  `plot_digit` also accepts any arguments that are applicable to the base plot function in R: `main` to add a plot title, `xlab` or `ylab` to add x and y axis labels, etc.  `image` in R is a legacy function from SPlus that has a quirk that it plots columns in reverse order.  Be careful if using `image` to create your own figures!  You can change the color scheme for `image` using different color range methods.  Look online to find instructions for this.  There are other methods of plotting the MNIST data in R and Python that can be easily found via a quick Google search.

Warning!  A common mistake will be to pass a vector of length 145 (pixels + label) to this function.  If you get an error, check and make sure that you aren't passing the label to the function!

***

## Question 1 (15 pts.)

Let's start by working with the training data to gain some comfort working with the MNIST data.

### Part 1 (5 pts.)

Create a plot that shows 9 random images in the training data.  Label each image with its corresponding label.  Do the images match the labels?

```{r}
sample_9 <- sample(1:25000, 9)
par(mfrow = c(3,3))
for (i in sample_9){
  label = train_y[i, 1]
  plot_digit(train_x[i,], bw = FALSE, main = paste("label: ", label))
}
par(mfrow = c(1,1))
```


### Part 2 (5 pts.)

For each digit (0 - 9), compute the average within class pixel value for each of the 144 pixels across the training data.  Create a plot that shows the 10 average digits.  Which class seems to show the most within class variation over images?  Which class seems to the least within class variation over images?

```{r}
meanPixel <- data.frame(matrix(nrow=10,ncol=144))
for (i in 0:10){
  index <- which(train_y$label == i)
  subsets <- as.matrix(train_x[index,])
  for (j in 1:144){
    meanPixel[i + 1,j] = mean(subsets[,j])
  }
}
head(meanPixel[,1:10])
```

```{r}
par(mfrow = c(3,2))
for (i in 1:10){
  plot_digit(meanPixel[i,], bw = FALSE, main = paste("digit:", i - 1))
}
par(mfrow = c(1,2))
```

Judging the graph, **0**, **5**, **2** seems to have less within class variation over the image.It is likely that our model will perform well in predicting these digits.

On the other hand, digits like **1**, **4**, **6**,**8**, **9** display a relatively large within class variation over the image, meaning that it can be hard for the model to classify these digits.

### Part 3 (5 pts.)

Using the average images, come up with a measure of similarity or dissimilarity between different digits.  It doesn't need to be particularly elegant. 

Which pairs of digits will be easiest to tell apart?  Which pairs will be most difficult?  Thinking about how people write different digits, does your similarity measure make sense?

### Rules

One measure of that we can leverage to classify different digit can be measure coordinate (location) that appear to be the darkest for a given graph. The reasoning behind is that different digits has different emphasis (where we dedicate most the ink to) on strokes.

Hence, if a large amount of samples all shares similar darkest coordinate, it is highly likely that these samples belong to the same group.

## Question 2 (15 pts.)

Let's start with one of the digit pairs that is easiest to tell apart: 0s and 1s.  For this question, we'll build a classifier to discriminate between images of 0s and images of 1s.  Start by subsetting your training and validation data sets to only include 0s and 1s (a literal manifestation of the classification problem).

### Part 1 (8 pts.)

For reasons you will soon see, we'll only consider one classification approaches for this problem - logistic regression.  Using your training data, train a logistic regression classifier for the literal 0/1 problem.  Compute a 10-fold CV measure of expected prediction accuracy using the training data. Similarly, compute the accuracy of your trained logistic regression model on the validation set.  What do you find here?

Intuitively, explain this result.  Think carefully about the how logistic regression is approaching this classification problem.

Plot up to 4 images in the validation set that are misclassified with respect to the Bayes' classifier.  Does this misclassification make sense?  What about these images leads the classifier to incorrectly guess the proper label?

Note: the underlying algorithm for logistic regression proceeds iteratively attempting to minimize the logistic regression loss function.  Because many of the predictors in this problem have variance close to zero, the default number of iterations (typically 25) may not be enough for the algorithm to converge.  You can deal with this issue by setting the maximum number of IRLS iterations to a larger value - 100 should be sufficient.  In R, this can be achieved within your `glm` call by adding an additional control argument - `control = list(maxit = 100)`.

### Solution

```{r}
index_0_1 <- append(which(train_y$label == 0), which(train_y$label == 1))
train_x_0_1 <- train_x[index_0_1,]
train_y_0_1 <- train_y[index_0_1,]
train_xy_0_1 <- cbind(train_x_0_1, train_y_0_1)
train_xy_0_1$label <- as.factor(train_xy_0_1$label)
index_0_1 <- append(which(validate_y$label == 0), which(validate_y$label == 1))
test_x_0_1 <- validate_x[index_0_1,]
test_y_0_1 <- validate_y[index_0_1,]
```

**Logistic Regression**

training error
```{r warning=FALSE}
log.fit<-glm(label~.,data=train_xy_0_1,family=binomial, control = list(maxit = 100))
## Accuracy
pred <- predict(log.fit, train_x_0_1, type = "response")
confmatrix <- table(observed = train_y_0_1$label, predict = pred > 0.5)
confmatrix
print(paste("misclassification rate: ", (confmatrix[1,2] + confmatrix[2,1]) / sum(confmatrix)))
```

10-fold cv
```{r warning=FALSE}
misclass_cv_err <- c()
#Randomly shuffle the data
set.seed(123)
rand_df<-data.frame(train_xy_0_1[sample(nrow(train_xy_0_1)),])
#Create n folds equally size folds
n_folds <- cut(seq(1,nrow(rand_df)),breaks=10,labels=FALSE)
# perform the k-fold cross validation
for (i in 1:10){
  #Segment the data by fold using the which() function 
  index <- which(n_folds == i, arr.ind = T)
  test_data <- rand_df[index,]
  train_data <- rand_df[-index,]
  y_name = names(rand_df)[145]
  x_name = names(rand_df)[-145]
  test_x <- test_data[, x_name]
  test_y <- test_data[, y_name]
  # construct the model and predict the outcome
  log.fit<-glm(label~.,data=train_data,family=binomial, control = list(maxit = 100))
  pred_cv = predict(log.fit, newdata=test_x, type = "response")
  # get the misclassfied error
  confmatrix <- table(observed = test_y, predict = pred_cv > 0.5)
  misclass_cv_err[i] <- (confmatrix[1,2] + confmatrix[2,1]) / sum(confmatrix)
}
misclass_cv_err
paste("Misclassified rate w/ 10 fold on training data: ", round(mean(misclass_cv_err), 5))
```

10-fold CV using CARET build-in function
```{r warning=FALSE}
library(caret)
ctrlspecs <- trainControl(method = 'cv', number = 10, savePredictions = "all", classProbs = F)
set.seed(123)
model1 <- train(label~., data = train_xy_0_1, method = "glm", family = binomial, trControl = ctrlspecs)
model1
```

Test Error
```{r}
pred <- predict(log.fit, test_x_0_1, type = "response")
confmatrix <- table(observed = test_y_0_1$label, predict = pred > 0.5)
confmatrix
print(paste("misclassification rate w/ test set: ", round((confmatrix[1,2] + confmatrix[2,1]) / sum(confmatrix), 7)))
```

As we observe in the output, logistic regression did a great job in classifying digit $1$ and digit $2$.And the prediction loss error is pretty consistent across the board.

One reasoning behind can be the fact that logistic regression works well when the classification decision boundary is linearly separable and the cut is distinct. What logistic regression does is essentially calculating the probability of the digit appears to be $1$ / $0$ given a set of pixel point's locations in a linear combinated manner. Since $1$ & $0$ are pretty distinguishable in a sense that there are not many overlapping spaces when writing these two digits, it is reasonable to see such great performance coming from the model.

Hence, in most of cases, the logistic model is able to classify with confidence, and rarely gets to a point where it needs to make a decision hesitantly ($P=0.5$.) 

Plots for misclassified number
```{r}
validateSet <- cbind(test_y_0_1, pred)
wrongLabel_index <- which((validateSet$label == 1 & validateSet$pred < 0.5) | (validateSet$label == 0 & validateSet$pred > 0.5))
par(mfrow = c(2,2))
for (i in 1:4){
  index = wrongLabel_index[i]
  plot_digit(test_x_0_1[wrongLabel_index[i],], bw = FALSE, 
             main = paste("Predict label:", round(validateSet[wrongLabel_index[i], 2], 5),
                          "; Actual label:", validateSet[wrongLabel_index[i], 1]))
}
```

I don't blame the model for these misclassifications. If I was presented with these numbers, it is highly likely that I will be having a hard time classify them.

These digits are written in a irregular manner that it is hard for the model to classify them solely based on the pixel location. All of them are tilted in some ways that make the pencil pixel to appear in locations that is uncommon for classifying the actual label.

### Part 2 (7 pts.)

Let's try to understand exactly how this classifier is working.  For the training data, compute a logistic regression classifier with the LASSO penalty on the coefficients.  Use K-fold CV to find a value of $\lambda$ that sparsely represents the set of coefficient and viably minimizes the expected misclassification rate for out of sample data.

Using the coefficients associated with your chosen value of $\lambda$, create a plot that shows the relationship between the pixels and the coefficients.  Positive coefficients are associated with pixels where a pencil pixel in that location (value > 0) **increases** the predicted probability that the image is a 1 while negative coefficients are associated with pixels where a pencil pixel in that location (value > 0) **decreases** the predicted probability that the image is a 1.

Leveraging this plot, come up with a set of rules related to the location of pencil pixels that a human who had the pixel mappings could evaluate to determine if an image is a zero or one.  The rules don't need to exactly relate to specific pixels.  Rather, they can relate to relative location of the pixels (center vs. noncenter, for example).

This task is, more or less, an example of **computer vision** - trying to use classification methods to make computer view images in the same way as humans.

### Solution

```{r, message=FALSE}
library(glmnet)
library(tidyverse)
```

```{r}
set.seed(123)
# Dumy code categorical predictor variables
train_x <- model.matrix(label~., train_xy_0_1)[,-1]
# Convert the outcome (class) to a numerical variable
train_y <- as.matrix(train_y_0_1$label)
cv.lasso <- cv.glmnet(train_x, train_y, alpha = 1, family = "binomial")
plot(cv.lasso)
# lambda.min
print(cv.lasso$lambda.min)
```

```{r}
coef_optim_LASSO <- coef(cv.lasso, s= cv.lasso$lambda.min)
coef_df <- as.data.frame(summary(coef_optim_LASSO))
col_names_df <- rbind(c("Intercept", 0),data.frame(colnames(train_x_0_1)))
# saving all leftover significant predictors
coef_shrink <- col_names_df[c(coef_df$i),1]
coef_df <- cbind(coef_shrink, coef_df)
coef_df <- coef_df[,-c(2,3)]
colnames(coef_df)[1] <- "predictors"
colnames(coef_df)[2] <- "coef"
coef_df$coef <- round(coef_df$coef, 7)
```

Positive Coefficient
```{r}
coef_df %>% arrange(desc(coef)) %>% top_n(10)
```

Negative Coefficient
```{r}
coef_df %>% arrange(coef) %>% top_n(-10)
```

Plot
```{r}
row <- seq(1, 12)
col <- seq(1, 12)
grid_df <- expand.grid(Col = col, Row = row)
grid_df[, 'coef'] <- 0
index <- as.data.frame(summary(coef_optim_LASSO))[, c('i', 'x')]
index <- index[-1,]
for (i in index$i){
  grid_df[i, 3] = index[which(index$i == i), 2]
}
grid_df$coef_sign <- factor(grid_df$coef, levels = c("positive", "0", "negative"))
grid_df$coef_sign[grid_df$coef > 0]<-"positive"
grid_df$coef_sign[grid_df$coef == 0]<-"0"
grid_df$coef_sign[grid_df$coef < 0]<-"negative"
seq <- seq(1, 20)
library(ggplot2)
gg <- ggplot(grid_df, aes(x=Row, y=Col)) + 
  geom_point(aes(col = coef_sign, size = coef)) + 
  labs( 
       y="Col", 
       x="Row", 
       title="Relationship between the pixels and the coefficients") + 
  scale_x_continuous("Row", labels = as.character(seq), breaks = seq) +
  scale_y_continuous("Col", labels = as.character(seq), breaks = seq)
plot(gg)
```

Judging the graph above, it is quite intuitive to understand how penalized logistic regression works in this scenarios.

How negative coefficients appear on the graph follows a seemingly circular pattern, which is exactly how **0** looks through human's eye. Hence, if pencil pixels follow a circular pattern at the center of a given image, we can probably say that the image is a $0$ 

On the other hand, positive coefficients often appears at the corner of the relationship graph. This phenomenon also make tons of sense since a lot of $1$s in the data set are actually drawn tilted rather than up right. Plus, there is unlikely that pencil pixels hit the corners when people are writing $0$, making it even more reasonable for the model to classify in such manner.
As a result, if pencil pixels appear at the center and corner of the graph, aligning diagonally, we can probably say that is a $1$.



## Question 3 (25 pts.)

Now, let's work with a more difficult pair - 4s and 9s.  Logically, these are going to be more difficult to distinguish!  Start by subsetting your training and validation sets to only include 4s and 9s.  There may be some situations where you need to recode these to zeros and ones!  When making predictions on the test set, be sure to recode the predictions back to 4s and 9s.

```{r, include=FALSE}
library(ggplot2)
library(data.table)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
```

### Part 1 (5 pts.)

Set up an array for storing 10-fold CV estimates for each model.
```{r}
misclass_rates = c()
```

Start by replicating your analysis for 0s and 1s using logistic regression for 4s and 9s.  Compute a 10-fold CV measure of expected prediction accuracy and find the misclassification rate for your validation set.  How does this compare to the performance of logistic regression for 0s and 1s?  Why do these results differ?

Using the same LASSO approach as above, create a plot that shows which pixels are important for the classification.  How does this image differ from the 0/1 case?

reloading the data
```{r}
train_x <- fread("MNISTTrainX.csv")
train_y <- fread("MNISTTrainY.csv")
validate_x <- fread("MNISTValidationX.csv")
validate_y <- fread("MNISTValidationY.csv")
test_x <- fread("MNISTTestXRand.csv")
test_y <- fread("MNISTTestYRand.csv")
```


```{r}
set.seed(123)
index_4_9 <- append(which(train_y$label == 4), which(train_y$label == 9))
train_x_4_9 <- train_x[index_4_9,]
train_y_4_9 <- train_y[index_4_9,]
train_xy_4_9 <- cbind(train_x_4_9, train_y_4_9)
train_xy_4_9$label <- as.factor(train_xy_4_9$label)
# validation set
index_4_9 <- append(which(validate_y$label == 4), which(validate_y$label == 9))
test_x_4_9 <- validate_x[index_4_9,]
test_y_4_9 <- validate_y[index_4_9,]
```

**Logistic Regression**

training error
```{r}
set.seed(123)
log.fit<-glm(label~.,data=train_xy_4_9,family=binomial, control = list(maxit = 100))
## Accuracy
pred <- predict(log.fit, train_x_4_9, type = "response")
confmatrix <- table(observed = train_y_4_9$label, predict = pred > 0.5)
confmatrix
print(paste("misclassification rate: ", (confmatrix[1,2] + confmatrix[2,1]) / sum(confmatrix)))
```

10-fold cv
```{r, warning=FALSE}
set.seed(123)
misclass_cv_err <- c()
#Randomly shuffle the data
set.seed(123)
rand_df<-data.frame(train_xy_4_9[sample(nrow(train_xy_4_9)),])
#Create n folds equally size folds
n_folds <- cut(seq(1,nrow(rand_df)),breaks=10,labels=FALSE)
# perform the k-fold cross validation
for (i in 1:10){
  #Segment the data by fold using the which() function 
  index <- which(n_folds == i, arr.ind = T)
  test_data <- rand_df[index,]
  train_data <- rand_df[-index,]
  y_name = names(rand_df)[145]
  x_name = names(rand_df)[-145]
  test_x <- test_data[, x_name]
  test_y <- test_data[, y_name]
  # construct the model and predict the outcome
  log.fit<-glm(label~.,data=train_data,family=binomial, control = list(maxit = 100))
  pred_cv = predict(log.fit, newdata=test_x, type = "response")
  # get the misclassfied error
  confmatrix <- table(observed = test_y, predict = pred_cv > 0.5)
  misclass_cv_err[i] <- (confmatrix[1,2] + confmatrix[2,1]) / sum(confmatrix)
}
misclass_cv_err
paste("Misclassified rate w/ 10 fold on training data: ", round(mean(misclass_cv_err), 5))
misclass_rates$logistic = round(mean(misclass_cv_err), 5)
print("10-fold CV accuracy:")
print(1-round(mean(misclass_cv_err), 5))
```

misclassification rate for validation set
```{r}
  log.fit<-glm(label~.,data=train_xy_4_9,family=binomial, control = list(maxit = 100))
  pred = predict(log.fit, newdata=test_x_4_9, type = "response")
  confmatrix <- table(observed = test_y_4_9$label, predict = pred > 0.5)
  (confmatrix[1,2] + confmatrix[2,1]) / sum(confmatrix)
  print("validation set accuracy:")
  print(1-(confmatrix[1,2] + confmatrix[2,1]) / sum(confmatrix))
```
The 10-fold CV estimate and validation set errors for 0-1 classification are 0.0026 and 0.0023, respectively. Logistic regression (LR) performs better on 0-1 classification. 

Here is my opinion on the LR performances on these two problems. The distinction between 0 and 1 is quite clear, while the distinction between 4 and 9 is sometimes unclear (some "9" look like "4"). Probably, 0 and 1 digits could be relatively easily separated by a linear decision boundary, while 4 and 9 have complex decision boundary. As LR draws linear boundary, we expect it to perform better on 0-1 classification.

**logistic regression with LASSO regularization**

```{r,echo=FALSE}
library(glmnet)
library(tidyverse)
```

```{r}
set.seed(123)
# Dumy code categorical predictor variables
train_x <- model.matrix(label~., train_xy_4_9)[,-1]
# Convert the outcome (class) to a numerical variable
train_y <- as.matrix(train_y_4_9$label)
cv.lasso <- cv.glmnet(train_x, train_y, alpha = 1, family = "binomial")
plot(cv.lasso)
print(cv.lasso$lambda.min)
```

```{r}
coef_optim_LASSO <- coef(cv.lasso, s= cv.lasso$lambda.min)
coef_df <- as.data.frame(summary(coef_optim_LASSO))
col_names_df <- rbind(c("Intercept", 0),data.frame(colnames(train_x_4_9)))
# saving all leftover significant predictors
coef_shrink <- col_names_df[c(coef_df$i),1]
coef_df <- cbind(coef_shrink, coef_df)
coef_df <- coef_df[,-c(2,3)]
colnames(coef_df)[1] <- "predictors"
colnames(coef_df)[2] <- "coef"
coef_df$coef <- round(coef_df$coef, 7)
```

Positive Coefficient
```{r}
coef_df %>% arrange(desc(coef)) %>% top_n(10)
```

Negative Coefficient
```{r}
coef_df %>% arrange(coef) %>% top_n(-10)
```

```{r}
row <- seq(1, 12)
col <- seq(1, 12)
grid_df <- expand.grid(Col = col, Row = row)
grid_df[, 'coef'] <- 0
index <- as.data.frame(summary(coef_optim_LASSO))[, c('i', 'x')]
index <- index[-1,]
for (i in index$i){
  grid_df[i, 3] = index[which(index$i == i), 2]
}
grid_df$coef_sign <- factor(grid_df$coef, levels = c("positive", "0", "negative"))
grid_df$coef_sign[grid_df$coef > 0]<-"positive"
grid_df$coef_sign[grid_df$coef == 0]<-"0"
grid_df$coef_sign[grid_df$coef < 0]<-"negative"
```

This is the pixel importance plot for 0-1 classification
```{r}
plot(gg)
```

This is the plot for 4-9 classification.
```{r}
seq <- seq(1, 20)
library(ggplot2)
gg <- ggplot(grid_df, aes(x=Row, y=Col)) + 
  geom_point(aes(col = coef_sign, size = coef)) + 
  labs( 
       y="Col", 
       x="Row", 
       title="Relationship between the pixels and the coefficients") + 
  scale_x_continuous("Row", labels = as.character(seq), breaks = seq) +
  scale_y_continuous("Col", labels = as.character(seq), breaks = seq)
plot(gg)
```

The graph for 4-9 classification looks more complicated than the graph for 0-1. Whereas we can clearly detect a seemingly circular pattern in the middle of 0-1 graph, we find it difficult to do similar thing in 4-9 graph. 

However, we may still see that, on the center-left of the graph, there is a positive pixel pattern that looks like a circle followed by a tail, which looks like digit 9. And on the center-right, we see a straight oblique line and a straight horizontal line, which look like a pattern for digit 4.

### Part 2 (5 pts.)

Compute QDA and Naive Bayes classifiers for the 4/9 problem.  Compare their performance to your logistic regression.  What's going on here?  Why do we see what we see?  Think carefully about the assumptions under-the-hood for QDA, Naive Bayes, and logistic regression and the structure of the pixel data in the MNIST data set.

**QDA classifier**

```{r, warning=FALSE}
set.seed(123)
library(MASS)
# load the datasets
train_x <- fread("MNISTTrainX.csv")
train_y <- fread("MNISTTrainY.csv")
validate_x <- fread("MNISTValidationX.csv")
validate_y <- fread("MNISTValidationY.csv")
test_x <- fread("MNISTTestXRand.csv")
test_y <- fread("MNISTTestYRand.csv")
index_4_9 <- append(which(train_y$label == 4), which(train_y$label == 9))
train_x_4_9 <- train_x[index_4_9,]
train_y_4_9 <- train_y[index_4_9,]
train_xy_4_9 <- cbind(train_x_4_9, train_y_4_9)
train_xy_4_9$label <- as.factor(train_xy_4_9$label)
index_4_9 <- append(which(validate_y$label == 4), which(validate_y$label == 9))
test_x_4_9 <- validate_x[index_4_9,]
test_y_4_9 <- validate_y[index_4_9,]
```

training error
```{r}
set.seed(123)
# log.fit<-glm(label~.,data=train_xy_4_9,family=binomial, control = list(maxit = 100))
qda.fit<-qda(label~.,data=train_xy_4_9, control = list(maxit = 100))
## Accuracy
pred <- predict(qda.fit, train_x_4_9, type = "response")
confmatrix <- table(observed = train_y_4_9$label, predict = pred$posterior[,2] > 0.5)
confmatrix
print(paste("misclassification rate: ", (confmatrix[1,2] + confmatrix[2,1]) / sum(confmatrix)))
```

10-fold cross validation
```{r, warning=FALSE}
set.seed(123)
misclass_cv_err <- c()
#Randomly shuffle the data
set.seed(123)
rand_df<-data.frame(train_xy_4_9[sample(nrow(train_xy_4_9)),])
#Create n folds equally size folds
n_folds <- cut(seq(1,nrow(rand_df)),breaks=10,labels=FALSE)
# perform the k-fold cross validation
for (i in 1:10){
  #Segment the data by fold using the which() function 
  index <- which(n_folds == i, arr.ind = T)
  test_data <- rand_df[index,]
  train_data <- rand_df[-index,]
  y_name = names(rand_df)[145]
  x_name = names(rand_df)[-145]
  test_x <- test_data[, x_name]
  test_y <- test_data[, y_name]
  # construct the model and predict the outcome
  # log.fit<-glm(label~.,data=train_data,family=binomial, control = list(maxit = 100))
  qda.fit<-qda(label~.,data=train_data, control = list(maxit = 100))
  pred_cv = predict(qda.fit, newdata=test_x, type = "response")
  # get the misclassfied error
  confmatrix <- table(observed = test_y, predict = pred_cv$posterior[,2] > 0.5)
  misclass_cv_err[i] <- (confmatrix[1,2] + confmatrix[2,1]) / sum(confmatrix)
}
misclass_cv_err
paste("Misclassified rate w/ 10 fold on training data: ", round(mean(misclass_cv_err), 5))
misclass_rates$qda=round(mean(misclass_cv_err), 5)
```


**Naive Bayes Classifier**

training error
```{r,echo=FALSE}
set.seed(123)
library(naivebayes)
# log.fit<-glm(label~.,data=train_xy_4_9,family=binomial, control = list(maxit = 100))
nb.fit<-naive_bayes(label ~ ., data = train_xy_4_9, usekernel = T)
## Accuracy
pred <- predict(nb.fit, train_x_4_9, type = "prob")
confmatrix <- table(observed = train_y_4_9$label, predict = pred[,2] > 0.5)
confmatrix
print(paste("misclassification rate: ", (confmatrix[1,2] + confmatrix[2,1]) / sum(confmatrix)))
```

10-fold cross validation
```{r, warning=FALSE}
set.seed(123)
misclass_cv_err <- c()
#Randomly shuffle the data
set.seed(123)
rand_df<-data.frame(train_xy_4_9[sample(nrow(train_xy_4_9)),])
#Create n folds equally size folds
n_folds <- cut(seq(1,nrow(rand_df)),breaks=10,labels=FALSE)
# perform the k-fold cross validation
for (i in 1:10){
  #Segment the data by fold using the which() function 
  index <- which(n_folds == i, arr.ind = T)
  test_data <- rand_df[index,]
  train_data <- rand_df[-index,]
  y_name = names(rand_df)[145]
  x_name = names(rand_df)[-145]
  test_x <- test_data[, x_name]
  test_y <- test_data[, y_name]
  # construct the model and predict the outcome
  # log.fit<-glm(label~.,data=train_data,family=binomial, control = list(maxit = 100))
  nb.fit<-naive_bayes(label ~ ., data = train_xy_4_9, usekernel = T)
  pred_cv = predict(nb.fit, newdata=test_x, type = "prob")
  # get the misclassfied error
  confmatrix <- table(observed = test_y, predict = pred_cv[,2] > 0.5)
  misclass_cv_err[i] <- (confmatrix[1,2] + confmatrix[2,1]) / sum(confmatrix)
}
misclass_cv_err
paste("Misclassification rate w/ 10 fold on training data: ", round(mean(misclass_cv_err), 5))
misclass_rates$nb=round(mean(misclass_cv_err), 5)
```

Let us compare misclassification rates.
```{r}
misclass_rates
```

AS we can see, logistic regression performs better than the generative methods, and the reason may be that the assumption for QDA and NB are not tenable in our dataset. 

QDA assumes that features, pixels in our case, follow a multivariate normal distribution, but it may not be the case in our data. Most of the pictures have white pixels in peripheral space; so they are not normal in distribution; the pixels in the middle are highly dependent on the number patterns so they are not normal either.

Naive Bayes classifer assumes pixels are independent, but our pictures have correlated pixels. For example, if a group of pixels are black, then possibly pixels the pixels to the left and the ones to the right are also black, constituting a horizontal line of the digit 4.

Logistic regression does not depend on these assumptions about feature distribution.


### Part 3 (10 pts.)

Using the full suite of classification approaches discussed in class, find a classification approach that **minimizes** the expected misclassification rate of 4s and 9s on a true out of sample data set.

Your answer should discuss the possible approaches to this problem and explain how you made your final choice.  Discuss how you chose any tuning parameter values.

You do not need to run all possible classification methods to get full credit for this question!  There are some methods that we can rule out without ever running them.  When doing this, provide grounded reasoning related to the strengths and weaknesses of different approaches.

For your chosen method, explain **why** it outperforms all other approaches.  Think carefully about strengths and weaknesses.  

Finally, present at least 4 examples of misclassified images.  Could we ever expect a classification algorithm to get those images correct?


**Logistic GAM**

training error
```{r}
set.seed(123)
library(gam)
lgam.fit<-gam(label~.,data=train_xy_4_9,family=binomial)
## Accuracy
pred <- predict(lgam.fit, train_x_4_9, type = "response")
confmatrix <- table(observed = train_y_4_9$label, predict = pred > 0.5)
confmatrix
print(paste("misclassification rate: ", (confmatrix[1,2] + confmatrix[2,1]) / sum(confmatrix)))
```

10-fold cv
```{r, warning=FALSE}
set.seed(123)
misclass_cv_err <- c()
#Randomly shuffle the data
set.seed(123)
rand_df<-data.frame(train_xy_4_9[sample(nrow(train_xy_4_9)),])
#Create n folds equally size folds
n_folds <- cut(seq(1,nrow(rand_df)),breaks=10,labels=FALSE)
# perform the k-fold cross validation
for (i in 1:10){
  #Segment the data by fold using the which() function 
  index <- which(n_folds == i, arr.ind = T)
  test_data <- rand_df[index,]
  train_data <- rand_df[-index,]
  y_name = names(rand_df)[145]
  x_name = names(rand_df)[-145]
  test_x <- test_data[, x_name]
  test_y <- test_data[, y_name]
  # construct the model and predict the outcome
  lgam.fit<-gam(label~.,data=train_data,family=binomial)
  pred_cv = predict(lgam.fit, newdata=test_x, type = "response")
  # get the misclassfied error
  confmatrix <- table(observed = test_y, predict = pred_cv > 0.5)
  misclass_cv_err[i] <- (confmatrix[1,2] + confmatrix[2,1]) / sum(confmatrix)
}
misclass_cv_err
paste("Misclassified rate w/ 10 fold on training data: ", round(mean(misclass_cv_err), 5))
misclass_rates$logistic_gam=round(mean(misclass_cv_err), 5)
```

**SVC**

```{r}
set.seed(123)
library(e1071)
tune_svm1<-e1071::tune(method=svm, label~., data=train_xy_4_9,
                       kernel="linear",ranges=list(cost=10^(seq(-2.5,-1.5,length = 5))))

plot(tune_svm1)
```

Compute 10-fold CV estimate for SVC linear.
```{r}
best_svm1=tune_svm1$best.model
print("10-fold cross validation estimate for SVC:")
tune_svm1$best.performance
misclass_rates$svc=tune_svm1$best.performance
```


**SVM with radial kernel**

```{r}
set.seed(123)
library(kernlab)
#Set up a cost tuning path
cost_vals <- 10^seq(-1,3,length = 5)
#Holder for Cost 10-fold CV vals
cv_val <- c()
#Loop over all costs
cverror=c()
for(i in 1:length(cost_vals)){
  svm1 <- kernlab::ksvm(label ~ ., data = train_xy_4_9, type = "C-svc", kernel = "rbfdot", C = cost_vals[i], kpar = "automatic", cross = 10)
  #Store the CV value
  cv_val[i] <- svm1@cross
  print(paste("Cost = ",cost_vals[i], " ; CV Error = ", svm1@cross, sep=""))
  cverror=c(svm1@cross,cverror)
}
misclass_rates$svm=min(cverror)
```
The best model has cost=10.


**random forest**

```{r}
set.seed(123)
library(caret)
library(e1071)
numFolds <- trainControl(method = "cv", number = 10)
cpGrid <- expand.grid(.cp = 10^seq(-3,-1,0.01))
cvrf<-train(label~.,, data = train_xy_4_9, method = "rpart", trControl = numFolds, tuneGrid = cpGrid)
print("best for rf")
print(max(cvrf$results$Accuracy))
misclass_rates$rf=1-max(cvrf$results$Accuracy)
```


**stacking**

```{r,echo=FALSE,warning=FALSE}
set.seed(123)
library(SuperLearner)
#See all available methods with listWrappers()
#Split data into x_train and y_train
y_train <- train_xy_4_9$label
x_train <- train_xy_4_9
x_train$label <- NULL
#Let's run a model stack with 5 models: random forest, svm, logistic regression, 
#qda, and mean (we just take the average)
#Set some tuning parameters

y_train <- as.numeric(as.character(y_train))
y_train[y_train==4]=0
y_train[y_train==9]=1
# stack <- SuperLearner(Y = y_train, X = x_train, family = binomial(), cvControl = list(V = 10),
# SL.library = c("SL.ranger.better", "SL.ksvm.better","SL.glm","SL.qda","SL.mean"))
stack <- SuperLearner(Y = y_train, X = x_train, cvControl = list(V = 10), family = binomial(),
SL.library = c("SL.glm","SL.qda","SL.svm"))
```

```{r}
y_valid <- test_y_4_9$label
x_valid <- test_x_4_9
x_valid$label <- NULL
y_valid[y_valid==4]=0
y_valid[y_valid==9]=1
preds <- predict(stack, x_valid)
```

Compute estimate of prediction error using validation set.
```{r}
error=0
pred = preds$pred
pred[pred<0.5]=NA
pred[!is.na(pred)]=0
pred[is.na(pred)]=1
for (i in 1:length(pred)){
  if (pred[i]!=y_valid[i]){
    error=error+1
  }
}
print("misclassification rate on validation set")
error/length(pred)
misclass_rates$stacking=1-error/length(pred)
```

**XGBoost**

```{r, message=FALSE}
set.seed(123)
library(xgboost)
library(caTools)
y_train <- train_xy_4_9$label
y_train <- as.numeric(as.character(y_train))
y_train[y_train==4]=0
y_train[y_train==9]=1
params=list(
  objective='binary:logistic',
  eta=1,
  max_depth=6
)
xgbcv=xgb.cv(params=params,data=as.matrix(train_x_4_9),nrounds=20,nfold=10,label=y_train,metrics='error',verbose=F)
xgbcv$evaluation_log
print("cross validation estimate:")
min(xgbcv$evaluation_log$test_error_mean)
misclass_rates$xgboost=min(xgbcv$evaluation_log$test_error_mean)
```


We choose a collection of approaches that includes all the model types: discriminative models (logistic regressions), generative models (NB and QDA classifier) and flexible models (SVC, SVM, XGBoost, stacking and random forest). Some of them are linear models (logistic regression) and some are not (SVC, SVM, random forest, etc). We do not use nearest-neighbor classifiers, because they are almost guaranteed to perform badly due to the high dimensionality of our features.

Now, let us see the classification rates for different methods.
```{r}
misclass_rates
```

It turns out that SVM (radial kernel) is the best model, and XGBoost and stack are performing almost as good as SVM. While the boundary between 4 and 9 may be complex and unclear (discussed in previous part), SVM with radial kernel is able to draw nonlinear decision boundary, which is a great advantage for this problem. Besides, the distribution of our features and the interaction among them are very complicated, so it is hard to make any assumption about data distribution and independence. SVM does not make such assumption, making it potentially more powerful than generative classifiers.

Besides, ensemble methods like XGBoost and stacking are also very competitive. XGBoost aggregates a bunch of weak learners (trees) and make it more powerful than a decision tree. It does not make any data distribution or independence assumption, making it great choice for large datasets with many features and complicated distribution (MNIST). Stacking aggregates multiple types of learners (SVM, logistic regression, etc.) and outperforms the base learners by combining them using meta learner. In our example, we have QDA classifier, logistic regression (LR) and SVM as base learners, and each of them have their respective advantages and disadvantages. Then, a meta learner combines their advantages and minimizes their drawbacks, making it a very strong learning algorithm.

Now, let us see some misclassified digits by SVM.
```{r, eval = FALSE}
set.seed(123)
svm_best <- kernlab::ksvm(label ~ ., data = train_xy_4_9, type = "C-svc", kernel = "rbfdot", C = 10, kpar = "automatic")
pred <- predict(svm_best, test_x_4_9)
pred = as.numeric(as.character(pred))
```

```{r}
set.seed(123)
misclass_index = c()
q3test = cbind(test_x_4_9,test_y_4_9)
for (i in 1:length(pred)){
  if (pred[i]!=q3test$label[i]){
    misclass_index=c(misclass_index,i)
  }
}
q3test_for_plot=q3test[misclass_index,]
q3test_for_plot$label=NULL
index = sample.int(nrow(q3test_for_plot),9)
for (i in index){
  plot_digit(x=q3test_for_plot[i,])
}
```

Some of the misclassified digits seems easily separable from the wrong class (like the last digit). Probably a linear separator, such as logistic regression and SVC (linear), would do a good job. Some of them are very hard to distinguish every by human eyes, and so most of the approaches probably cannot handle them.

### Part 4 (5 pts.)

Use the hidden test set to generate a prediction for each included image.  Your predictions should be stored in a matrix that has the image key as the first column and the integer value of the class in the second column.  Save this matrix as `Q3Predictions.csv` and include it with your final submission.

Points for this question will be given with respect to classification accuracy.  Let $E_i$ be the proportion of observations in the test set (that are actually 4s and 9s) misclassified.  Let $E_{min}$ be the minimum proportion of misclassified observations across the class.  Then, you final point total for this part will be:

$$5 \times \frac{E_{max}}{E_i}$$
Processing testing dataset.
```{r, warning=FALSE}
test_x <- fread("MNISTTestXRand.csv")
test_y <- fread("MNISTTestYRand.csv")


t_x_4_9 <- test_x
t_y_4_9 <- test_y
t_xy_4_9 <- cbind(t_x_4_9, t_y_4_9)
t_xy_4_9$label <- as.factor(t_xy_4_9$label)
```


```{r, eval = FALSE}
set.seed(123)
svm_best <- kernlab::ksvm(label ~ ., data = train_xy_4_9, type = "C-svc", kernel = "rbfdot", C = 10, kpar = "automatic")
pred <- predict(svm_best, t_x_4_9)
```

store in a csv file
```{r}
pred_result=cbind(t_y_4_9$key, as.numeric(as.character(pred)))
write.csv(pred_result, "Q3Predictions.csv",row.names = FALSE)
```


## Question 4 (20 pts.)

Now, let's work with **three classes** - 3s, 5s, and 8s.  Start by subsetting your training and validation sets to only include 3s, 5s and 8s.

### Part 1 (5 pts.)

Let's start with multinomial logistic regression.  Compute a 10-fold CV measure of the expected misclassification error and compute the error rate on the validation set.  How does this compare to your previous two-class analyses?

Using the **posterior probabilities** that each observation in the validation set belongs to each class, find 2 images that are close to each decision boundary: 3/5, 3/8, 5/8, and 3/5/8.  Plot these images and discuss any factors discriminating between these three digits that multinomial logistic regression is missing. Hint: Think about the contextual aspects that tell a human that a digit is a digit.

Note: As with logistic regression, multinomial logistic regression iteratively optimizes the multinomial logistic loss function.  The default of 100 iterations is likely not enough.  You can change this in `nnet::multinom` by adding an argument `maxit = 1000`.

### Solution
```{r}
train_x <- fread("MNISTTrainX.csv")
train_y <- fread("MNISTTrainY.csv")
index_3 <- c()
for (i in c(3, 5, 8)){
  index_3 <- append(index_3, which(train_y$label == i))
}
train_x_3 <- train_x[index_3,]
train_y_3 <- train_y[index_3,]
train_xy_3 <- cbind(train_x_3, train_y_3)
train_xy_3$label <- as.factor(train_xy_3$label)
index_3 <- c()
for (i in c(3, 5, 8)){
  index_3 <- append(index_3, which(validate_y$label == i))
}
test_x_3 <- validate_x[index_3,]
test_y_3 <- validate_y[index_3,]
```

Multinomial Logistic Regression (10-fold CV)
```{r, echo= TRUE, results = 'hide'}
library(nnet)
misclass_cv_err <- c()
#Randomly shuffle the data
set.seed(123)
rand_df<-data.frame(train_xy_3[sample(nrow(train_xy_3)),])
#Create n folds equally size folds
n_folds <- cut(seq(1,nrow(rand_df)),breaks=10,labels=FALSE)
# perform the k-fold cross validation
for (i in 1:10){
  #Segment the data by fold using the which() function 
  index <- which(n_folds == i, arr.ind = T)
  test_data <- rand_df[index,]
  train_data <- rand_df[-index,]
  y_name = names(rand_df)[145]
  x_name = names(rand_df)[-145]
  test_x <- test_data[, x_name]
  test_y <- test_data[, y_name]
  # construct the model and predict the outcome
  multi_log.fit <- multinom(label ~ . , data = train_data, maxit = 1000)
  pred_cv = predict(multi_log.fit, newdata=test_x)
  # get the misclassfied error
  confmatrix <- table(pred_cv, test_y)
  misclass_cv_err[i] <- 1 - sum(diag(confmatrix)) / sum(confmatrix)
}
```

```{r}
# 10-fold error
misclass_cv_err
paste("Misclassified rate w/ 10 fold on training data: ", round(mean(misclass_cv_err), 5))
```

Test Error
```{r echo=TRUE,results = 'hide'}
multi_log.fit <- multinom(label ~ . , data = train_xy_3, maxit = 1000)
pred <- predict(multi_log.fit, test_x_3)
```

```{r}
# test accuracy
confmatrix <- table(pred, test_y_3$label)
confmatrix
print(paste("Misclassified rate on validation set: ", round(1 - sum(diag(confmatrix)) / sum(confmatrix), 5)))
```

Close Decision Boundary: $3$ & $5$
```{r}
pred_prob <- as.data.frame(predict(multi_log.fit, test_x_3, type = "prob"))
names(pred_prob) <- c("digit_3", "digit_5", "digit_8")
closeBoundary_3_5 <- which(((pred_prob$digit_3 > 0.48) & (pred_prob$digit_3 < 0.52)) 
                           & ((pred_prob$digit_5 > 0.48) & (pred_prob$digit_5 < 0.52)))
pred_prob[closeBoundary_3_5,]
par(mfrow = c(2,1))
for (i in c(1, 3)){
  index = closeBoundary_3_5[i]
  plot_digit(test_x_3[index,], bw = FALSE, 
             main = paste("Predict label:", pred[index],
                          "; Actual label:", test_y_3[index, 1]))
}
```

Close Decision Boundary: $3$ & $8$
```{r}
closeBoundary_3_8 <- which(((pred_prob$digit_3 > 0.48) & (pred_prob$digit_3 < 0.52)) 
                           & ((pred_prob$digit_8 > 0.48) & (pred_prob$digit_8 < 0.52)))
pred_prob[closeBoundary_3_8,]
par(mfrow = c(2,1))
for (i in c(3, 6)){
  index = closeBoundary_3_8[i]
  plot_digit(test_x_3[index,], bw = FALSE, 
             main = paste("Predict label:", pred[index],
                          "; Actual label:", test_y_3[index, 1]))
}
```
Close Decision Boundary: $3$ & $8$
```{r}
closeBoundary_5_8 <- which(((pred_prob$digit_5 > 0.48) & (pred_prob$digit_5 < 0.52)) 
                           & ((pred_prob$digit_8 > 0.48) & (pred_prob$digit_8 < 0.52)))
pred_prob[closeBoundary_5_8,]
par(mfrow = c(2,1))
for (i in c(2, 5)){
  index = closeBoundary_5_8[i]
  plot_digit(test_x_3[index,], bw = FALSE, 
             main = paste("Predict label:", pred[index],
                          "; Actual label:", test_y_3[index, 1]))
}
```

Close Decision Boundary: $3$ & $5$ & $8$
```{r}
closeBoundary_3_5_8 <- which((pred_prob$digit_3 > 0.27) 
                           & (pred_prob$digit_5 > 0.27)
                           & (pred_prob$digit_8 > 0.27))
pred_prob[closeBoundary_3_5_8,]
par(mfrow = c(2,1))
for (i in c(2, 4)){
  index = closeBoundary_3_5_8[i]
  plot_digit(test_x_3[index,], bw = FALSE, 
             main = paste("Predict label:", pred[index],
                          "; Actual label:", test_y_3[index, 1]))
}
```

One factor that prevent multinomial logistic regression to distinguish some of the number is the classifier's sole focus on leveraging the pixel density of certain location to make classification decision.

In my opinion, what makes logistic regression has such a great classifiers can also leads to some drawbacks. The classifier did great in most of the case where there is a distinct decision boundary that we can draw (clear, well-written digits in our case.)

Plus, regression approach has the tendency to average things. While this is not a problem for digits with small within class variance like $0$, $2$, $3$, it might fail to identify some special cases when things get more complicated (large within class variation, ambiguious classification boundary). 

As decision boundary get more blurry (various strokes, blips of the written digits, alike digits,) that is when the classifier starts to become hesitate in making the decision; there are different classes share the same pencil pixel density for certain locations. 

As human, on the other hands, we can identify whatever digit that is presented to us because we just know it; there are always certain features that associate the given digit regardless how it is written out.

Hence, our model could have do a better job in classification if it can not only draw out the decision boundary base on the location of pixel density, but also establish some connection between features(coordinate in our case) and the class itself.

### Part 2 (10 pts.)

Using any information gained from Part 1 to your advantage, find a classification approach that **minimizes** the expected misclassification rate of 3s, 5s, and 8s on a true out of sample data set.

Your answer should discuss the possible approaches to this problem and explain how you made your final choice.  Discuss how you chose any tuning parameter values.

You do not need to run all possible classification methods to get full credit for this question!  There are some methods that we can rule out without ever running them.  When doing this, provide grounded reasoning related to the strengths and weaknesses of different approaches.

For your chosen method, explain **why** it outperforms all other approaches.  Think carefully about strengths and weaknesses.  

Finally, present at least 4 examples of misclassified images.  Could we ever expect a classification algorithm to get those images correct?

### Solution

**Model 1: Multinomial Logistic w/ variable selection - LASSO**

Let's first see if shrinkage/regularization approach work in this scenario.

```{r, eval = FALSE}
# set.seed(123)
# train_X <- model.matrix(label~., train_xy_3)[,-1]
# train_Y <- as.matrix(train_y_3$label)
# cv.multinom_LASSO <- cv.glmnet(train_X, train_Y, alpha = 1, family = "multinomial")
# plot(cv.multinom_LASSO)
# 
# print(cv.multinom_LASSO$lambda.min)
```
Since it takes a long time to run a multinomial logistic regression models with LASSO, I decide to only include the result here rather than taking the time running the code chunk again and again. Note: the path below should be the path to 000003.png image.

`lambda.min` = 0.001180999

![plot, cv.multinom_LASSO](000003.png)


```{r, eval = FALSE}
# coef_optim_multi.LASSO <- coef(cv.multinom_LASSO, s= cv.multinom_LASSO$lambda.min)
# coef_shrink <- coef_optim_LASSO@i[-1]
# 
# rowName_shrink <- c()
# for (i in 1:length(coef_shrink)){
#   rowName_shrink <- append(rowName_shrink, names(train_x_3)[coef_shrink[i]])
# }
# 
# train_x_shrink <- train_x_3[, c(2,3,8,  23,  28,  29,  32,  36,  46,  51,  52,  57,  58,  59,
#               61,  63,  64,  67,  69,  70,  71,  72,  75,  77,
#               78,  79,  81,  82,  87, 88, 89,  90,  92,  93,  94,  95,
#               103, 113, 120, 125, 126, 131, 135, 136)]
# 
# test_x_shrink <- test_x_3[, c(2,3,8,  23,  28,  29,  32,  36,  46,  51,  52,  57,  58,  59,
#               61,  63,  64,  67,  69,  70,  71,  72,  75,  77,
#               78,  79,  81,  82,  87, 88, 89,  90,  92,  93,  94,  95,
#               103, 113, 120, 125, 126, 131, 135, 136)]
# 
# train_xy_shrink <- cbind(test_x_shrink, train_xy_3$label)
# 
# multi_shrink_log.fit <- multinom(V2 ~ . , data = train_xy_shrink, maxit = 1000)
# pred <- predict(multi_shrink_log.fit, test_x_shrink)
# 
# # test accuracy
# confmatrix <- table(pred, test_y_3$label)
# confmatrix
# print(paste("Misclassified rate on validation set: ", round(1 - sum(diag(confmatrix)) / sum(confmatrix), 5)))
```

*Test Error*

| actual / pred | 3 | 5 | 8 |
| --- | --- | --- | --- | 
| 3   | 1133 | 171 | 32
| 5 | 258 | 194 | 1422
| 8    | 109 | 1135 | 46

Misclassified rate on validation set:  0.69489

It looks like the shrinkage approach is not suitable for multiclass digit classification. It might be the case where some less relevant features for one class can be the determine factor for other classes.

Hence, in order to gain better accuracy, it is better for us still include all predictors regardless for the rest of the models.

**Model 2: Naive Bayes**

Let try some generative classification methods
```{r}
library(naivebayes)
nb_x <- data.frame(train_x_3)
nb_y <- as.character(train_y_3$label)
nb_mod_kde <- naivebayes::naive_bayes(x = nb_x, y = nb_y, usekernel = TRUE)
nb_mod_gauss <- naivebayes::naive_bayes(x = nb_x, y = nb_y, usekernel = FALSE)
pred_kde <- predict(nb_mod_kde, test_x_3)
pred_gauss <- predict(nb_mod_gauss, test_x_3)
# kde accuracy
confmatrix <- table(pred_kde, test_y_3$label)
confmatrix
print(paste("Misclassified rate on validation set: ", round(1 - sum(diag(confmatrix)) / sum(confmatrix), 5)))
# gauss accuracy
confmatrix <- table(pred_gauss, test_y_3$label)
confmatrix
print(paste("Misclassified rate on validation set: ", round(1 - sum(diag(confmatrix)) / sum(confmatrix), 5)))
```

NB does not seems to perform well in this case. I am guessing that it is because NB is too good in picking out digits with odd pattern/forms and eventually winds up overfitting the training set.

NB works well when features are independet to each other, which is highly unlikely in our case. When classifying digits, features, the location of pencil pixel in our case, are likely to asscociate with each other given a paricular class of digits. NB's failure in constructing a well-rounded relationship between predictor may also explain why the model performs so poorly.

**Model 3: LDA**
```{r}
library(MASS)
lda_mod <- MASS::lda(label ~ ., data = train_xy_3)
pred_lda <- predict(lda_mod, test_x_3)
# LDA accuracy
confmatrix <- table(pred_lda$class, test_y_3$label)
confmatrix
print(paste("Misclassified rate on validation set: ", round(1 - sum(diag(confmatrix)) / sum(confmatrix), 5)))
```

Similar to multinomial logistic, LDA also has a great accuracy in classifying digits, just little bit behind multinomial in terms of misclassification rate.

**Model 4: SVC & SVM**

Let's try the infamous Support Vector Classifier / Machine

Because of the limited computing power, we sample 500 to make the computation more easier to conduct for SVM
```{r}
library(e1071)
set.seed(385)
train_xy_3_sample <- train_xy_3[sample(which(train_xy_3$label == 3), 500),]
train_xy_3_sample <- rbind(train_xy_3_sample, train_xy_3[sample(which(train_xy_3$label == 5), 500),])
train_xy_3_sample <- rbind(train_xy_3_sample, train_xy_3[sample(which(train_xy_3$label == 8), 500),])
train_x_svm <- train_xy_3_sample[,-145]
train_y_svm <- train_xy_3_sample[,145]
```

```{r warning=FALSE}
svm_train_10000 <- svm(label ~ ., data = train_xy_3_sample, kernel = "linear", cost = 100, scale = FALSE)
svm_train_10000_radial <- svm(label ~ ., data = train_xy_3_sample, kernel = "radial", cost = 100, scale = FALSE)
pred_svm <- predict(svm_train_10000, test_x_3)
pred_svm_radial <- predict(svm_train_10000_radial, test_x_3)
# SVM_linear accuracy
confmatrix <- table(pred_svm, test_y_3$label)
confmatrix
print(paste("Misclassified rate on validation set: ", round(1 - sum(diag(confmatrix)) / sum(confmatrix), 5)))
# SVM_radial accuracy
confmatrix <- table(pred_svm_radial, test_y_3$label)
confmatrix
print(paste("Misclassified rate on validation set: ", round(1 - sum(diag(confmatrix)) / sum(confmatrix), 5)))
```

The result is actually pretty shocking. The SVC outperforms SVM by an incredible margin. SVM totally failed in putting the digits in the right group.

Judging the above result, it seems like non-linear approach to does not work in this case. My theory is that even though non-linear approaches are good to identifying weitrd trends/distribution, it is too sensitive to those anomaly unfortunately. As result, the classifier winds up taking into those wired factors into accounts and misclassifies a lot of regularized digits. 

Plus, most of the digits are consistent with the shape that it supposes to have (when we are averaging the pixel for each digit, most of the digits are recognizable and distinguishable.) Hence, models that are good at separating linear boundary, like multinomial logistic, LDA, and SVC might be better in this case. 

Let's tune the SVC above to get a better result
```{r}
tune_svm1 <- tune(method = svm, label ~ ., data = train_xy_3_sample, 
                         kernel = "linear", ranges = list(cost = 10^(seq(-2.5,-1.5,length = 50))))
svm_best <- tune_svm1$best.model
pred_svm_best <- predict(svm_best, test_x_3)
# tune SVM accuracy
confmatrix <- table(pred_svm_best, test_y_3$label)
confmatrix
print(paste("Misclassified rate on validation set: ", round(1 - sum(diag(confmatrix)) / sum(confmatrix), 5)))
```

Like we anticipated, SVC is great in classifying the three class problem, even outperforming our original winner - multinomial logistic regression.

**Model 5: XGBoost**

Let's use ensemble approach to see it helps with improving the classification accuracy.

```{r}
library(caTools)
library(xgboost)
XGB_train <- cbind(train_x_3, train_y_3)
XGB_cv <- cbind(test_x_3, test_y_3)
# convert every variable to numeric, even the integer variables
XGB_train <- as.data.frame(lapply(XGB_train, as.numeric))
XGB_cv <- as.data.frame(lapply(XGB_cv, as.numeric))
# convert data to xgboost format
XGB.data.train <- xgb.DMatrix(data = data.matrix(XGB_train[, 1:ncol(XGB_train) - 1]), 
                              label = XGB_train$label)
XGB.data.cv <- xgb.DMatrix(data = data.matrix(XGB_cv[, 1:ncol(XGB_cv) - 1]), 
                           label = XGB_cv$label)
watchlist <- list(train  = XGB.data.train, test = XGB.data.cv)
parameters <- list(
    # General Parameters
    booster            = "gbtree",          # default = "gbtree"
    silent             = 0,                 # default = 0
    # Booster Parameters
    eta                = 0.3,               # default = 0.3, range: [0,1]
    gamma              = 0,                 # default = 0,   range: [0,inf]
    max_depth          = 6,                 # default = 6,   range: [1,inf]
    min_child_weight   = 1,                 # default = 1,   range: [0,inf]
    subsample          = 1,                 # default = 1,   range: (0,1]
    colsample_bytree   = 1,                 # default = 1,   range: (0,1]
    colsample_bylevel  = 1,                 # default = 1,   range: (0,1]
    lambda             = 1,                 # default = 1
    alpha              = 0,                 # default = 0
    # Task Parameters
    objective          = "multi:softmax",   # default = "reg:linear"
    eval_metric        = "merror",
    num_class          = 10,
    seed               = 1234               # reproducability seed
    )
    
xgb.model <- xgb.train(parameters, XGB.data.train, nrounds = 10, watchlist)
```

```{r}
XGB_pred <- predict(xgb.model, XGB.data.cv)
confmatrix <- table(XGB_pred, XGB_cv$label)
confmatrix
print(paste("Misclassified rate on validation set: ", round(1 - sum(diag(confmatrix)) / sum(confmatrix), 5)))
```

As we laern from the class, ensemble approach typically outperforms most of the single-classification models. XGBoost is an absolutely beast when doing the classification. It outperforms our second candidate SVC by a huge margin. 

XGBoost is essentially a much better version of tree based methods by aggregating a bunch of weak learners together. We do not need to make any assumption necessarily about the data itself, which make it quite flexible to deal with large data set like MNIST. Plus, the boosting makes outliers have minimal impact on the prediction, preventing potential overfitting to occur. It workds well in our case where the dataset is large, contains many features, and is presents with outliers

Nevertheless, XGBoost has some setbacks too: it needs some serious tuning to realize its potential, and it is possible to overfit the data without proper tuning. Here, I just use all default parameter in terms of tunign the model. Although it performs well in the validation, it is still a concern if it does a much better job than other candidate model in the hidden test set.

Let's see some the misclassification by XGBoost
```{r}
predicitions <- data.frame(XGB_pred, XGB_cv$label)
misclass_XGB <- which((predicitions$XGB_cv.label == 3) & (predicitions$XGB_pred != 3))
misclass_XGB <- append(misclass_XGB,which((predicitions$XGB_cv.label == 5) & (predicitions$XGB_pred != 5)))
misclass_XGB <- append(misclass_XGB,which((predicitions$XGB_cv.label == 8) & (predicitions$XGB_pred != 8)))
sample_misclass <- sample(misclass_XGB, 6)
par(mfrow = c(2,3))
for (i in sample_misclass){
  plot_digit(test_x_3[i,], bw = FALSE, 
             main = paste("Predict label:", predicitions[i,1],
                          "; Actual label:", predicitions[i, 2]))
}
```

About mistakes. Some digits are so badly written that neither Superpowerful Algorithm nor Human will never recognize them. But for some other digits there are just not enough train set clusters to classify them correctly.

### Part 3 (5 pts.)

Use the hidden test set to generate a prediction for each included image.  Your predictions should be stored in a matrix that has the image key as the first column and the integer value of the class in the second column.  Save this matrix as `Q4Predictions.csv` and include it with your final submission.

Points for this question will be given with respect to classification accuracy.  Let $E_i$ be the proportion of observations in the test set (that are actually 3s, 5s, and 8s) misclassified.  Let $E_{min}$ be the minimum proportion of misclassified observations across the class.  Then, you final point total for this part will be:

$$5 \times \frac{E_{max}}{E_i}$$
### Solution

```{r, eval = FALSE}
XGB_q4_testX <- as.data.frame(lapply(test_x_rand, as.numeric))
# convert data to xgboost format
XGB.data.q4Test <- xgb.DMatrix(data = data.matrix(XGB_q4_testX[, 1:ncol(XGB_q4_testX)]))
XGB_pred_testQ4 <- predict(xgb.model, XGB.data.q4Test)
```

```{r, eval = FALSE}
test_y_rand <- cbind(test_y_rand, XGB_pred_testQ4)
test_y_rand <- test_y_rand[, c(1, 3)]
head(test_y_rand)
```

```{r, eval = FALSE}
write.csv(test_y_rand,"Q4Predictions.csv", row.names = FALSE)
```



## Question 5 (25 pts.)

Finally, let's work with the full MNIST data set.  This is a 10 class classification problem that has been studied extensively.  With your work on this problem, you will join the club of data scientists who have taken a crack at one of machine learning's most infamous classification tasks!

### Part 1 (15 pts.)

Use any tools in our classification arsenal to try to minimize the expected misclassification rate for out of sample handwritten digits.

In your answer, you should benchmark any algorithms that are suited for the problem.  For any classification methods that you know will not work well (by virtue of the structure of the data), justify why it's not worth the time to check it.  Be sure to explain your method for tuning any hyperparameters.

For any models you run, produce a table that shows the misclassification rate on the validation set.  Similarly, record the compute time needed to arrive at your final model (including any hyperparameter tuning) and include this in the same table.  See [this StackOverflow thread](https://stackoverflow.com/questions/6262203/measuring-function-execution-time-in-r) for an elementary way to do this in R.

Which model performs the best on the 10 class problem?  Why do you think this model performs the best?  Compare your approach to other possible approaches when answering this question.   

Is the tradeoff in accuracy vs. compute time worth the gain in realistic scenarios?  Think about how these algorithms might scale as $N$ and $P$ get larger.  Specifically, discuss the problem of hyperparameter tuning and how this might contribute to difficulty in implementing your chosen approach.

For one-off classification problems, the computational time may not matter.  However, there is significant research into the area of **online classification algorithms** - algorithms in which new predictions can be made quickly using existing data and the classification algorithm can be updated using new training data as it arrives.  See [this Wikipedia page](https://en.wikipedia.org/wiki/Online_machine_learning) for more information on this topic.


### Solution

First of all, we don't think K-Nearest Neighbors would work well here because the curse of dimensionality hits KNN very hard. KNN relies on distance between data points to make prediction. The higher the dimension, the more likely that two data points are going to be farther apart. Therefore, we are not using KNN here.

```{r}
x.train <- read.csv("MNISTTrainX.csv")
y.train <- read.csv("MNISTTrainY.csv")
x.test  <- read.csv("MNISTValidationX.csv")
y.test  <- read.csv("MNISTValidationY.csv")

train <- data.frame(x.train, y.train)
test <- data.frame(x.test, y.test)


train$label <- train$label %>% as.factor()
test$label <- test$label %>% as.factor()

```

The first model we will try is Logistic Regression.

**Logistic Regression**

```{r, results = 'hide'}
library(nnet)

log.fit <- multinom(label ~., data = train, MaxNWts =10000000)

log.pred <- predict(log.fit, test, type = "class")

log.test.acc <- mean((log.pred ==  test$label))

log.time <- system.time(log.fit <- multinom(label ~., data = train, MaxNWts =10000000))
```

**LDA & QDA**

```{r}
library(MASS)

lda.fit <- lda(label ~ ., data = train)

lda.pred <- predict(lda.fit, test)$class

lda.test.acc <- mean((lda.pred ==  test$label))

qda.fit <- qda(label ~ ., data = train)

qda.pred <- predict(qda.fit, test)$class

qda.test.acc <- mean((qda.pred ==  test$label))

lda.time <- system.time(lda.fit <- lda(label ~ ., data = train))
qda.time <- system.time(qda.fit <- qda(label ~ ., data = train))
```

**Tree Based Methods**

**Random Forest**

```{r, results='hide'}
library(ranger)

rf.fit <- ranger(label~., data = train, classification = T)

rf.pred <- predict(rf.fit, test)$predictions

rf.test.acc <- mean((rf.pred == test$label))

rf.time <- system.time(rf.fit <- ranger(label~., data = train, classification = T))
```

**XGBoost**

```{r, results='hide'}
library(xgboost)

x.train.xgb <- read.csv("MNISTTrainX.csv")
y.train.xgb <- read.csv("MNISTTrainY.csv")
x.test.xgb <- read.csv("MNISTValidationX.csv")
y.test.xgb <- read.csv("MNISTValidationY.csv")

train.xgb <- data.frame(x.train.xgb, y.train.xgb)
test.xgb <- data.frame(x.test.xgb, y.test.xgb)

train.xgb <- as.data.frame(lapply(train.xgb, as.numeric))
test.xgb <- as.data.frame(lapply(test.xgb, as.numeric))

train.xgb <- xgb.DMatrix(data = data.matrix(train.xgb[,1:144]),
                         label = train.xgb$label)

test.xgb <- xgb.DMatrix(data = data.matrix(test.xgb[,1:144]),
                         label = test.xgb$label)

watch.list <- list(train = train.xgb, test = test.xgb)

xgb.ps <- list(
  booster = "gbtree",
  objective = "multi:softmax",
  eval_metric = "merror",
  num_class = 11
)

xgb.fit <- xgb.train(xgb.ps, train.xgb, nrounds = 10, watchlist = watch.list)

xgb.pred <- predict(xgb.fit, newdata = test.xgb)

xgb.test.acc <- mean(xgb.pred == test$label)

xgb.time <- system.time(xgb.fit <- xgb.train(xgb.ps, train.xgb, nrounds = 10, watchlist = watch.list))
```

**SVM**

```{r,message=FALSE, results='hide'}
library(kernlab)
library(mlr)


sub.train <- train %>% group_by(as.factor(label)) %>% sample_n(1000) #subsetting the data to save time
sub.train <- sub.train[,1:145]

# Use CV to choose a Cost

MNIST.task <- makeClassifTask(id = "MNIST", data = sub.train, target = "label")

discret.ps <- makeParamSet(makeDiscreteParam("C", values = c(.001,.01,.1,1,10,100,1000)))

ctrl <- makeTuneControlGrid()

rdesc <- makeResampleDesc("CV", iters = 5L)

res <- tuneParams("classif.ksvm", 
                  task = MNIST.task, 
                  resampling = rdesc, 
                  par.set = discret.ps, 
                  control = ctrl)

svm.time1 <- 
  system.time(res <- tuneParams("classif.ksvm", 
                                task = MNIST.task, 
                                resampling = rdesc, 
                                par.set = discret.ps, 
                                control = ctrl))

# Use the best Cost to train SVM. (ksvm function has a default for estimating the best gamma)

svm2.fit <- ksvm(label ~ ., data = sub.train, kernel = "rbfdot", kpar = "automatic", C = 100) #using radial kernel

svm2.pred <- predict(svm2.fit, test)

svm.test.acc <- mean((svm2.pred ==  test$label))

svm.time2 <- system.time(svm2.fit <- ksvm(label ~ ., data = sub.train, kernel = "rbfdot", kpar = "automatic", C = 100))
```


Now we compare all the model and the time it takes to train them

```{r}
accuracy <- c(log.test.acc, lda.test.acc, qda.test.acc, rf.test.acc, xgb.test.acc, svm.test.acc)
svm.time <- svm.time1[3]+svm.time2[3]
time <- c(log.time[3], lda.time[3], qda.time[3], rf.time[3],xgb.time[3], svm.time)
models <- c("Logistic", "LDA", "QDA", "Random Forest", "Extreme Gradient Boost", "SVM")

data.frame(models,accuracy, time)
```

There are more models, such as stacked classifier, that we didn't try for the 10-class problem because we want to save time. Random Forest and SVM seem to be performing best here. We think this might be because of the non-linearity of the decision boundary. Random Forest with recommended hyperparameters requires significantly less time than a tuned SVM with radial kernel while having even similar accuracy. For these reasons, we will use Random Forest for Part 3 of this problem. 


### Part 2 (5 pts.)

For your final model, compute a **confusion matrix** for the validation set that shows how often each digit is classified in each class.  Which incorrect classifications happen most frequently? 

For the most commonly confused digit pairs, plot examples that are misclassified.  What aspect of these images led to their misclassification?

The MNIST data as presented to you has been simplified in some ways.  There are also other data cleaning tasks that can improve predictive accuracy.  What are some steps that could be taken in the data cleaning stage that would help your chosen method improve its misclassification rate?


```{r}
table(test$label, rf.pred)
```

From this confusion matrix, we see that the most confused pair is 4 and 9.

```{r}
confused.test <- test %>% mutate(pred = rf.pred) %>% filter(label!=pred)

confused_4_9 <- confused.test %>% filter((label == 4 & pred == 9)| (label == 9 & pred == 4))

sample_4 <- subset(confused_4_9, label == 4)
sample_9 <- subset(confused_4_9, label == 9)

sample_pair <- rbind(sample_4[3,], sample_9[4,])

sample_pair_y <- sample_pair[145]

sample_pair_x <- sample_pair[1:144]

par(mfrow = c(1,2))
for (i in 1:2){
  label = sample_pair_y[i, 1]
  plot_digit(sample_pair_x[i,], bw = FALSE, main = paste("Predict label:",sample_pair[i,146],"label: ", sample_pair[i,145]))
}

```

These two look very much alike. We think this might be because "9" doesn't have a curve towards the end and the upper part where there should be a circle looks like a triangle.

We think we can improve prediction with some unsupervised methods to control the noise and reduce the dimension of feature space. A method we can use is clustering. Using k-centroid clustering, we can identify some groups of numbers. People write numbers such as "2" in ways that may seem significantly different to a machine.

### Part 3 (5 pts.)

Use the hidden test set to generate a prediction for each included image.  Your predictions should be stored in a matrix that has the image key as the first column and the integer value of the class in the second column.  Save this matrix as `Q5Predictions.csv` and include it with your final submission.

Points for this question will be given with respect to classification accuracy.  Let $E_i$ be the proportion of observations in the test set misclassified.  Let $E_{min}$ be the minimum proportion of misclassified observations across the class.  Then, you final point total for this part will be:

$$5 \times \frac{E_{max}}{E_i}$$

```{r}
x.test2 <- read.csv("MNISTTestXRand.csv")
y.test2 <- read.csv("MNISTTestYRand.csv")

Rand.pred <- predict(rf.fit, x.test2)$predictions

write.csv(Rand.pred, "Q5Predictions.csv",row.names = FALSE)
```






  

