# convert data to xgboost format
XGB.data.train <- xgb.DMatrix(data = data.matrix(XGB_train[, 2:ncol(train)]), label = XGB_train$label)
XGB_train[, 2:ncol(train)]
XGB_train[, 2:ncol(XGB_train)]
XGB_train[, 1:ncol(XGB_train)]
length(XGB_train[, 1:ncol(XGB_train)])
length(XGB_train[, 1:ncol(XGB_train) - 1])
XGB_train[, 1:ncol(XGB_train) - 1]
XGB.data.train <- xgb.DMatrix(data = data.matrix(XGB_train[, 1:ncol(train) - 1]), label = XGB_train$label)
XGB.data.train <- xgb.DMatrix(data = data.matrix(XGB_train[, 1:ncol(XGB_train) - 1]), label = XGB_train$label)
test <- XGB_train[, 1:ncol(XGB_train) - 1]
View(test)
# convert data to xgboost format
XGB.data.train <- xgb.DMatrix(data = data.matrix(XGB_train[, 1:ncol(XGB_train) - 1]),
label = XGB_train$label)
XGB.data.cv <- xgb.DMatrix(data = data.matrix(XGB_cv[, 2:ncol(XGB_cv)]),
label = XGB_cv$label)
watchlist <- list(train  = data.train, test = data.cv)
watchlist <- list(train  = XGB.data.train, test = XGB.data.cv)
parameters <- list(
# General Parameters
booster            = "gbtree",          # default = "gbtree"
silent             = 0,                 # default = 0
# Booster Parameters
eta                = 0.3,               # default = 0.3, range: [0,1]
gamma              = 0,                 # default = 0,   range: [0,∞]
max_depth          = 6,                 # default = 6,   range: [1,∞]
min_child_weight   = 1,                 # default = 1,   range: [0,∞]
subsample          = 1,                 # default = 1,   range: (0,1]
colsample_bytree   = 1,                 # default = 1,   range: (0,1]
colsample_bylevel  = 1,                 # default = 1,  range: (0,1]
lambda             = 1,                 # default = 1
alpha              = 0,                 # default = 0
# Task Parameters
objective          = "multi:softmax",   # default = "reg:linear"
eval_metric        = "merror",
num_class          = 10,
seed               = 1234               # reproducability seed
)
xgb.model <- xgb.train(parameters, data.train, nrounds = 10, watchlist)
xgb.model <- xgb.train(parameters, XBG.data.train, nrounds = 10, watchlist)
xgb.model <- xgb.train(parameters, XGB.data.train, nrounds = 10, watchlist)
XGB_pred <- predict(xgb.model, test_x_3)
XGB_pred <- predict(xgb.model, XGB.data.cv)
XGB.data.cv <- xgb.DMatrix(data = data.matrix(XGB_cv[, 1:ncol(XGB_cv) - 1]),
label = XGB_cv$label)
XGB_pred <- predict(xgb.model, XGB.data.cv)
confmatrix <- table(XGB_pred, test_y_3$label)
XGB_pred <- predict(xgb.model, XGB.data.cv)
confmatrix <- table(XGB_pred, XGB_cv$label)
confmatrix
print(paste("Misclassified rate on validation set: ", round(1 - sum(diag(confmatrix)) / sum(confmatrix), 5)))
GB_train <- train_xy_3
XGB_cv <- test_x_3
XGB_train <- train_xy_3
XGB_cv <- test_x_3
XGB_train <- as.data.frame(lapply(XGB_train, as.numeric))
XGB_cv <- as.data.frame(lapply(XGB_cv, as.numeric))
XGB.data.train <- xgb.DMatrix(data = data.matrix(XGB_train[, 1:ncol(XGB_train) - 1]),
label = XGB_train$label)
XGB.data.cv <- xgb.DMatrix(data = data.matrix(XGB_cv[, 1:ncol(XGB_cv)]),
label = XGB_cv$label)
XGB_cv
length(XGB_cv)
length(XGB.data.train)
XGB_cv[, 1:ncol(XGB_cv)]
length(XGB_cv[, 1:ncol(XGB_cv)])
length(XGB_train[, 1:ncol(XGB_train) - 1])
XGB_cv <- cbind(test_x_3, test_y_3)
length(XGB_cv)
ength(XGB_cv[, 1:ncol(XGB_cv)])
length(XGB_cv[, 1:ncol(XGB_cv)])
length(XGB_train[, 1:ncol(XGB_train) - 1])
library(caTools)
library(xgboost)
#set.seed(385)
#split <- sample.split(train_xy_3$label, SplitRatio = 0.8)
XGB_train <- train_xy_3
XGB_cv <- cbind(test_x_3, test_y_3)
XGB_train <- as.data.frame(lapply(XGB_train, as.numeric))
XGB_cv <- as.data.frame(lapply(XGB_cv, as.numeric))
XGB.data.train <- xgb.DMatrix(data = data.matrix(XGB_train[, 1:ncol(XGB_train) - 1]),
label = XGB_train$label)
XGB.data.cv <- xgb.DMatrix(data = data.matrix(XGB_cv[, 1:ncol(XGB_cv)]),
label = XGB_cv$label)
watchlist <- list(train  = XGB.data.train, test = XGB.data.cv)
parameters <- list(
# General Parameters
booster            = "gbtree",          # default = "gbtree"
silent             = 0,                 # default = 0
# Booster Parameters
eta                = 0.3,               # default = 0.3, range: [0,1]
gamma              = 0,                 # default = 0,   range: [0,∞]
max_depth          = 6,                 # default = 6,   range: [1,∞]
min_child_weight   = 1,                 # default = 1,   range: [0,∞]
subsample          = 1,                 # default = 1,   range: (0,1]
colsample_bytree   = 1,                 # default = 1,   range: (0,1]
colsample_bylevel  = 1,                 # default = 1,  range: (0,1]
lambda             = 1,                 # default = 1
alpha              = 0,                 # default = 0
# Task Parameters
objective          = "multi:softmax",   # default = "reg:linear"
eval_metric        = "merror",
num_class          = 10,
seed               = 1234               # reproducability seed
)
xgb.model <- xgb.train(parameters, XGB.data.train, nrounds = 10, watchlist)
set.seed(385)
xgb.model <- xgb.train(parameters, XGB.data.train, nrounds = 10, watchlist)
library(caTools)
library(xgboost)
set.seed(385)
#split <- sample.split(train_xy_3$label, SplitRatio = 0.8)
XGB_train <- train_xy_3
XGB_cv <- cbind(test_x_3, test_y_3)
# convert every variable to numeric, even the integer variables
XGB_train <- as.data.frame(lapply(XGB_train, as.numeric))
XGB_cv <- as.data.frame(lapply(XGB_cv, as.numeric))
# convert data to xgboost format
XGB.data.train <- xgb.DMatrix(data = data.matrix(XGB_train[, 1:ncol(XGB_train) - 1]),
label = XGB_train$label)
XGB.data.cv <- xgb.DMatrix(data = data.matrix(XGB_cv[, 1:ncol(XGB_cv)]),
label = XGB_cv$label)
watchlist <- list(train  = XGB.data.train, test = XGB.data.cv)
parameters <- list(
# General Parameters
booster            = "gbtree",          # default = "gbtree"
silent             = 0,                 # default = 0
# Booster Parameters
eta                = 0.3,               # default = 0.3, range: [0,1]
gamma              = 0,                 # default = 0,   range: [0,∞]
max_depth          = 6,                 # default = 6,   range: [1,∞]
min_child_weight   = 1,                 # default = 1,   range: [0,∞]
subsample          = 1,                 # default = 1,   range: (0,1]
colsample_bytree   = 1,                 # default = 1,   range: (0,1]
colsample_bylevel  = 1,                 # default = 1,  range: (0,1]
lambda             = 1,                 # default = 1
alpha              = 0,                 # default = 0
# Task Parameters
objective          = "multi:softmax",   # default = "reg:linear"
eval_metric        = "merror",
num_class          = 10,
seed               = 1234               # reproducability seed
)
xgb.model <- xgb.train(parameters, XGB.data.train, nrounds = 10, watchlist)
library(caTools)
library(xgboost)
set.seed(385)
#split <- sample.split(train_xy_3$label, SplitRatio = 0.8)
XGB_train <- train_xy_3
XGB_cv <- cbind(test_x_3, test_y_3)
# convert every variable to numeric, even the integer variables
XGB_train <- as.data.frame(lapply(XGB_train, as.numeric))
XGB_cv <- as.data.frame(lapply(XGB_cv, as.numeric))
# convert data to xgboost format
XGB.data.train <- xgb.DMatrix(data = data.matrix(XGB_train[, 1:ncol(XGB_train) - 1]),
label = XGB_train$label)
XGB.data.cv <- xgb.DMatrix(data = data.matrix(XGB_cv[, 1:ncol(XGB_cv) - 1]),
label = XGB_cv$label)
watchlist <- list(train  = XGB.data.train, test = XGB.data.cv)
parameters <- list(
# General Parameters
booster            = "gbtree",          # default = "gbtree"
silent             = 0,                 # default = 0
# Booster Parameters
eta                = 0.3,               # default = 0.3, range: [0,1]
gamma              = 0,                 # default = 0,   range: [0,∞]
max_depth          = 6,                 # default = 6,   range: [1,∞]
min_child_weight   = 1,                 # default = 1,   range: [0,∞]
subsample          = 1,                 # default = 1,   range: (0,1]
colsample_bytree   = 1,                 # default = 1,   range: (0,1]
colsample_bylevel  = 1,                 # default = 1,  range: (0,1]
lambda             = 1,                 # default = 1
alpha              = 0,                 # default = 0
# Task Parameters
objective          = "multi:softmax",   # default = "reg:linear"
eval_metric        = "merror",
num_class          = 10,
seed               = 1234               # reproducability seed
)
xgb.model <- xgb.train(parameters, XGB.data.train, nrounds = 10, watchlist)
XGB_pred <- predict(xgb.model, XGB.data.cv)
confmatrix <- table(XGB_pred, XGB_cv$label)
confmatrix
print(paste("Misclassified rate on validation set: ", round(1 - sum(diag(confmatrix)) / sum(confmatrix), 5)))
library(caTools)
library(xgboost)
set.seed(385)
#split <- sample.split(train_xy_3$label, SplitRatio = 0.8)
XGB_train <- train_xy_3
XGB_cv <- cbind(test_x_3, test_y_3)
# convert every variable to numeric, even the integer variables
XGB_train <- as.data.frame(lapply(XGB_train, as.numeric))
XGB_cv <- as.data.frame(lapply(XGB_cv, as.numeric))
# convert data to xgboost format
XGB.data.train <- xgb.DMatrix(data = data.matrix(XGB_train[, 1:ncol(XGB_train) - 1]),
label = XGB_train$label)
XGB.data.cv <- xgb.DMatrix(data = data.matrix(XGB_cv[, 1:ncol(XGB_cv) - 1]),
label = XGB_cv$label)
watchlist <- list(train  = XGB.data.train, test = XGB.data.cv)
parameters <- list(
# General Parameters
booster            = "gbtree",          # default = "gbtree"
silent             = 0,                 # default = 0
# Booster Parameters
eta                = 0.05,               # default = 0.3, range: [0,1]
gamma              = 0,                 # default = 0,   range: [0,∞]
max_depth          = 5,                 # default = 6,   range: [1,∞]
min_child_weight   = 1,                 # default = 1,   range: [0,∞]
subsample          = 0.7,                 # default = 1,   range: (0,1]
colsample_bytree   = 0.95,                 # default = 1,   range: (0,1]
colsample_bylevel  = 1,                 # default = 1,  range: (0,1]
lambda             = 0,                 # default = 1
alpha              = 0,                 # default = 0
# Task Parameters
objective          = "multi:softmax",   # default = "reg:linear"
eval_metric        = "merror",
num_class          = 10,
seed               = 1234               # reproducability seed
)
xgb.model <- xgb.train(parameters, XGB.data.train, nrounds = 10, watchlist)
XGB_pred <- predict(xgb.model, XGB.data.cv)
confmatrix <- table(XGB_pred, XGB_cv$label)
confmatrix
print(paste("Misclassified rate on validation set: ", round(1 - sum(diag(confmatrix)) / sum(confmatrix), 5)))
library(caTools)
library(xgboost)
set.seed(385)
#split <- sample.split(train_xy_3$label, SplitRatio = 0.8)
XGB_train <- train_xy_3
XGB_cv <- cbind(test_x_3, test_y_3)
# convert every variable to numeric, even the integer variables
XGB_train <- as.data.frame(lapply(XGB_train, as.numeric))
XGB_cv <- as.data.frame(lapply(XGB_cv, as.numeric))
# convert data to xgboost format
XGB.data.train <- xgb.DMatrix(data = data.matrix(XGB_train[, 1:ncol(XGB_train) - 1]),
label = XGB_train$label)
XGB.data.cv <- xgb.DMatrix(data = data.matrix(XGB_cv[, 1:ncol(XGB_cv) - 1]),
label = XGB_cv$label)
watchlist <- list(train  = XGB.data.train, test = XGB.data.cv)
parameters <- list(
# General Parameters
booster            = "gbtree",          # default = "gbtree"
silent             = 0,                 # default = 0
# Booster Parameters
eta                = 0.05,               # default = 0.3, range: [0,1]
gamma              = 0,                 # default = 0,   range: [0,∞]
max_depth          = 5,                 # default = 6,   range: [1,∞]
min_child_weight   = 1,                 # default = 1,   range: [0,∞]
subsample          = 0.7,                 # default = 1,   range: (0,1]
colsample_bytree   = 0.95,                 # default = 1,   range: (0,1]
colsample_bylevel  = 1,                 # default = 1,  range: (0,1]
lambda             = 1,                 # default = 1
alpha              = 0,                 # default = 0
# Task Parameters
objective          = "multi:softmax",   # default = "reg:linear"
eval_metric        = "merror",
num_class          = 10,
seed               = 1234               # reproducability seed
)
xgb.model <- xgb.train(parameters, XGB.data.train, nrounds = 10, watchlist)
XGB_pred <- predict(xgb.model, XGB.data.cv)
confmatrix <- table(XGB_pred, XGB_cv$label)
confmatrix
print(paste("Misclassified rate on validation set: ", round(1 - sum(diag(confmatrix)) / sum(confmatrix), 5)))
library(caTools)
library(xgboost)
set.seed(385)
#split <- sample.split(train_xy_3$label, SplitRatio = 0.8)
XGB_train <- train_xy_3
XGB_cv <- cbind(test_x_3, test_y_3)
# convert every variable to numeric, even the integer variables
XGB_train <- as.data.frame(lapply(XGB_train, as.numeric))
XGB_cv <- as.data.frame(lapply(XGB_cv, as.numeric))
# convert data to xgboost format
XGB.data.train <- xgb.DMatrix(data = data.matrix(XGB_train[, 1:ncol(XGB_train) - 1]),
label = XGB_train$label)
XGB.data.cv <- xgb.DMatrix(data = data.matrix(XGB_cv[, 1:ncol(XGB_cv) - 1]),
label = XGB_cv$label)
watchlist <- list(train  = XGB.data.train, test = XGB.data.cv)
parameters <- list(
# General Parameters
booster            = "gbtree",          # default = "gbtree"
silent             = 0,                 # default = 0
# Booster Parameters
eta                = 0.3,               # default = 0.3, range: [0,1]
gamma              = 0,                 # default = 0,   range: [0,∞]
max_depth          = 6,                 # default = 6,   range: [1,∞]
min_child_weight   = 1,                 # default = 1,   range: [0,∞]
subsample          = 1,                 # default = 1,   range: (0,1]
colsample_bytree   = 1,                 # default = 1,   range: (0,1]
colsample_bylevel  = 1,                 # default = 1,   range: (0,1]
lambda             = 1,                 # default = 1
alpha              = 0,                 # default = 0
# Task Parameters
objective          = "multi:softmax",   # default = "reg:linear"
eval_metric        = "merror",
num_class          = 10,
seed               = 1234               # reproducability seed
)
xgb.model <- xgb.train(parameters, XGB.data.train, nrounds = 10, watchlist)
library(caTools)
library(xgboost)
set.seed(385)
#split <- sample.split(train_xy_3$label, SplitRatio = 0.8)
XGB_train <- train_xy_3
XGB_cv <- cbind(test_x_3, test_y_3)
# convert every variable to numeric, even the integer variables
XGB_train <- as.data.frame(lapply(XGB_train, as.numeric))
XGB_cv <- as.data.frame(lapply(XGB_cv, as.numeric))
# convert data to xgboost format
XGB.data.train <- xgb.DMatrix(data = data.matrix(XGB_train[, 1:ncol(XGB_train) - 1]),
label = XGB_train$label)
XGB.data.cv <- xgb.DMatrix(data = data.matrix(XGB_cv[, 1:ncol(XGB_cv) - 1]),
label = XGB_cv$label)
watchlist <- list(train  = XGB.data.train, test = XGB.data.cv)
parameters <- list(
# General Parameters
booster            = "gbtree",          # default = "gbtree"
silent             = 0,                 # default = 0
# Booster Parameters
eta                = 0.3,               # default = 0.3, range: [0,1]
gamma              = 0,                 # default = 0,   range: [0,∞]
max_depth          = 6,                 # default = 6,   range: [1,∞]
min_child_weight   = 1,                 # default = 1,   range: [0,∞]
subsample          = 1,                 # default = 1,   range: (0,1]
colsample_bytree   = 1,                 # default = 1,   range: (0,1]
colsample_bylevel  = 1,                 # default = 1,   range: (0,1]
lambda             = 1,                 # default = 1
alpha              = 0,                 # default = 0
# Task Parameters
objective          = "multi:softmax",   # default = "reg:linear"
eval_metric        = "merror",
num_class          = 10,
seed               = 1234               # reproducability seed
)
xgb.model <- xgb.train(parameters, XGB.data.train, nrounds = 10, watchlist)
XGB_pred <- predict(xgb.model, XGB.data.cv)
confmatrix <- table(XGB_pred, XGB_cv$label)
confmatrix
print(paste("Misclassified rate on validation set: ", round(1 - sum(diag(confmatrix)) / sum(confmatrix), 5)))
# loading the data
setwd("/Users/zejiachen/Desktop/Sspring 2022/Statstical Learning/DataAnalysis#2")
rm(list=ls(all=TRUE))
train_x <- fread("MNISTTrainX.csv")
library(ggplot2)
library(data.table)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
# loading the data
setwd("/Users/zejiachen/Desktop/Sspring 2022/Statstical Learning/DataAnalysis#2")
rm(list=ls(all=TRUE))
train_x <- fread("MNISTTrainX.csv")
train_y <- fread("MNISTTrainY.csv")
validate_x <- fread("MNISTValidationX.csv")
validate_y <- fread("MNISTValidationY.csv")
test_x <- fread("MNISTTestXRand.csv")
test_y <- fread("MNISTTestYRand.csv")
plot_digit <- function(x, bw = FALSE,...){
if(sqrt(length(x)) != round(sqrt(length(x)))){
stop(print("Not a square image! Something is wrong here."))
}
n <- sqrt(length(x))
if(bw == TRUE){
x <- as.numeric(x > 50)*256
}
par(pty = "s")
image(matrix(as.matrix(x), nrow = n)[,n:1], col = gray(12:1 / 12), ...)
}
sample_9 <- sample(1:25000, 9)
par(mfrow = c(3,3))
for (i in sample_9){
label = train_y[i, 1]
plot_digit(train_x[i,], bw = FALSE, main = paste("label: ", label))
}
par(mfrow = c(1,1))
train_x <- fread("MNISTTrainX.csv")
train_y <- fread("MNISTTrainY.csv")
index_3 <- c()
for (i in c(3, 5, 8)){
index_3 <- append(index_3, which(train_y$label == i))
}
train_x_3 <- train_x[index_3,]
train_y_3 <- train_y[index_3,]
train_xy_3 <- cbind(train_x_3, train_y_3)
train_xy_3$label <- as.factor(train_xy_3$label)
index_3 <- c()
for (i in c(3, 5, 8)){
index_3 <- append(index_3, which(validate_y$label == i))
}
test_x_3 <- validate_x[index_3,]
test_y_3 <- validate_y[index_3,]
library(caTools)
library(xgboost)
set.seed(385)
#split <- sample.split(train_xy_3$label, SplitRatio = 0.8)
XGB_train <- train_xy_3
XGB_cv <- cbind(test_x_3, test_y_3)
# convert every variable to numeric, even the integer variables
XGB_train <- as.data.frame(lapply(XGB_train, as.numeric))
XGB_cv <- as.data.frame(lapply(XGB_cv, as.numeric))
# convert data to xgboost format
XGB.data.train <- xgb.DMatrix(data = data.matrix(XGB_train[, 1:ncol(XGB_train) - 1]),
label = XGB_train$label)
XGB.data.cv <- xgb.DMatrix(data = data.matrix(XGB_cv[, 1:ncol(XGB_cv) - 1]),
label = XGB_cv$label)
watchlist <- list(train  = XGB.data.train, test = XGB.data.cv)
parameters <- list(
# General Parameters
booster            = "gbtree",          # default = "gbtree"
silent             = 0,                 # default = 0
# Booster Parameters
eta                = 0.3,               # default = 0.3, range: [0,1]
gamma              = 0,                 # default = 0,   range: [0,∞]
max_depth          = 6,                 # default = 6,   range: [1,∞]
min_child_weight   = 1,                 # default = 1,   range: [0,∞]
subsample          = 1,                 # default = 1,   range: (0,1]
colsample_bytree   = 1,                 # default = 1,   range: (0,1]
colsample_bylevel  = 1,                 # default = 1,   range: (0,1]
lambda             = 1,                 # default = 1
alpha              = 0,                 # default = 0
# Task Parameters
objective          = "multi:softmax",   # default = "reg:linear"
eval_metric        = "merror",
num_class          = 10,
seed               = 1234               # reproducability seed
)
xgb.model <- xgb.train(parameters, XGB.data.train, nrounds = 10, watchlist)
XGB_pred <- predict(xgb.model, XGB.data.cv)
confmatrix <- table(XGB_pred, XGB_cv$label)
confmatrix
print(paste("Misclassified rate on validation set: ", round(1 - sum(diag(confmatrix)) / sum(confmatrix), 5)))
library(ggplot2)
library(data.table)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
# loading the data
setwd("/Users/zejiachen/Desktop/Sspring 2022/Statstical Learning/DataAnalysis#2")
rm(list=ls(all=TRUE))
train_x <- fread("MNISTTrainX.csv")
train_y <- fread("MNISTTrainY.csv")
validate_x <- fread("MNISTValidationX.csv")
validate_y <- fread("MNISTValidationY.csv")
test_x <- fread("MNISTTestXRand.csv")
test_y <- fread("MNISTTestYRand.csv")
plot_digit <- function(x, bw = FALSE,...){
if(sqrt(length(x)) != round(sqrt(length(x)))){
stop(print("Not a square image! Something is wrong here."))
}
n <- sqrt(length(x))
if(bw == TRUE){
x <- as.numeric(x > 50)*256
}
par(pty = "s")
image(matrix(as.matrix(x), nrow = n)[,n:1], col = gray(12:1 / 12), ...)
}
sample_9 <- sample(1:25000, 9)
par(mfrow = c(3,3))
for (i in sample_9){
label = train_y[i, 1]
plot_digit(train_x[i,], bw = FALSE, main = paste("label: ", label))
}
par(mfrow = c(1,1))
meanPixel <- data.frame(matrix(nrow=10,ncol=144))
for (i in 0:10){
index <- which(train_y$label == i)
subsets <- as.matrix(train_x[index,])
for (j in 1:144){
meanPixel[i + 1,j] = mean(subsets[,j])
}
}
head(meanPixel[,1:10])
par(mfrow = c(3,2))
for (i in 1:10){
plot_digit(meanPixel[i,], bw = FALSE, main = paste("digit:", i - 1))
}
par(mfrow = c(1,2))
index_0_1 <- append(which(train_y$label == 0), which(train_y$label == 1))
train_x_0_1 <- train_x[index_0_1,]
train_y_0_1 <- train_y[index_0_1,]
train_xy_0_1 <- cbind(train_x_0_1, train_y_0_1)
train_xy_0_1$label <- as.factor(train_xy_0_1$label)
index_0_1 <- append(which(validate_y$label == 0), which(validate_y$label == 1))
test_x_0_1 <- validate_x[index_0_1,]
test_y_0_1 <- validate_y[index_0_1,]
log.fit<-glm(label~.,data=train_xy_0_1,family=binomial, control = list(maxit = 100))
## Accuracy
pred <- predict(log.fit, train_x_0_1, type = "response")
confmatrix <- table(observed = train_y_0_1$label, predict = pred > 0.5)
confmatrix
print(paste("misclassification rate: ", (confmatrix[1,2] + confmatrix[2,1]) / sum(confmatrix)))
misclass_cv_err <- c()
#Randomly shuffle the data
set.seed(123)
rand_df<-data.frame(train_xy_0_1[sample(nrow(train_xy_0_1)),])
#Create n folds equally size folds
n_folds <- cut(seq(1,nrow(rand_df)),breaks=10,labels=FALSE)
# perform the k-fold cross validation
for (i in 1:10){
#Segment the data by fold using the which() function
index <- which(n_folds == i, arr.ind = T)
test_data <- rand_df[index,]
train_data <- rand_df[-index,]
y_name = names(rand_df)[145]
x_name = names(rand_df)[-145]
test_x <- test_data[, x_name]
test_y <- test_data[, y_name]
# construct the model and predict the outcome
log.fit<-glm(label~.,data=train_data,family=binomial, control = list(maxit = 100))
pred_cv = predict(log.fit, newdata=test_x, type = "response")
# get the misclassfied error
confmatrix <- table(observed = test_y, predict = pred_cv > 0.5)
misclass_cv_err[i] <- (confmatrix[1,2] + confmatrix[2,1]) / sum(confmatrix)
}
