---
title: "Midterm Project"
author: "Zejia Chen, Francis Lin, Xuanyang Lin"
date: "February 16th, 2022"
output:
  pdf_document:
    toc: no
    toc_depth: '2'
  prettydoc::html_pretty:
    df_print: kable
    theme: leonids
    highlight: github
    toc: no
    toc_depth: 2
    toc_float:
      collapsed: no
urlcolor: blue
---


```{r, include=FALSE, message=F}
library(ggplot2)
library(dplyr)
library(data.table)
library(glmnet)
library(plotmo)
library(caret)
library(vip)
library(randomForest)
library(ModelMetrics)
library(mgcv)
library(caTools)
library(stringr)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
```


# Problem 1
```{r}
df_train <- read.csv("midterm1_train.csv")
df_proto.test <- read.csv("midterm1_testProto.csv")
```

Data Cleaning
```{r}
# take of log of `share`
df_train$shares = log(df_train$shares)
df_proto.test$shares = log(df_proto.test$shares)
```

```{r}
# adding the `viral content category` variable

df_train$data_channel_is_viral = df_train$data_channel_is_lifestyle

for (i in 1:nrow(df_train)){
  if (df_train[i, "data_channel_is_lifestyle"] == 0 &
      df_train[i, "data_channel_is_entertainment"] == 0 &
      df_train[i, "data_channel_is_bus"] == 0 &
      df_train[i, "data_channel_is_socmed"] == 0 &
      df_train[i, "data_channel_is_tech"] == 0 &
      df_train[i, "data_channel_is_world"] == 0){
    
        df_train[i, "data_channel_is_viral"] = 1
        
  } else{
        df_train[i, "data_channel_is_viral"] = 0
      }
}

df_train$data_channel_is_viral <- as.integer(df_train$data_channel_is_viral)

```

```{r}
df_proto.test$data_channel_is_viral = df_proto.test$data_channel_is_lifestyle

for (i in 1:nrow(df_proto.test)){
  if (df_proto.test[i, "data_channel_is_lifestyle"] == 0 &
      df_proto.test[i, "data_channel_is_entertainment"] == 0 &
      df_proto.test[i, "data_channel_is_bus"] == 0 &
      df_proto.test[i, "data_channel_is_socmed"] == 0 &
      df_proto.test[i, "data_channel_is_tech"] == 0 &
      df_proto.test[i, "data_channel_is_world"] == 0){
    
        df_proto.test[i, "data_channel_is_viral"] = 1
        
  } else{
        df_proto.test[i, "data_channel_is_viral"] = 0
      }
}

df_proto.test$data_channel_is_viral <- as.integer(df_proto.test$data_channel_is_viral)
```


```{r}
# dropping variables that might cause collinearity issue

df_train <- subset(df_train, select = -c(is_weekend, LDA_04, url))

df_proto.test <- subset(df_proto.test, select = -c(is_weekend, LDA_04, url))

```

Train-test Split
```{r}
set.seed(1234)

ind <- sample(2, nrow(df_train), replace = T, prob = c(0.7, 0.3))
df_train_sample <- df_train[ind == 1, ]
df_test_sample <- df_train[ind == 2, ]
```


---
## Model Testing
```{r, message=F}
library(caret)
library(glmnet)
library(dplyr)
```


**Multiple Linear Regression**

`Coefficients: (2 not defined because of singularities)` ??

```{r}
lm_fit <- lm(shares ~ ., data = df_train_sample)
```


```{r}
coef_df <- data.frame(lm_fit$coefficients)
colnames(coef_df)[1] <- "coef"
coef_df$coef <- round(coef_df$coef, 3)

coef_df %>% arrange(desc(coef)) %>%top_n(12)
```


**KNN**

Error: `#Error: vector memory exhausted (limit reached?)`

```{r}
trControl <- trainControl(method = "cv", number = 10)
 
set.seed(1234)
 
knn_fit <- train(shares ~., data = df_train_sample, method = 'knn',
              trControl = trControl)
```

```{r}
knn_fit

plot(knn_fit)
```

```{r}
knn_importance <- varImp(knn_fit)

knn_importance <- data.frame(knn_importance[1])

knn_importance %>% arrange(desc(knn_importance)) %>%top_n(12)
```




**LASSO**

```{r}
trn_x <- as.matrix(df_train_sample[,-58])
trn_y <- as.matrix(df_train_sample$shares)
```


```{r}
fit_cv_LASSO<- cv.glmnet(trn_x, trn_y, type.measure = "mse", alpha = 1, family = "gaussian", nfolds = 10)

plot(fit_cv_LASSO)
```


Let use `1se` to retrieve our ‘significant’ predictors, since it minimize the number of predictor. Since there 50+ predictors, we might want to eliminate as much predictors as possible, preventing any cases of overfitting so that we can reduce as much variance during the validation process. 
```{r}
coef_optim_LASSO <- coef(fit_cv_LASSO, s= fit_cv_LASSO$lambda.1se)
```

```{r}
theme_set(theme_bw())
coef_df <- as.data.frame(summary(coef_optim_LASSO))
col_names_df <- rbind(c("Intercept", 0),data.frame(colnames(trn_x)))
# saving all leftover significant predictors
coef_shrink <- col_names_df[c(coef_df$i),1]
coef_df <- cbind(coef_shrink, coef_df)
coef_df <- coef_df[,-c(2,3)]
colnames(coef_df)[1] <- "predictors"
colnames(coef_df)[2] <- "coef"
coef_df$coef <- round(coef_df$coef, 7)

coef_df %>% arrange(desc(coef)) %>% top_n(12)
coef_df %>% arrange(coef) %>% top_n(-10)

```

**GAM**
```{r,message=F}
library(mgcv)
```

We will use the top five most positive & negative predictors selected by LASSO.

```{r}
gam2 <- gam(shares ~ s(kw_avg_avg) + s(kw_min_avg) + s(LDA_02) + s(self_reference_avg_sharess) +
            data_channel_is_viral + s(global_subjectivity) + s(self_reference_min_shares) +
            s(self_reference_max_shares),
            data = df_train_sample)
plot(gam2, pages = 1)
```



```{r}
round(sqrt(summary(gam2)$sp.criterion), 5)
```



```{r}
gam1<-gam(shares~ s(global_subjectivity) + weekday_is_saturday + weekday_is_sunday + s(kw_min_avg)
          + data_channel_is_socmed + s(LDA_00) + s(rate_positive_words)
          + s(global_rate_negative_words)
          + s(global_rate_positive_words) + s(LDA_02) + min_positive_polarity
          +  data_channel_is_entertainment + s(global_sentiment_polarity)
          + s(avg_negative_polarity), data=df_train_sample)

plot(gam1, pages = 1)
```

```{r}
round(sqrt(summary(gam1)$sp.criterion), 5)
```


**Bagged Tree**
```{r}
library(ranger)
```

```{r}
bagged_tree <- ranger(shares ~ ., data = df_train_sample, mtry = ncol(df_train_sample) - 1, num.trees = 500)
```


**Random Forest**

```{r}
set.seed(1234)

rf_fit <- ranger(shares ~ ., data = df_train_sample, mtry = floor(sqrt(ncol(df_train) - 1)), num.trees = 500)
```


```{r}
# saving the model
saveRDS(rf_fit, "rf.rds")
```

Variable Importance
```{r}
rf_fit2 <- ranger(shares ~ ., data = df_train_sample, mtry = floor(sqrt(ncol(df_train) - 1)), 
                  num.trees = 500, importance = 'permutation')
rf_fit3 <- ranger(shares ~ ., data = df_train_sample, mtry = floor(sqrt(ncol(df_train) - 1)), 
                  num.trees = 500, importance = 'impurity')
```



```{r}
colnames <- colnames(df_train)
colnames <- colnames[-58]

rf_fit2_importance <- data.frame(rf_fit2$variable.importance)
rf_fit2_importance$Predictors <- colnames
colnames(rf_fit2_importance)[1] <- "Importance"
rf_fit2_top <- rf_fit2_importance %>% arrange(desc(rf_fit2_importance))
rf_fit2_top <- rf_fit2_top[1:15,]


rf_fit3_importance <- data.frame(rf_fit3$variable.importance)
rf_fit3_importance$Predictors <- colnames
colnames(rf_fit3_importance)[1] <- "Importance"
rf_fit3_top <- rf_fit3_importance %>% arrange(desc(rf_fit3_importance))
rf_fit3_top <- rf_fit3_top[1:15,]
```

```{r}
fit1 <- ggplot(rf_fit2_top, aes(x=reorder(Predictors,-Importance), y=Importance, label=round(Importance, 3))) + 
  geom_point(col="tomato2", size=3) +   # Draw points
  geom_segment(aes(x=Predictors, 
                   xend=Predictors, 
                   y=min(Importance), 
                   yend=max(Importance)), 
               linetype="dashed", 
               size=0.1) +   # Draw dashed lines
  labs(title="Variable Importance", y = "Importance", x = "Predictors") +  
  coord_flip()
```

```{r}
fit2 <- ggplot(rf_fit3_top, aes(x=reorder(Predictors,-Importance), y=Importance, label=round(Importance, 3))) + 
  geom_point(col="tomato2", size=3) +   # Draw points
  geom_segment(aes(x=Predictors, 
                   xend=Predictors, 
                   y=min(Importance), 
                   yend=max(Importance)), 
               linetype="dashed", 
               size=0.1) +   # Draw dashed lines
  labs(title="Variable Importance", y = "Importance", x = "Predictors") +  
  coord_flip()
```


```{r}
library("ggpubr")
ggarrange(fit1, fit2,
          labels = c("Permutation", "Impurity"))
```

**Boosted Regression Tree**
```{r, message=F}
library(gbm)
```

```{r, message=F}
set.seed(1234)

boost_tree_1 <- gbm(shares ~., data = df_train_sample, distribution = "gaussian", 
                    n.trees = 500, interaction.depth = 1, shrinkage = 0.005, bag.fraction = 1, cv.folds = 5)

boost_tree_2 <- gbm(shares ~., data = df_train_sample, distribution = "gaussian", 
                    n.trees = 500, interaction.depth = 2, shrinkage = 0.005, bag.fraction = 1, cv.folds = 5)
```

```{r}
# importance measure
boost_tree_1_importance <- data.frame(summary(boost_tree_1, plotit = FALSE))
boost_tree_1_importance %>% arrange(desc(boost_tree_1_importance$rel.inf)) %>% top_n(10)

boost_tree_2_importance <- data.frame(summary(boost_tree_2, plotit = FALSE))
boost_tree_2_importance %>% arrange(desc(boost_tree_2_importance$rel.inf)) %>% top_n(10)


# CV estimate of EPE at each step
plot(seq(1, 500), boost_tree_1$cv.error, type = "l")
plot(seq(1, 500), boost_tree_2$cv.error, type = "l")
```

```{r}
print(paste("boost_tree_1: ", boost_tree_1$cv.error[500]))
print(paste("boost_tree_2: ", boost_tree_2$cv.error[500]))

```


---
## Write Predict function

Since *Random Forest` has the best test set performance, we will use it as our team's Big Predictive Model

```{r}
q1_midterm1_predict <- function(train, test) {
  
  print("proprocessing the data...")
  
  # reshape the train set to fit the model
  train$shares = log(train$shares)
  test$shares = log(test$shares)
  
  # adding the `viral content category` variable
  train$data_channel_is_viral = train$data_channel_is_lifestyle
  for (i in 1:nrow(train)){
    if (train[i, "data_channel_is_lifestyle"] == 0 &
        train[i, "data_channel_is_entertainment"] == 0 &
        train[i, "data_channel_is_bus"] == 0 &
        train[i, "data_channel_is_socmed"] == 0 &
        train[i, "data_channel_is_tech"] == 0 &
        train[i, "data_channel_is_world"] == 0){
      
          train[i, "data_channel_is_viral"] = 1
          
    } else{
          train[i, "data_channel_is_viral"] = 0
        }
  }
  train$data_channel_is_viral <- as.integer(train$data_channel_is_viral)
  
  test$data_channel_is_viral = test$data_channel_is_lifestyle
  for (i in 1:nrow(test)){
    if (test[i, "data_channel_is_lifestyle"] == 0 &
        test[i, "data_channel_is_entertainment"] == 0 &
        test[i, "data_channel_is_bus"] == 0 &
        test[i, "data_channel_is_socmed"] == 0 &
        test[i, "data_channel_is_tech"] == 0 &
        test[i, "data_channel_is_world"] == 0){
      
          test[i, "data_channel_is_viral"] = 1
          
    } else{
          test[i, "data_channel_is_viral"] = 0
        }
  }
  test$data_channel_is_viral <- as.integer(test$data_channel_is_viral)
    
    # dropping variables
  train <- subset(train, select = -c(is_weekend, LDA_04, url))
  test <- subset(test, select = -c(is_weekend, LDA_04, url))
  
  # model training - Random Forest
  library(ranger)
  set.seed(1234)
  rf_fit <- ranger(shares ~ ., data = train, mtry = floor(sqrt(ncol(train) - 1)), num.trees = 500)
  
  pred = predict(rf_fit, test)
  return(pred$prediction)
  
}
```


## Testing
```{r}
setwd('/Users/mikelin/Desktop/QTM 385-6 Statistical Learning')
df_train <- read.csv("midterm1_train.csv")
df_proto.test <- read.csv("midterm1_testProto.csv")
```

```{r}
pred <- q1_midterm1_predict(df_train, df_proto.test)
```

```{r}
pred
```

# Problem 2

After evaluating a total of nine models, random forest, not surprisingly, becomes the best model to predict article popularity with the least RMSE.

However, such selection comes with a cost of interpretability. We don't really understand the effect that each predictor have on the article popularity except knowing the variable importance of the predictor.

Here comes the trade-off, precision vs. interpretability. Depending on the goal of the assignment, we may want to pivot to one versus the other. Nevertheless, in this midterm analysis, our ultimate goal is to build a precious model with one / a subset of predictors.

In that sense, it is apposite to use random forest in the first part and evaluate the variable importance to make the subsequent predictor selection process a little bit easier. 

According the random forest predictor evaluation,`kw_max_avg` [Avg. keyword (max. shares)], `self_reference_avg_sharess` [Avg. shares of referenced articles in Mashable], `LDA_02` [Closeness to LDA topic 2] stand out to be some of the most significant predictors for determining the Mashable article's popularity. In addition, variable importance evaluated by other models (LASSO, KNN, Boosted Random Forest) more or less agree to this result. 

From a granular perspective, we can somewhat deduce that there are three major factors that determine a article's popularity in Mashable. 

1. Avg. Keyword of the article
2. Shares of referenced articles
3. The topic of the article

And we can certainly start to get into the detail of these three area if we would love to investigate further about factors that are affecting Mashable article's popularity.

# Problem 3

For this problem we need to select no more than 5 predictors to include in the model. For convenience, we will only consider the model with exactly 5 predictors and not explore models with less than 5 predictors. We will use LASSO regression and random forest to select a list of top 5 most important predictors.

In the previous part, we have a cross validation object for lasso model, however, for this problem we need the original list of models because we need to observe which predictors' coefficients are regularized to 0 as $\lambda$ increases.

First, we build the large model that contains all the predictors and get the coefficients
```{r}
lasso.fit <- glmnet(trn_x, trn_y, alpha = 1)
lasso.coef <- predict(lasso.fit, type = "coefficient")
lasso.coef <- lasso.coef %>% as.matrix %>% as.data.frame()
plot(lasso.fit, xvar = "lambda")
```

```{r}
head(lasso.fit$lambda)
```

Note that `glmnet` orders $\lambda$ in descending order. Also note that once a coefficient has been shrunk to, it will stay at 0. Therefore, to get the list of predictor importance, we simply count the number of 0 values for each predictor. The predictors that have less 0 values are the ones that have their coefficients shrunk to 0 last, and therefore are the important predictors that we are looking for.

```{r}
lasso.coef[lasso.coef==0]<- NA ## set all 0 to NAs
tlasso.coef<-t(lasso.coef) %>% as.data.frame()
lasso.importance <-colSums(is.na(tlasso.coef)) %>% as.data.frame()


names(lasso.importance)<-"count"
lasso.importance<-arrange(lasso.importance, count)
```

Now we get a list of top 5 most important predictors
```{r}
lasso.select <- rownames(lasso.importance)[2:6] #do not include `intercept`

lasso.select
```

Next, we get the list of top 5 most important predictors from our previous random forest models. Here, we want to consider both impurity-based importance and permutation-based importance because impurity-based importance is computed within the training data and may not be generalization for prediction, which is our objective.

```{r}
rf.perm.select <- rf_fit2_importance %>% arrange(desc(Importance)) %>% head(5) %>% rownames()
rf.impurity.select <- rf_fit3_importance %>% arrange(desc(Importance)) %>% head(5) %>% rownames()
```

We can get a list of unique predictors across these 3 lists and get all possible combinations of these predictors.

```{r}
all.select <- unique(c(lasso.select,rf.perm.select,rf.impurity.select))
all.select
```

```{r}
list.select<- combn(all.select, 5)
```

Now we run the models. We will pick the model that has best performance by roost mean test error.

**LASSO regression**

```{r}
lasso.errors <- data.frame(index = seq(1,ncol(list.select),1),
                           rmse = vector(length = ncol(list.select)))

for (i in 1:ncol(list.select)){
  trn_x <- df_train_sample[,list.select[,i]] %>% as.matrix()
  tst_x <- df_test_sample[,list.select[,i]] %>% as.matrix()
  
  cv.lasso.fit <- cv.glmnet(trn_x, trn_y, alpha = 1)
  lasso.prediction <- predict(cv.lasso.fit, newx = tst_x, s = cv.lasso.fit$lambda.1se) %>% as.vector()
  
  lasso.errors[i,"rmse"] <- mse(pred = lasso.prediction, actual = df_test_sample$shares) %>% sqrt()
}

lasso.errors %>% arrange(rmse) %>% head()
```

We see that with lasso model, model #20 has the least root MSE. Before, storing this as the final model, we should also try other models.

**ridge regression**

```{r}
ridge.errors <- data.frame(index = seq(1,ncol(list.select),1),
                           rmse = vector(length = ncol(list.select)))

for (i in 1:ncol(list.select)){
  trn_x <- df_train_sample[,list.select[,i]] %>% as.matrix()
  tst_x <- df_test_sample[,list.select[,i]] %>% as.matrix()
  
  cv.ridge.fit <- cv.glmnet(trn_x, trn_y, alpha = 0)
  ridge.prediction <- predict(cv.ridge.fit, newx = tst_x, s = cv.ridge.fit$lambda.1se) %>% as.vector()
  
  ridge.errors[i,"rmse"] <- mse(pred = ridge.prediction, actual = df_test_sample$shares) %>% sqrt()
}

ridge.errors %>% arrange(rmse) %>% head()
```

**GAM**

```{r}
gam.errors <- data.frame(index = seq(1,ncol(list.select),1),
                           rmse = vector(length = ncol(list.select)))

for (i in 1:ncol(list.select)){
  trn <- df_train_sample[,c(list.select[,i], "shares")]
  
  tst <- df_test_sample[,c(list.select[,i], "shares")]
  
  formula <- as.formula(paste0("shares","~ ", paste(list.select[,i], collapse=" + ")))
  gam.fit <- gam(formula, data = trn)
  
  gam.prediction <- predict(gam.fit, newdata = tst, type = "response") %>% as.data.frame()
  gam.prediction <- gam.prediction[,1]
  gam.errors[i,"rmse"] <- mse(pred = gam.prediction, actual = df_test_sample$shares) %>% sqrt()
}

gam.errors %>% arrange(rmse) %>% head()
```

**GAM with spines**

Here we need to be careful because some predictors are factors

```{r}
df_train_sample[,all.select] %>% str()
```
We can see that `data_channel_is_viral` and `data_channel_is_entertainment` so we need to write an if-else statement to avoid them when applying spline to other predictors

```{r}
gam.s.errors <- data.frame(index = seq(1,ncol(list.select),1),
                           rmse = vector(length = ncol(list.select)))

for (i in 1:ncol(list.select)){
  trn <- df_train_sample[,c(list.select[,i], "shares")]
  tst <- df_test_sample[,c(list.select[,i], "shares")]
  
  if("data_channel_is_viral" %in% list.select[,i]==T & 
     "data_channel_is_entertainment" %in% list.select[,i]==T){
    
        non.factors1 <- list.select[,i] %>% str_subset("data_channel_is_viral",negate = T) %>% 
                                            str_subset("data_channel_is_entertainment",negate = T)
      
        formula <- as.formula(paste0("shares","~ ", 
                                   paste(paste0("s(",non.factors1,")"), collapse=" + "),
                                   "+data_channel_is_viral+data_channel_is_entertainment" ))
      
    } else if("data_channel_is_viral" %in% list.select[,i]==T & 
              "data_channel_is_entertainment" %in% list.select[,i]==F) {
        non.factors2 <- list.select[,i] %>% str_subset("data_channel_is_viral",negate = T)
        formula <- as.formula(paste0("shares","~ ", 
                                     paste(paste0("s(",non.factors2,")"),collapse=" + "),
                                     "+data_channel_is_viral"))
    }
      else if("data_channel_is_viral" %in% list.select[,i]==F & 
              "data_channel_is_entertainment" %in% list.select[,i]==T) {
        non.factors3 <- list.select[,i] %>% str_subset("data_channel_is_entertainment",negate = T)
        formula <- as.formula(paste0("shares","~ ", 
                                     paste(paste0("s(",non.factors3,")"),collapse=" + "),
                                     "+data_channel_is_entertainment"))
      
  }  else{
        formula <- as.formula(paste0("shares","~ ", paste(paste0("s(",list.select[,i],")"), collapse=" + ")))
  }
  
  gam.s.fit <- gam(formula, data=trn)
  
  gam.s.prediction <- predict(gam.s.fit, newdata = tst, type = "response") %>% as.data.frame()
  gam.s.prediction <- gam.s.prediction[,1]
  gam.s.errors[i,"rmse"] <- mse(pred = gam.s.prediction, actual = df_test_sample$shares) %>% sqrt()
}

gam.s.errors %>% arrange(rmse) %>% head()
```

Now we compare across all the best performing models

```{r}
model.options <- data.frame(models = c("LASSO","Ridge","GAM","GAM w/splines"),
                           rmse = c(arrange(lasso.errors, rmse)[1,2],
                                    arrange(ridge.errors, rmse)[1,2],
                                    arrange(gam.errors, rmse)[1,2],
                                    arrange(gam.s.errors, rmse)[1,2]))
model.options %>% arrange(rmse)
```

Judging by this table, GAM model with splines produces the least root MSE.

Now we trace it back to get the model. From the **GAM with spines** section, we can see that the 23th combination of the predictors gives the least root MSE

```{r}
final.select <- list.select[,23]
final.select
```

Get the model and note that `data_channel_is_entertainment` is a factor

```{r}

final.nonfactors <- final.select %>% str_subset("data_channel_is_entertainment", negate = T)
final.formula <- as.formula(paste0("shares","~ ", 
                                     paste(paste0("s(",final.nonfactors,")"),collapse=" + "),
                                     "+data_channel_is_entertainment"))
final.model <- gam(final.formula, data = df_train_sample)
```

Write the function that takes in a test data and outputs prediction

```{r}
q3_midterm1_predict <- function(train, test){
  # reshape training and test data
  library(dplyr)
  library(stringr)
  
  train$shares = log(train$shares)
  test$shares = log(test$shares)
  
  train <- subset(train, select = -c(is_weekend, LDA_04, url))
  test <- subset(test, select = -c(is_weekend, LDA_04, url))
  
  # load the 5 features that we found to be most useful
  features <- c("kw_avg_avg", "LDA_02", "data_channel_is_entertainment", "self_reference_avg_sharess", "kw_min_avg")
  
  non.fac.features <- features %>% str_subset("data_channel_is_entertainment",negate = T)
  # construct model
  library(mgcv)
  formula <- as.formula(paste0("shares","~ ", 
                                paste(paste0("s(",non.fac.features,")"),collapse=" + "),
                                "+data_channel_is_entertainment"))
  gam.model <- gam(formula, data = train)
  
  #compute predictions
  prediction <- predict(gam.model, newdata = test, type = "response") %>% as.data.frame()
  return(prediction[,1])
}
```

Test the prediction function

```{r, message=F}
q3_midterm1_predict(df_train, df_proto.test)
```

# Problem 4

```{r}
summary(final.model)
```

```{r}
plot(final.model, pages = 1)
```

The summary shows that all 5 predictors are significant at 0.001 level. The model tells us that if an article is shared on the Entertainment channel, it may get less shares. Neither `LDA_02` nor `kw_min_avg` display significant partial effects because the line is almost constant at 0. There is a peak at about 6000 for `kw_avg_avg`. For `self_reference_avg_shares`, the effect falls negative after 600000, and there appears to be a peak at 200000.

With this model, we expect an article that is not share on the Entertainment channel, has average keyword per share at around 6000, and has average shares of referenced articles on Mashable at 200000 to have a higher number of shares. Therefore, the actionable plan for Mashable is to edit and publish articles that fit the above statistics.

The change in predictor surface is a reduction in dimension. This is potentially helpful for prediction purpose because with high dimensional data, if we do not select features, there is a high chance of overfitting. In fact, the final model, by using less but also important predictors, has a `GCV` that is less than the GAM model constructed in Problem has.

```{r}
round(sqrt(summary(gam1)$sp.criterion), 5)
round(sqrt(summary(final.model)$sp.criterion), 5)
```

Finally, the desired statistics mentioned above cannot be viewed as a robust causal inference. First, we are using only 5 predictors out of 59, so immediately we might be ommiting some predictors that has a causal relationship with the number of shares. Secondly, machine learning recognizes patterns in the data, which is useful for description and prediction purposes but is insufficient to establish causality. The model does not look into what explains article sharing behavior, has a large human factor. Mashable should also listen to user feedback to improve article sharing.



## Question 5 - Building a Predictive Model with a Single Predictor (15 pts.)

Now, take the idea of creating a small actionable predictive model to the extreme and find the single predictor model that minimizes expected prediction error for log of shares on the unseen test data set.  Discuss your process and how you decided on a final model.  Create a plot that shows your predictive curve for the training data **and** for a test set that was not used to train the model (a hint about the process you should take for all of the questions).  How does the predictive surface for your single predictor compare to the equivalent impact of the predictor in the subset and full models?  Can you intuitively explain this result? 

Once you decide on a final model, write a function called `q5_midterm1_predict` that takes 4 arguments:

  1. `test` - a $N_{test} \times P$ matrix, data frame, or other type of reasonable object that includes the new set of 58 predictors (and potentially other covariates) included in your training data.  You should be able to pass the prototype test set included with this assignment to `test`.
  
  2. `train` - a $N_{train} \times P$ matrix, data frame, or other type of reasonable object that includes the set of 58 predictors (and potentially other covariates) included in your training data **or** a fitted model object.  If `train` is a matrix or data frame, you should be able to pass the entire training data in its first read form (as a data frame, for example) to `train`.  If `train` is a model object, please indicate through comments in your code which model object should be passed to the function.
  
  3. `seed` - a random seed that can be used to ensure that the fitting procedure is identical across function calls and computers.
  
Your function should train the model on `train` and produce $N_{Test}$ predictions for the `test` observations.  Your function should only return those predictions as a vector or equivalent data type.

Points for this question will be allocated as follows:

  1. 7 points for demonstrating a robust exploration over different possible methods of training the model considering both variable selection and potential nonlinearities.
  
  2. 3 points for a meaningful discussion of how you compared the potential models and ultimately chose your single predictor.
  
  3. 5 points for comparison of variable impact across models and discussion of logic for differences.



# Problem 5

The models that we are considering are linear regression, KNN, GAM and regression tree. However, since regression tree on a single predictor is overly sinple and very high in bias, the prediction error is expected to be very high. So, we exclude regression tree in our list. So, the models to consider are linear regression, GAM and KNN. In specific, linear regression fits a linear model, and GAM handles nonlinearity in the relationship between predictor and outcome.

We make an exhaustive search on our predictor space: trying out every possible predictors for each type of model (lm, KNN, GAM). We split the data into training and testing, and select the model based on cross validation estimates. We select the model with smallest CV estimate of prediction error.

```{r}

rm(list=ls(all=TRUE))
df_train <- read.csv("midterm1_train.csv")
df_test <- read.csv("midterm1_testProto.csv")
```

**Data Cleaning**
```{r}
# take of log of `share`
df_train$shares = log(df_train$shares)
df_test$shares = log(df_test$shares)
```

```{r}
# adding the `viral content category` variable
df_train$data_channel_is_viral = df_train$data_channel_is_lifestyle
for (i in 1:nrow(df_train)){
  if (df_train[i, "data_channel_is_lifestyle"] == 0 &
      df_train[i, "data_channel_is_entertainment"] == 0 &
      df_train[i, "data_channel_is_bus"] == 0 &
      df_train[i, "data_channel_is_socmed"] == 0 &
      df_train[i, "data_channel_is_tech"] == 0 &
      df_train[i, "data_channel_is_world"] == 0){
    
        df_train[i, "data_channel_is_viral"] = 1
        
  } else{
        df_train[i, "data_channel_is_viral"] = 0
      }
}
df_train$data_channel_is_viral <- as.integer(df_train$data_channel_is_viral)
```

```{r}
df_test$data_channel_is_viral = df_test$data_channel_is_lifestyle
for (i in 1:nrow(df_test)){
  if (df_test[i, "data_channel_is_lifestyle"] == 0 &
      df_test[i, "data_channel_is_entertainment"] == 0 &
      df_test[i, "data_channel_is_bus"] == 0 &
      df_test[i, "data_channel_is_socmed"] == 0 &
      df_test[i, "data_channel_is_tech"] == 0 &
      df_test[i, "data_channel_is_world"] == 0){
    
        df_test[i, "data_channel_is_viral"] = 1
        
  } else{
        df_test[i, "data_channel_is_viral"] = 0
      }
}
df_test$data_channel_is_viral <- as.integer(df_test$data_channel_is_viral)
```


```{r}
# dropping variables that might cause collinearity issue
df_train <- subset(df_train, select = -c(is_weekend, LDA_04, url))
df_test <- subset(df_test, select = -c(is_weekend, LDA_04, url))
```


**Split 'df_train' into a training set and a testing set**

```{r}
set.seed(1234)
v <- as.vector(c(rep(TRUE,80),rep(FALSE,20))) #create 70 TRUE, 30 FALSE
ind <- sample(v)  
df_train_train <- df_train[ind, ] 
df_train_test <- df_train[!ind, ] 
```

**Linear Regression**

```{r}
LOOCV_q5<-c()
for (var in colnames(df_train_train)){
  if (var!='shares'){
    lm_fit_q5<-lm(paste('shares','~',df_train_train[var]), data=df_train_train)
    LOOCV_q5<-c(LOOCV_q5,mean((lm_fit_q5$residuals/(1-hatvalues(lm_fit_q5)))^2))
    print(paste(var,":",mean((lm_fit_q5$residuals/(1-hatvalues(lm_fit_q5)))^2)))
  }
}
```

'kw_avg_avg' has lowest LOOCV error, so the best predictor is'kw_avg_avg'.

**KNN**

```{r}
rmse<-c()
vars<-c()
var_for_knn<-c('timedelta','n_tokens_content','n_unique_tokens','n_non_stop_unique_tokens','average_token_length','LDA_00','LDA_01','LDA_02', 'LDA_03','global_subjectivity', 'global_sentiment_polarity', 'global_rate_positive_words')
trControl <- trainControl(method = "cv", number = 10)
for (var in colnames(df_train_train)){ 
  if ((var %in% var_for_knn)){
    f<-formula(paste('shares','~',var))
    knn_fit <- train(f, data = df_train_train, method = 'knn',
              trControl = trControl)
    rmse<-c(rmse,min(knn_fit$results$RMSE))
    vars<-c(vars,var)
    
  } 
}
print(paste("The lowest RMSE for KNN: ",min(rmse)))
```


**GAM**

```{r}
library(mgcv)
```

```{r}
GCV_q5<-c()
factor_vars <- c('n_non_stop_words','num_videos','data_channel_is_lifestyle', 'data_channel_is_entertainment','data_channel_is_bus','data_channel_is_socmed',
                 'data_channel_is_tech', 'data_channel_is_world', 'kw_min_min', 'kw_max_min', 'kw_avg_min', 'kw_min_max', 'kw_max_max','kw_avg_max', 'kw_min_avg', 'kw_max_avg', 'kw_avg_avg',
                 'weekday_is_monday', 'weekday_is_tuesday', 'weekday_is_wednesday','weekday_is_thursday', 'weekday_is_friday', 'weekday_is_saturday', 'weekday_is_sunday','data_channel_is_viral','shares')
for (var in colnames(df_train_train)){ 
  if (!(var %in% factor_vars)){
    gam_fit_q5<-gam(substitute(shares~s(i),list(i=as.name(var))), data=df_train_train)
    print(paste(var,': ',summary(gam_fit_q5)$sp.criterion))
  } 
}
```

'self_reference_avg_sharess' gives lowest GCV error, so we choose it as predictor for GAM model.



**Comparison**

```{r}
print("MSE:")
# Linear Regression
print(paste('Linear Regression',round(0.825784601995825, 5)))
# KNN
print(paste('KNN',0.9280095^2))
# GAM
print(paste('GAM',0.83))
```

The best single-predictor model is simple linear regression with 'kw_avg_avg'. 


```{r}
best_model<-lm(shares~kw_avg_avg,data=df_train_train)
best_model
ggplot(data=df_train_train,aes(x=kw_avg_avg,y=shares))+geom_point()+geom_abline(intercept=best_model$coefficients[1],slope=best_model$coefficients[2])+ggtitle("predictive curve for training set")
ggplot(data=df_train_test,aes(x=kw_avg_avg,y=shares))+geom_point()+geom_abline(intercept=best_model$coefficients[1],slope=best_model$coefficients[2])+ggtitle("predictive curve for testing set")

```

```{r}
lm(shares ~ ., data = df_train_train)
```

On simple linear regression with kw_avg_avg, the coefficient for the predictor is smaller than the coefficient of kw_avg_avg in full linear regression model. The reason may be that The reason may be that single predictor model fails to consider other predictors that may contribute to the outcome. The average keyword in average shares (kw_avg_avg) may be closely correlated with some other predictors that are negatively correlated with the outcome. For example, kw_avg_avg may be positively correlated with best keyword (avgerage shares), while the best keyword (avgerage shares) is negatively correlated with outcome, which decreases the coefficient value for kw_avg_avg in single-predictor model.


```{r}
a=(predict(best_model,df_train_test)-df_train_test['shares'])^2
summary(a)
```
Thus, the out-of-sample error is 0.82792.


```{r}
# 'train' argument is the training data. Please opass 'df_train' (the entire training dataset) as input.
# Please pass 1234 as seed into the function.
q5_midterm1_predict <- function(test, train, seed){
  fit<-lm(shares~kw_avg_avg, data=train)
  shares_pred<-predict(fit, test['kw_avg_avg'])
  return(shares_pred)
}

# call the function
q5_midterm1_predict(df_test,df_train,1234)
# print(df_test['shares'])
```



## Question 6 - Do your models perform well on out-of-sample data? (12 pts.)

Finally, let's get everything ready to be run on a true out of sample data set.  Please include a code block here that includes `q1_midterm1_predict`, `q3_midterm1_predict`, and `q5_midterm1_predict`.  Once the assignments are turned in, I'll use these functions to train your chosen models and create predictions for the out of sample data set.  I'll post the data set on the Canvas site after all midterms are turned in.

For each model, I'll compute a root mean squared prediction error.  Across all models turned in, points will be allocated as follows:

  1. For Q1 and Q3, let $\text{RMSE}_{\text{Min}}$ be the minimum prediction error over the submitted models.  Let $\text{RMSE}_i$ be your predictive error.  Your prediction points will be computed as:
  
  $$5 \times \frac{\text{RMSE}_{\text{Min}}}{\text{RMSE}_{i}}$$
  
  2. For Q5 it will be:
  
  $$2 \times \frac{\text{RMSE}_{\text{Min}}}{\text{RMSE}_{i}}$$

The goal here is to add a low-stakes incentive and fun "competition" to try to optimize your predictive models.  I expect that the performance across groups/students will be quite close, so this won't greatly affect your final grade.

**If your prediction functions do not run, I will give zero points!**  I will make every effort to debug on my end, but make sure that your functions can run from a clean R/Python environment.  If using libraries/packages, please indicate the packages at the top of your code block/build in a package call that does not require `library()`.  Example functions are included below.  Code cleanliness and usability is important, so please treat this task with some rigor!

Example Function with train data frame:

```{r, include=TRUE, eval = FALSE}
test_proto <- data.table::fread("midterm1_testProto.csv")
#Train is a data frame and test is a data frame
q1_midterm1_predict <- function(train, test){
  #Let's train a KNN-regression model with 25 nearest neighbors
  train_x <- train
  train_x$shares <- NULL
  train_x$url <- NULL
  train_x <- as.matrix(train_x)
  train_y <- as.matrix(log(train$shares))
  test_x <- test
  test_x$shares <- NULL
  test_x$url <- NULL
  test_x <- as.matrix(test_x)
  knn_mod <- FNN::knn.reg(train = train_x, test = test_x, y = train_y, k = 25)
  return(knn_mod$pred)
}
#It works
q1_midterm_predict(train = train, test = test_proto)
```

Example function with train model object:

```{r, include=TRUE, eval = FALSE}
test_proto <- data.table::fread("midterm1_testProto.csv")
#Let's run a linear regression model
train_proc <- train
train_proc$url <- NULL
train_proc$LDA_04 <- NULL
train_proc$is_weekend <- NULL
train_proc$weekday_is_wednesday <- NULL
lm_mod <- lm(log(shares) ~ ., data = train_proc)
#Save the model object
saveRDS(lm_mod, "q1_midterm_predict.rds")
#We can read the model object back in
pred_mod <- readRDS("q1_midterm_predict.rds")
#You can check that pred_mod is the same as lm_mod!
#Write a function that takes the model object as train
q1_midterm1_predict <- function(train, test){
  #Using the train model object, make predictions
  return(predict(train,newdata = test))
}
#It works
q1_midterm_predict(train = pred_mod, test = test_proto)
```



### q1_midterm1_predict

```{r}
q1_midterm1_predict <- function(train, test, seed) {
  set.seed(1234)
  
  print("proprocessing the data...")
  
  # reshape the train set to fit the model
  train$shares = log(train$shares)
  test$shares = log(test$shares)
  
  # adding the `viral content category` variable
  train$data_channel_is_viral = train$data_channel_is_lifestyle
  for (i in 1:nrow(train)){
    if (train[i, "data_channel_is_lifestyle"] == 0 &
        train[i, "data_channel_is_entertainment"] == 0 &
        train[i, "data_channel_is_bus"] == 0 &
        train[i, "data_channel_is_socmed"] == 0 &
        train[i, "data_channel_is_tech"] == 0 &
        train[i, "data_channel_is_world"] == 0){
      
          train[i, "data_channel_is_viral"] = 1
          
    } else{
          train[i, "data_channel_is_viral"] = 0
        }
  }
  train$data_channel_is_viral <- as.integer(train$data_channel_is_viral)
  
  test$data_channel_is_viral = test$data_channel_is_lifestyle
  for (i in 1:nrow(test)){
    if (test[i, "data_channel_is_lifestyle"] == 0 &
        test[i, "data_channel_is_entertainment"] == 0 &
        test[i, "data_channel_is_bus"] == 0 &
        test[i, "data_channel_is_socmed"] == 0 &
        test[i, "data_channel_is_tech"] == 0 &
        test[i, "data_channel_is_world"] == 0){
      
          test[i, "data_channel_is_viral"] = 1
          
    } else{
          test[i, "data_channel_is_viral"] = 0
        }
  }
  test$data_channel_is_viral <- as.integer(test$data_channel_is_viral)
    
    # dropping variables
  train <- subset(train, select = -c(is_weekend, LDA_04, url))
  test <- subset(test, select = -c(is_weekend, LDA_04, url))
  
  # model training - Random Forest
  library(ranger)
  set.seed(1234)
  rf_fit <- ranger(shares ~ ., data = train, mtry = floor(sqrt(ncol(train) - 1)), num.trees = 500)
  
  pred = predict(rf_fit, test)
  return(pred$prediction)
  
}
```

Testing the function
```{r}

df_train <- read.csv("midterm1_train.csv")
df_proto.test <- read.csv("midterm1_testProto.csv")
```

```{r}
q1_midterm1_predict(df_train, df_proto.test, 1234)
```


### q3_midterm1_predict

```{r}
q3_midterm1_predict <- function(train, test, seed){
  # reshape training and test data
  library(dplyr)
  library(stringr)
  set.seed(1234)
  train$shares = log(train$shares)
  test$shares = log(test$shares)
  
  train <- subset(train, select = -c(is_weekend, LDA_04, url))
  test <- subset(test, select = -c(is_weekend, LDA_04, url))
  
  # load the 5 features that we found to be most useful
  features <- c("kw_avg_avg", "LDA_02", "data_channel_is_entertainment", "self_reference_avg_sharess", "kw_min_avg")
  
  non.fac.features <- features %>% str_subset("data_channel_is_entertainment",negate = T)
  # construct model
  library(mgcv)
  formula <- as.formula(paste0("shares","~ ", 
                                paste(paste0("s(",non.fac.features,")"),collapse=" + "),
                                "+data_channel_is_entertainment"))
  gam.model <- gam(formula, data = train)
  
  #compute predictions
  prediction <- predict(gam.model, newdata = test, type = "response") %>% as.data.frame()
  return(prediction[,1])
}
```

Test the prediction function

```{r, message=F}

df_train <- read.csv("midterm1_train.csv")
df_proto.test <- read.csv("midterm1_testProto.csv")
q3_midterm1_predict(df_train, df_proto.test,1234)
```



### q5_midterm1_predict

```{r}
# 'train' argument is the training data. Please opass 'df_train' (the entire training dataset) as input.
# Please pass 1234 as seed into the function.


q5_midterm1_predict <- function(test, train, seed){
  # take of log of `share`
  set.seed(1234)
  train$shares = log(train$shares)
  test$shares = log(test$shares)

  # fit the model and predict
  
  fit<-lm(shares~kw_avg_avg, data=train)
  shares_pred<-predict(fit, test['kw_avg_avg'])
  return(shares_pred)
}
```

Test the function.
```{r}

df_train <- read.csv("midterm1_train.csv")
df_test <- read.csv("midterm1_testProto.csv")
q5_midterm1_predict(df_test,df_train,1234)
```





  










