colnames(temp)[1] <- "predict"
mse_ss <- mean(( temp$predict - temp$observe )^2)
test
test_pred <- data.frame(S.F.Ratio = df_test[, 'S.F.Ratio'])
# 3rd order poly
test_poly <- predict(lm_optim, test_pred)
mse_poly <- mean(( test_poly - df_test$Outstate )^2)
# b-spline
test_bs <- predict(lm_bspline, test_pred)
mse_bs <- mean(( test_bs - df_test$Outstate )^2)
# natural spline
test_ns <- predict(lm_nspline, test_pred)
mse_ns <- mean(( test_ns - df_test$Outstate )^2)
# smooth spline
test_ss <- predict(lm_sspline, test_pred)[2]
temp <- data.frame(predict = test_ss, observe = df_test$Outstate)
colnames(temp)[1] <- "predict"
mse_ss <- mean(( temp$predict - temp$observe )^2)
test_pred <- cbind(test_pred, poly = test_pred, test_bs, test_ns,
test_ss = test_ss[2], Outstate = df_test$Outstate)
test_pred <- cbind(test_pred, poly = test_pred, test_bs, test_ns,
test_ss = temp$predict, Outstate = df_test$Outstate)
View(test_pred)
test_pred <- cbind(test_pred, poly = test_poly, test_bs, test_ns,
test_ss = temp$predict, Outstate = df_test$Outstate)
test_pred <- data.frame(S.F.Ratio = df_test[, 'S.F.Ratio'])
# 3rd order poly
test_poly <- predict(lm_optim, test_pred)
mse_poly <- mean(( test_poly - df_test$Outstate )^2)
# b-spline
test_bs <- predict(lm_bspline, test_pred)
mse_bs <- mean(( test_bs - df_test$Outstate )^2)
# natural spline
test_ns <- predict(lm_nspline, test_pred)
mse_ns <- mean(( test_ns - df_test$Outstate )^2)
# smooth spline
test_ss <- predict(lm_sspline, test_pred)[2]
temp <- data.frame(predict = test_ss, observe = df_test$Outstate)
colnames(temp)[1] <- "predict"
mse_ss <- mean(( temp$predict - temp$observe )^2)
test_pred <- cbind(test_pred, poly = test_poly, test_bs, test_ns,
test_ss = temp$predict, Outstate = df_test$Outstate)
test_pred <- cbind(test_pred, poly = test_poly, test_bs, test_ns,
test_ss = temp$predict, Outstate = df_test$Outstate)
colors <- c("Observed Tuition" = "grey", "bSpline" = "blue", "nSpline" = "red",
"sSpline" = "green", "poly" = "orange")
ggplot(test_pred, aes(x = S.F.Ratio)) +
geom_point(aes(y = Outstate, color = "Observed Tuition")) +
geom_line(aes(y = test_bs, color = "bSpline")) +
geom_line(aes(y = test_ns, color = "nSpline")) +
geom_line(aes(y = test_ss, color = "sSpline")) +
geom_line(aes(y = poly, color = "poly")) +
labs(
y="Tuittion",
x="S.F Ratio",
title="Validation",
color = "Legend"
) +
scale_color_manual(values = colors)
test_pred <- data.frame(S.F.Ratio = df_test[, 'S.F.Ratio'])
# 3rd order poly
test_poly <- predict(lm_optim, test_pred)
mse_poly <- mean(( test_poly - df_test$Outstate )^2)
# b-spline
test_bs <- predict(lm_bspline, test_pred)
mse_bs <- mean(( test_bs - df_test$Outstate )^2)
# natural spline
test_ns <- predict(lm_nspline, test_pred)
mse_ns <- mean(( test_ns - df_test$Outstate )^2)
# smooth spline
test_ss <- predict(lm_sspline, test_pred)[2]
temp <- data.frame(predict = test_ss, observe = df_test$Outstate)
colnames(temp)[1] <- "predict"
mse_ss <- mean(( temp$predict - temp$observe )^2)
test_pred <- cbind(test_pred, poly = test_poly, test_bs, test_ns,
test_ss = temp$predict, Outstate = df_test$Outstate)
colors <- c("Observed Tuition" = "grey", "bSpline" = "blue", "nSpline" = "red",
"sSpline" = "green", "poly" = "orange")
ggplot(test_pred, aes(x = S.F.Ratio)) +
geom_point(aes(y = Outstate, color = "Observed Tuition")) +
geom_line(aes(y = test_bs, color = "bSpline")) +
geom_line(aes(y = test_ns, color = "nSpline")) +
geom_line(aes(y = test_ss, color = "sSpline")) +
geom_line(aes(y = poly, color = "poly")) +
labs(
y="Tuittion",
x="S.F Ratio",
title="Validation",
color = "Legend"
) +
scale_color_manual(values = colors)
mse <- data.frame(mse_poly, mse_bs, mse_ns,mse_ss)
mse
round(mse, 3)
mse <- data.frame(mse_poly, mse_bs, mse_ns,mse_ss)
round(mse, 5)
round(mse, 3)
ggplot(df_EPE, aes(x=seq, y=poly_EPE)) +
geom_line() +
scale_x_continuous("K_th polynomial", labels = as.character(seq), breaks = seq) +
labs(
y="LOOCV",
x="K_th polynomial",
title="K_th polynomial vs. LOOCV")
ModelMetrics::mse(pred=prediction.lasso, actual = df_test$Outstate)
fit.lasso <- glmnet::glmnet(df_train.predictor, df_train.outcome, alpha = 1)
library(ggplot2)
library(data.table)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
library(ggplot2)
rm(list=ls(all=TRUE))
setwd("/Users/zejiachen/Desktop/Sspring 2022/Statstical Learning/problemSet 4")
df_train <- read.csv("college_train.csv")
df_test <- read.csv("college_test.csv")
df_train$Outstate = log(df_train$Outstate)
df_test$Outstate = log(df_test$Outstate)
ggplot(df_train, aes(x = S.F.Ratio,
y = Outstate)) +
geom_point() +
labs(
x="student/faculty ratio",
y="outstate tuition",
)
poly_EPE <- c()
for (order in c(1:8)){
lm_fit <- lm(Outstate ~ poly(S.F.Ratio, degree = order, raw = T), data = df_train)
poly_EPE <- append(poly_EPE, mean((lm_fit$residuals/(1-hatvalues(lm_fit)))^2))
}
seq = seq(1, 8)
df_EPE <- data.frame(seq, poly_EPE)
ggplot(df_EPE, aes(x=seq, y=poly_EPE)) +
geom_line() +
scale_x_continuous("K_th polynomial", labels = as.character(seq), breaks = seq) +
labs(
y="LOOCV",
x="K_th polynomial",
title="K_th polynomial vs. LOOCV")
lm_optim <- lm(Outstate ~poly(S.F.Ratio, degree = 3, raw = T), data = df_train)
df_predict <- data.frame(S.F.Ratio = df_train[,'S.F.Ratio'])
pred_Outstate <- predict(lm_optim, df_predict)
df_predict <- cbind(df_predict, pred_Outstate, Outstate = df_train$Outstate)
colors <- c("Observed Tuition" = "grey", "Predicted Tuition" = "blue")
ggplot(df_predict, aes(x = S.F.Ratio)) +
geom_point(aes(y = Outstate, color = "Observed Tuition")) +
geom_line(aes(y = pred_Outstate, color = "Predicted Tuition")) +
labs(
y="Tuittion",
x="S.F Ratio",
title="Cubic Linear Polynomial Prediction",
color = "Legend"
) +
scale_color_manual(values = colors)
library(splines)
lm_bspline <- lm(Outstate ~ bs(S.F.Ratio, df = 10, degree = 3), data = df_train)
lm_nspline <- lm(Outstate ~ ns(S.F.Ratio), data = df_train)
lm_sspline <- with(df_train, smooth.spline(S.F.Ratio, Outstate))
df_predict_splines <- data.frame(S.F.Ratio = df_train[,'S.F.Ratio'])
pred_bs <- predict(lm_bspline, df_predict_splines)
pred_ns <- predict(lm_nspline, df_predict_splines)
pred_ss <- predict(lm_sspline, df_predict_splines)
df_predict_splines <- cbind(df_predict_splines, pred_bs = pred_bs, pred_ns,
pred_ss = pred_ss[2], Outstate = df_train$Outstate)
colnames(df_predict_splines)[4] <- "pred_ss"
colors <- c("Observed Tuition" = "grey", "bSpline" = "blue", "nSpline" = "red", "sSpline" = "green")
ggplot(df_predict_splines, aes(x = S.F.Ratio)) +
geom_point(aes(y = Outstate, color = "Observed Tuition")) +
geom_line(aes(y = pred_bs, color = "bSpline")) +
geom_line(aes(y = pred_ns, color = "nSpline")) +
geom_line(aes(y = pred_ss, color = "sSpline")) +
labs(
y="Tuittion",
x="S.F Ratio",
title="Various Splines",
color = "Legend"
) +
scale_color_manual(values = colors)
test_pred <- data.frame(S.F.Ratio = df_test[, 'S.F.Ratio'])
# 3rd order poly
test_poly <- predict(lm_optim, test_pred)
mse_poly <- mean(( test_poly - df_test$Outstate )^2)
# b-spline
test_bs <- predict(lm_bspline, test_pred)
mse_bs <- mean(( test_bs - df_test$Outstate )^2)
# natural spline
test_ns <- predict(lm_nspline, test_pred)
mse_ns <- mean(( test_ns - df_test$Outstate )^2)
# smooth spline
test_ss <- predict(lm_sspline, test_pred)[2]
temp <- data.frame(predict = test_ss, observe = df_test$Outstate)
colnames(temp)[1] <- "predict"
mse_ss <- mean(( temp$predict - temp$observe )^2)
test_pred <- cbind(test_pred, poly = test_poly, test_bs, test_ns,
test_ss = temp$predict, Outstate = df_test$Outstate)
colors <- c("Observed Tuition" = "grey", "bSpline" = "blue", "nSpline" = "red",
"sSpline" = "green", "poly" = "orange")
ggplot(test_pred, aes(x = S.F.Ratio)) +
geom_point(aes(y = Outstate, color = "Observed Tuition")) +
geom_line(aes(y = test_bs, color = "bSpline")) +
geom_line(aes(y = test_ns, color = "nSpline")) +
geom_line(aes(y = test_ss, color = "sSpline")) +
geom_line(aes(y = poly, color = "poly")) +
labs(
y="Tuittion",
x="S.F Ratio",
title="Validation",
color = "Legend"
) +
scale_color_manual(values = colors)
mse <- data.frame(mse_poly, mse_bs, mse_ns,mse_ss)
round(mse, 3)
library(caret)
library(glmnet)
library(dplyr)
# convert `Private` into a factor
df_trainFac <- df_train[,-19]
df_trainFac$Private[df_train$Private == "Yes"] <- 1
df_trainFac$Private[df_train$Private == "No"] <- 0
df_trainFac$Private <- as.factor(df_trainFac$Private)
# Reshape the factor `Private` to fit the data set into LASSO
dummy_trn <- caret::dummyVars("~ .", data = df_trainFac)
df_trainFac <- data.frame(predict(dummy_trn, newdata = df_trainFac))
trn_x <- as.matrix(df_trainFac[,-10])
trn_y <- as.matrix(df_trainFac$Outstate)
fit_cv_LASSO<- cv.glmnet(trn_x, trn_y, type.measure = "mse", alpha = 1, family = "gaussian", nfolds = 10)
plot(fit_cv_LASSO)
fit_cv_LASSO
coef_optim_LASSO <- coef(fit_cv_LASSO, s= fit_cv_LASSO$lambda.1se)
theme_set(theme_bw())
coef_df <- as.data.frame(summary(coef_optim_LASSO))
col_names_df <- rbind(c("Intercept", 0),data.frame(colnames(trn_x)))
# saving all leftover significant predictors
coef_shrink <- col_names_df[c(coef_df$i),1]
coef_df <- cbind(coef_shrink, coef_df)
coef_df <- coef_df[,-c(2,3)]
colnames(coef_df)[1] <- "predictors"
colnames(coef_df)[2] <- "coef"
coef_df$coef <- round(coef_df$coef, 7)
coef_df %>% arrange(desc(coef))
library(mgcv)
# convert `Private` into a factor
df_trainFac2 <- df_train[,-19]
df_trainFac2$Private[df_train$Private == "Yes"] <- 1
df_trainFac2$Private[df_train$Private == "No"] <- 0
df_trainFac2$Private <- as.factor(df_trainFac2$Private)
gam1<-gam(Outstate~ Private + s(S.F.Ratio) + s(Grad.Rate)
+ s(perc.alumni) + s(PhD) + s(Room.Board) + s(Expend), data=df_trainFac2)
plot(gam1, pages  = 1)
summary(gam1)$sp.criterion
gam2<-gam(Outstate~Private+Grad.Rate+perc.alumni+PhD+Terminal+Room.Board+Expend+S.F.Ratio, data=df_trainFac2)
summary(gam2)$sp.criterion
gam3<-gam(Outstate~Private+perc.alumni+PhD+Terminal+Room.Board+s(Expend,Grad.Rate)+S.F.Ratio, data=df_trainFac2)
plot(gam3, pages = 1)
gam3
gam4<-gam(Outstate~Private+perc.alumni+s(PhD,Grad.Rate)+Terminal+Room.Board+Expend+S.F.Ratio, data=df_trainFac2)
plot(gam4)
gam4
library(tree)
tree=tree(Outstate~.,data=df_trainFac2)
plot(tree)
text(tree,pretty=2)
cv_tree=cv.tree(tree,K=20)
plot(cv_tree$size,cv_tree$dev,type='b',xlab='Number of Terminal Nodes',ylab='deviance')
p_tree=prune.tree(tree,best=7)
plot(p_tree)
text(p_tree)
df_testFac <- df_test[,-19]
df_testFac$Private[df_test$Private == "Yes"] <- 1
df_testFac$Private[df_test$Private == "No"] <- 0
df_testFac$Private <- as.factor(df_testFac$Private)
dummy_tst <- caret::dummyVars("~ .", data = df_testFac)
df_testFac <- data.frame(predict(dummy_tst, newdata = df_testFac))
df_train.predictor <- as.matrix(df_trainFac[,-10])
df_train.outcome <- as.matrix(df_trainFac[,10])
df_test.predictor <- as.matrix(df_testFac[,-10])
df_test.outcome <- as.matrix(df_testFac[,10])
tst_x <- as.matrix(df_testFac[,-10])
tst_y <- as.matrix(df_testFac$Outstate)
fit.lasso <- glmnet::glmnet(df_train.predictor, df_train.outcome, alpha = 1)
prediction.lasso <- predict(fit.lasso, newx = df_test.predictor, s= fit_cv_LASSO$lambda.1se)
ModelMetrics::mse(pred=prediction.lasso, actual = df_test$Outstate)
ModelMetrics::mse(pred=prediction.lasso, actual = df_test$Outstate)
ModelMetrics::mse(pred=prediction.GAM, actual = df_test$Outstate)
ModelMetrics::mse(pred=prediction.lasso, actual = df_test$Outstate)
ModelMetrics::mse(pred=prediction.GAM, actual = df_test$Outstate)
prediction.ptree <- predict(p_tree, newdata = df_testFac2)
df_testFac2 <- df_test[,-19]
df_testFac2$Private[df_test$Private == "Yes"] <- 1
df_testFac2$Private[df_test$Private == "No"] <- 0
df_testFac2$Private <- as.factor(df_testFac2$Private)
df_testFac2 <- df_testFac2[,-9]
names(df_testFac2)
prediction.GAM <- predict(gam1, newdata = df_testFac2)
prediction.tree <- predict(tree, newdata = df_testFac2)
prediction.ptree <- predict(p_tree, newdata = df_testFac2)
ModelMetrics::mse(pred=prediction.lasso, actual = df_test$Outstate)
ModelMetrics::mse(pred=prediction.GAM, actual = df_test$Outstate)
ModelMetrics::mse(pred=prediction.tree, actual = df_test$Outstate)
ModelMetrics::mse(pred=prediction.ptree, actual = df_test$Outstate)
ModelMetrics::mse(df_test$Outstate, prediction.lasso)
ModelMetrics::mse(df_test$Outstate, prediction.GAM)
ModelMetrics::mse(df_test$Outstate, prediction.tree)
ModelMetrics::mse(df_test$Outstate, prediction.ptree)
test.MSE <- data.frame(ModelMetrics::mse(pred=prediction.lasso, actual = df_test$Outstate)
ModelMetrics::mse(pred=prediction.GAM, actual = df_test$Outstate)
test.MSE <- data.frame(Models = c("Lasso", "GAM", "Full tree", "Pruned tree"),
MSE = c(ModelMetrics::mse(pred=prediction.lasso, actual = df_test$Outstate),
ModelMetrics::mse(pred=prediction.GAM, actual = df_test$Outstate),
ModelMetrics::mse(pred=prediction.tree, actual = df_test$Outstate),
ModelMetrics::mse(pred=prediction.ptree, actual = df_test$Outstate)))
test.MSE$MSE <- round(test.MSE$MSE, 7)
test.MSE %>% arrange(MSE)
knitr::opts_chunk$set(echo = TRUE)
data('meapsingle')
setwd("/Users/zejiachen/Desktop/Sspring 2022/QTM 220")
knitr::opts_chunk$set(echo = TRUE, prompt=FALSE, message = FALSE,comment=NA )
setwd("/Users/zejiachen/Desktop/Sspring 2022/QTM 220")
knitr::opts_chunk$set(echo = TRUE, prompt=FALSE, message = FALSE,comment=NA )
#import package
library(wooldridge)
library(ggplot2)
data('meap93')
str(meap93)
data('meapsingle')
knitr::opts_chunk$set(echo = TRUE)
X=as.matrix([meapsingle,c('pctsgle','lmedinc','free')])
knitr::opts_chunk$set(echo = TRUE)
X=as.matrix(meapsingle,c('pctsgle','lmedinc','free'))
Y=as.matrix(meapsingle$math4)
hat_beta=inv(t(X)%*%X)%*%t(X)%*%Y
knitr::opts_chunk$set(echo = TRUE)
X=as.matrix(meapsingle,c('pctsgle','lmedinc','free'))
Y=as.matrix(meapsingle$math4)
hat_beta=solve(t(X)%*%X)%*%t(X)%*%Y
knitr::opts_chunk$set(echo = TRUE)
X =as.matrix(meapsingle[,c('pctsgle','lmedinc','free')])
Xt = t(X)
Y =as.matrix(meapsingle$math4)
beta_hat2 = solve(t(X)%*%X)%*%t(X)%*%Y
beta_hat2
knitr::opts_chunk$set(echo = TRUE)
X =as.matrix(meapsingle[,c('pctsgle','lmedinc','free')])
Y =as.matrix(meapsingle$math4)
beta_hat2 = solve(t(X)%*%X)%*%t(X)%*%Y
beta_hat2
X =as.matrix(meapsingle[,c('pctsgle','lmedinc','free')])
Y =as.matrix(meapsingle$math4)
beta_hat2 = solve(t(X)%*%X)%*%t(X)%*%Y
beta_hat2
knitr::opts_chunk$set(echo = TRUE)
X =as.matrix(meapsingle[,c('pctsgle','lmedinc','free')])
Y =as.matrix(meapsingle$math4)
beta_hat = solve(t(X)%*%X)%*%t(X)%*%Y
beta_hat
knitr::opts_chunk$set(echo = TRUE)
summary(lm(math4 ~ pctsgle + lmedinc + free, data = meapsingle))
knitr::opts_chunk$set(echo = TRUE)
lm(math4 ~ pctsgle + lmedinc + free, data = meapsingle)$coefficients
knitr::opts_chunk$set(echo = TRUE)
meapsingle[,c('pctsgle','lmedinc','free')]
knitr::opts_chunk$set(echo = TRUE)
X
knitr::opts_chunk$set(echo = TRUE)
Y
knitr::opts_chunk$set(echo = TRUE)
meap_df=meapsingle[,c('math4','pctsgle','lmedinc','free')]
knitr::opts_chunk$set(echo = TRUE)
meap_df
knitr::opts_chunk$set(echo = TRUE)
X =as.matrix(meap_df[,c('pctsgle','lmedinc','free')])
Y =as.matrix(meap_df$math4)
beta_hat = solve(t(X)%*%X)%*%t(X)%*%Y
beta_hat
knitr::opts_chunk$set(echo = TRUE)
lm(math4 ~ pctsgle + lmedinc + free, data = meap_df)$coefficients
knitr::opts_chunk$set(echo = TRUE)
meap_df=meapsingle[,c('math4','pctsgle','lmedinc','free')]
meap_df$cons=1
X =as.matrix(meap_df[,c('pctsgle','lmedinc','free')])
Y =as.matrix(meap_df$math4)
beta_hat = solve(t(X)%*%X)%*%t(X)%*%Y
beta_hat
knitr::opts_chunk$set(echo = TRUE)
X =as.matrix(meap_df[,c('con','pctsgle','lmedinc','free')])
knitr::opts_chunk$set(echo = TRUE)
X =as.matrix(meap_df[,c('cons','pctsgle','lmedinc','free')])
Y =as.matrix(meap_df$math4)
beta_hat = solve(t(X)%*%X)%*%t(X)%*%Y
beta_hat
data('meapsingle')
meap_df=meapsingle[,c('math4','pctsgle','lmedinc','free')]
meap_df$cons=1
X =as.matrix(meap_df[,c('cons','pctsgle','lmedinc','free')])
Y =as.matrix(meap_df$math4)
beta_hat = solve(t(X)%*%X)%*%t(X)%*%Y
beta_hat
lm(math4 ~ pctsgle + lmedinc + free, data = meap_df)$coefficients
setwd("/Users/zejiachen/Desktop/Sspring 2022/QTM 220/hw3")
knitr::opts_chunk$set(echo = TRUE, prompt=FALSE, message = FALSE,comment=NA )
knitr::opts_chunk$set(echo = TRUE)
#import package
library(wooldridge)
library(ggplot2)
data('meap93')
str(meap93)
fit <- lm(math10 ~ log(expend) + lnchprg, data = meap93)
summary(fit)
fit1 <- lm(log(expend)~lnchprg, dat = meap93)
summary(fit1)
summary(lm(math10 ~ log(expend), data = meap93))
data("wage1")
fit <- lm(log(wage) ~ exper + tenure, data = wage1)
fit_educ <- lm(educ ~ exper + tenure, data = wage1)
data("wage1")
fit <- lm(log(wage) ~ exper + tenure, data = wage1)
fit_educ <- lm(educ ~ exper + tenure, data = wage1)
resid_wage <- fit$residuals
resid_educ <- fit_educ$residuals
resid_df <- data.frame(resid_wage, resid_educ)
fit_partial <- lm(resid_wage ~ 0 + resid_educ, data = resid_df)
summary(fit_partial)
summary(lm(log(wage)~educ + exper + tenure, data =wage1))
knitr::opts_chunk$set(echo = TRUE)
resid_wage <- fit$residuals
resid_educ <- fit_educ$residuals
resid_df <- data.frame(resid_wage, resid_educ)
fit_partial <- lm(resid_wage ~ 0 + resid_educ, data = resid_df)
summary(fit_partial)
lm(log(wage)~educ + exper + tenure, data =wage1)$coefficients
knitr::opts_chunk$set(echo = TRUE)
resid_wage <- fit$residuals
resid_educ <- fit_educ$residuals
resid_df <- data.frame(resid_wage, resid_educ)
fit_partial <- lm(resid_wage ~ 0 + resid_educ, data = resid_df)
fit_partial$coefficients
lm(log(wage)~educ + exper + tenure, data =wage1)$coefficients
knitr::opts_chunk$set(echo = TRUE)
resid_wage <- fit$residuals
resid_educ <- fit_educ$residuals
resid_df <- data.frame(resid_wage, resid_educ)
fit_partial <- lm(resid_wage ~ 0 + resid_educ, data = resid_df)
fit_partial
lm(log(wage)~educ + exper + tenure, data =wage1)$coefficients
knitr::opts_chunk$set(echo = TRUE)
resid_wage <- fit$residuals
resid_educ <- fit_educ$residuals
resid_df <- data.frame(resid_wage, resid_educ)
fit_partial <- lm(resid_wage ~ 0 + resid_educ, data = resid_df)
fit_partial$coefficients
lm(log(wage)~educ + exper + tenure, data =wage1)$coefficients
knitr::opts_chunk$set(echo = TRUE)
gg <- ggplot(resid_df, aes(x = resid_educ, y = resid_wage)) +
geom_point() +
geom_lin() +
labs(
y="r_wage",
x="r_educ",
)
knitr::opts_chunk$set(echo = TRUE)
gg <- ggplot(resid_df, aes(x = resid_educ, y = resid_wage)) +
geom_point() +
geom_line() +
labs(
y="r_wage",
x="r_educ",
)
plot(gg)
knitr::opts_chunk$set(echo = TRUE)
gg <- ggplot(resid_df, aes(x = resid_educ, y = resid_wage)) +
geom_point() +
labs(
y="r_wage",
x="r_educ",
)
plot(gg)
knitr::opts_chunk$set(echo = TRUE)
gg <- ggplot(resid_df, aes(x = resid_educ, y = resid_wage)) +
geom_point() +
geom_abline() +
labs(
y="r_wage",
x="r_educ",
)
plot(gg)
knitr::opts_chunk$set(echo = TRUE)
gg <- ggplot(resid_df, aes(x = resid_educ, y = resid_wage)) +
geom_point() +
geom_smooth(method = 'lm') +
labs(
y="r_wage",
x="r_educ",
)
plot(gg)
data("wage1")
# partially the effect of `educ`
fit <- lm(log(wage) ~ exper + tenure, data = wage1)
fit_educ <- lm(educ ~ exper + tenure, data = wage1)
resid_wage <- fit$residuals
resid_educ <- fit_educ$residuals
resid_df <- data.frame(resid_wage, resid_educ)
fit_partial <- lm(resid_wage ~ 0 + resid_educ, data = resid_df)
fit_partial$coefficients
# varify our result
lm(log(wage)~educ + exper + tenure, data =wage1)$coefficients
# plotting
gg <- ggplot(resid_df, aes(x = resid_educ, y = resid_wage)) +
geom_point() +
geom_smooth(method = 'lm') +
labs(
y="r_wage",
x="r_educ",
)
plot(gg)
