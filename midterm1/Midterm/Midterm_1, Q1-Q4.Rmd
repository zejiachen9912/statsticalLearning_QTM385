---
title: 'Midterm_1, Q1-Q4'
author: "Zejia Chen, Francis Lin, Mike Lin"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  prettydoc::html_pretty:
    df_print: kable
    theme: leonids
    highlight: github
    toc: no
    toc_depth: 2
    toc_float:
      collapsed: no
  html_document:
    toc: no
    toc_depth: '2'
    df_print: paged
  pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
---

```{r, include=FALSE, message=F}
library(ggplot2)
library(dplyr)
library(data.table)
library(glmnet)
library(plotmo)
library(caret)
library(vip)
library(randomForest)
library(ModelMetrics)
library(mgcv)
library(caTools)
library(stringr)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
```


# Problem 1
```{r}
df_train <- read.csv("midterm1_train.csv")
df_proto.test <- read.csv("midterm1_testProto.csv")
```

Data Cleaning
```{r}
# take of log of `share`
df_train$shares = log(df_train$shares)
df_proto.test$shares = log(df_proto.test$shares)
```

```{r}
# adding the `viral content category` variable

df_train$data_channel_is_viral = df_train$data_channel_is_lifestyle

for (i in 1:nrow(df_train)){
  if (df_train[i, "data_channel_is_lifestyle"] == 0 &
      df_train[i, "data_channel_is_entertainment"] == 0 &
      df_train[i, "data_channel_is_bus"] == 0 &
      df_train[i, "data_channel_is_socmed"] == 0 &
      df_train[i, "data_channel_is_tech"] == 0 &
      df_train[i, "data_channel_is_world"] == 0){
    
        df_train[i, "data_channel_is_viral"] = 1
        
  } else{
        df_train[i, "data_channel_is_viral"] = 0
      }
}

df_train$data_channel_is_viral <- as.integer(df_train$data_channel_is_viral)

```

```{r}
df_proto.test$data_channel_is_viral = df_proto.test$data_channel_is_lifestyle

for (i in 1:nrow(df_proto.test)){
  if (df_proto.test[i, "data_channel_is_lifestyle"] == 0 &
      df_proto.test[i, "data_channel_is_entertainment"] == 0 &
      df_proto.test[i, "data_channel_is_bus"] == 0 &
      df_proto.test[i, "data_channel_is_socmed"] == 0 &
      df_proto.test[i, "data_channel_is_tech"] == 0 &
      df_proto.test[i, "data_channel_is_world"] == 0){
    
        df_proto.test[i, "data_channel_is_viral"] = 1
        
  } else{
        df_proto.test[i, "data_channel_is_viral"] = 0
      }
}

df_proto.test$data_channel_is_viral <- as.integer(df_proto.test$data_channel_is_viral)
```


```{r}
# dropping variables that might cause collinearity issue

df_train <- subset(df_train, select = -c(is_weekend, LDA_04, url))

df_proto.test <- subset(df_proto.test, select = -c(is_weekend, LDA_04, url))

```

Train-test Split
```{r}
set.seed(1234)

ind <- sample(2, nrow(df_train), replace = T, prob = c(0.7, 0.3))
df_train_sample <- df_train[ind == 1, ]
df_test_sample <- df_train[ind == 2, ]
```


---
## Model Testing
```{r, message=F}
library(caret)
library(glmnet)
library(dplyr)
```


**Multiple Linear Regression**

`Coefficients: (2 not defined because of singularities)` ??

```{r}
lm_fit <- lm(shares ~ ., data = df_train_sample)
```


```{r}
coef_df <- data.frame(lm_fit$coefficients)
colnames(coef_df)[1] <- "coef"
coef_df$coef <- round(coef_df$coef, 3)

coef_df %>% arrange(desc(coef)) %>%top_n(12)
```


**KNN**

Error: `#Error: vector memory exhausted (limit reached?)`

```{r}
trControl <- trainControl(method = "cv", number = 10)
 
set.seed(1234)
 
knn_fit <- train(shares ~., data = df_train_sample, method = 'knn',
              trControl = trControl)
```

```{r}
knn_fit

plot(knn_fit)
```

```{r}
knn_importance <- varImp(knn_fit)

knn_importance <- data.frame(knn_importance[1])

knn_importance %>% arrange(desc(knn_importance)) %>%top_n(12)
```




**LASSO**

```{r}
trn_x <- as.matrix(df_train_sample[,-58])
trn_y <- as.matrix(df_train_sample$shares)
```


```{r}
fit_cv_LASSO<- cv.glmnet(trn_x, trn_y, type.measure = "mse", alpha = 1, family = "gaussian", nfolds = 10)

plot(fit_cv_LASSO)
```


Let use `1se` to retrieve our ‘significant’ predictors, since it minimize the number of predictor. Since there 50+ predictors, we might want to eliminate as much predictors as possible, preventing any cases of overfitting so that we can reduce as much variance during the validation process. 
```{r}
coef_optim_LASSO <- coef(fit_cv_LASSO, s= fit_cv_LASSO$lambda.1se)
```

```{r}
theme_set(theme_bw())
coef_df <- as.data.frame(summary(coef_optim_LASSO))
col_names_df <- rbind(c("Intercept", 0),data.frame(colnames(trn_x)))
# saving all leftover significant predictors
coef_shrink <- col_names_df[c(coef_df$i),1]
coef_df <- cbind(coef_shrink, coef_df)
coef_df <- coef_df[,-c(2,3)]
colnames(coef_df)[1] <- "predictors"
colnames(coef_df)[2] <- "coef"
coef_df$coef <- round(coef_df$coef, 7)

coef_df %>% arrange(desc(coef)) %>% top_n(12)
coef_df %>% arrange(coef) %>% top_n(-10)

```

**GAM**
```{r,message=F}
library(mgcv)
```

We will use the top five most positive & negative predictors selected by LASSO.

```{r}
gam2 <- gam(shares ~ s(kw_avg_avg) + s(kw_min_avg) + s(LDA_02) + s(self_reference_avg_sharess) +
            data_channel_is_viral + s(global_subjectivity) + s(self_reference_min_shares) +
            s(self_reference_max_shares),
            data = df_train_sample)
plot(gam2, pages = 1)
```



```{r}
round(sqrt(summary(gam2)$sp.criterion), 5)
```



```{r}
gam1<-gam(shares~ s(global_subjectivity) + weekday_is_saturday + weekday_is_sunday + s(kw_min_avg)
          + data_channel_is_socmed + s(LDA_00) + s(rate_positive_words)
          + s(global_rate_negative_words)
          + s(global_rate_positive_words) + s(LDA_02) + min_positive_polarity
          +  data_channel_is_entertainment + s(global_sentiment_polarity)
          + s(avg_negative_polarity), data=df_train_sample)

plot(gam1, pages = 1)
```

```{r}
round(sqrt(summary(gam1)$sp.criterion), 5)
```


**Bagged Tree**
```{r}
library(ranger)
```

```{r}
bagged_tree <- ranger(shares ~ ., data = df_train_sample, mtry = ncol(df_train_sample) - 1, num.trees = 500)
```


**Random Forest**

```{r}
set.seed(1234)

rf_fit <- ranger(shares ~ ., data = df_train_sample, mtry = floor(sqrt(ncol(df_train) - 1)), num.trees = 500)
```


```{r}
# saving the model
saveRDS(rf_fit, "rf.rds")
```

Variable Importance
```{r}
rf_fit2 <- ranger(shares ~ ., data = df_train_sample, mtry = floor(sqrt(ncol(df_train) - 1)), 
                  num.trees = 500, importance = 'permutation')
rf_fit3 <- ranger(shares ~ ., data = df_train_sample, mtry = floor(sqrt(ncol(df_train) - 1)), 
                  num.trees = 500, importance = 'impurity')
```



```{r}
colnames <- colnames(df_train)
colnames <- colnames[-58]

rf_fit2_importance <- data.frame(rf_fit2$variable.importance)
rf_fit2_importance$Predictors <- colnames
colnames(rf_fit2_importance)[1] <- "Importance"
rf_fit2_top <- rf_fit2_importance %>% arrange(desc(rf_fit2_importance))
rf_fit2_top <- rf_fit2_top[1:15,]


rf_fit3_importance <- data.frame(rf_fit3$variable.importance)
rf_fit3_importance$Predictors <- colnames
colnames(rf_fit3_importance)[1] <- "Importance"
rf_fit3_top <- rf_fit3_importance %>% arrange(desc(rf_fit3_importance))
rf_fit3_top <- rf_fit3_top[1:15,]
```

```{r}
fit1 <- ggplot(rf_fit2_top, aes(x=reorder(Predictors,-Importance), y=Importance, label=round(Importance, 3))) + 
  geom_point(col="tomato2", size=3) +   # Draw points
  geom_segment(aes(x=Predictors, 
                   xend=Predictors, 
                   y=min(Importance), 
                   yend=max(Importance)), 
               linetype="dashed", 
               size=0.1) +   # Draw dashed lines
  labs(title="Variable Importance", y = "Importance", x = "Predictors") +  
  coord_flip()
```

```{r}
fit2 <- ggplot(rf_fit3_top, aes(x=reorder(Predictors,-Importance), y=Importance, label=round(Importance, 3))) + 
  geom_point(col="tomato2", size=3) +   # Draw points
  geom_segment(aes(x=Predictors, 
                   xend=Predictors, 
                   y=min(Importance), 
                   yend=max(Importance)), 
               linetype="dashed", 
               size=0.1) +   # Draw dashed lines
  labs(title="Variable Importance", y = "Importance", x = "Predictors") +  
  coord_flip()
```


```{r}
library("ggpubr")
ggarrange(fit1, fit2,
          labels = c("Permutation", "Impurity"))
```

**Boosted Regression Tree**
```{r, message=F}
library(gbm)
```

```{r, message=F}
set.seed(1234)

boost_tree_1 <- gbm(shares ~., data = df_train_sample, distribution = "gaussian", 
                    n.trees = 500, interaction.depth = 1, shrinkage = 0.005, bag.fraction = 1, cv.folds = 5)

boost_tree_2 <- gbm(shares ~., data = df_train_sample, distribution = "gaussian", 
                    n.trees = 500, interaction.depth = 2, shrinkage = 0.005, bag.fraction = 1, cv.folds = 5)
```

```{r}
# importance measure
boost_tree_1_importance <- data.frame(summary(boost_tree_1, plotit = FALSE))
boost_tree_1_importance %>% arrange(desc(boost_tree_1_importance$rel.inf)) %>% top_n(10)

boost_tree_2_importance <- data.frame(summary(boost_tree_2, plotit = FALSE))
boost_tree_2_importance %>% arrange(desc(boost_tree_2_importance$rel.inf)) %>% top_n(10)


# CV estimate of EPE at each step
plot(seq(1, 500), boost_tree_1$cv.error, type = "l")
plot(seq(1, 500), boost_tree_2$cv.error, type = "l")
```

```{r}
print(paste("boost_tree_1: ", boost_tree_1$cv.error[500]))
print(paste("boost_tree_2: ", boost_tree_2$cv.error[500]))

```


---
## Write Predict function

Since *Random Forest` has the best test set performance, we will use it as our team's Big Predictive Model

```{r}
q1_midterm1_predict <- function(train, test) {
  
  print("proprocessing the data...")
  
  # reshape the train set to fit the model
  train$shares = log(train$shares)
  test$shares = log(test$shares)
  
  # adding the `viral content category` variable
  train$data_channel_is_viral = train$data_channel_is_lifestyle
  for (i in 1:nrow(train)){
    if (train[i, "data_channel_is_lifestyle"] == 0 &
        train[i, "data_channel_is_entertainment"] == 0 &
        train[i, "data_channel_is_bus"] == 0 &
        train[i, "data_channel_is_socmed"] == 0 &
        train[i, "data_channel_is_tech"] == 0 &
        train[i, "data_channel_is_world"] == 0){
      
          train[i, "data_channel_is_viral"] = 1
          
    } else{
          train[i, "data_channel_is_viral"] = 0
        }
  }
  train$data_channel_is_viral <- as.integer(train$data_channel_is_viral)
  
  test$data_channel_is_viral = test$data_channel_is_lifestyle
  for (i in 1:nrow(test)){
    if (test[i, "data_channel_is_lifestyle"] == 0 &
        test[i, "data_channel_is_entertainment"] == 0 &
        test[i, "data_channel_is_bus"] == 0 &
        test[i, "data_channel_is_socmed"] == 0 &
        test[i, "data_channel_is_tech"] == 0 &
        test[i, "data_channel_is_world"] == 0){
      
          test[i, "data_channel_is_viral"] = 1
          
    } else{
          test[i, "data_channel_is_viral"] = 0
        }
  }
  test$data_channel_is_viral <- as.integer(test$data_channel_is_viral)
    
    # dropping variables
  train <- subset(train, select = -c(is_weekend, LDA_04, url))
  test <- subset(test, select = -c(is_weekend, LDA_04, url))
  
  # model training - Random Forest
  library(ranger)
  set.seed(1234)
  rf_fit <- ranger(shares ~ ., data = train, mtry = floor(sqrt(ncol(train) - 1)), num.trees = 500)
  
  pred = predict(rf_fit, test)
  return(pred$prediction)
  
}
```


## Testing
```{r}
df_train <- read.csv("midterm1_train.csv")
df_proto.test <- read.csv("midterm1_testProto.csv")
```

```{r}
pred <- q1_midterm1_predict(df_train, df_proto.test)
```

```{r}
pred
```

# Problem 2

After evaluating a total of nine models, random forest, not surprisingly, becomes the best model to predict article popularity with the least RMSE.

However, such selection comes with a cost of interpretability. We don't really understand the effect that each predictor have on the article popularity except knowing the variable importance of the predictor.

Here comes the trade-off, precision vs. interpretability. Depending on the goal of the assignment, we may want to pivot to one versus the other. Nevertheless, in this midterm analysis, our ultimate goal is to build a precious model with one / a subset of predictors.

In that sense, it is apposite to use random forest in the first part and evaluate the variable importance to make the subsequent predictor selection process a little bit easier. 

According the random forest predictor evaluation,`kw_max_avg` [Avg. keyword (max. shares)], `self_reference_avg_sharess` [Avg. shares of referenced articles in Mashable], `LDA_02` [Closeness to LDA topic 2] stand out to be some of the most significant predictors for determining the Mashable article's popularity. In addition, variable importance evaluated by other models (LASSO, KNN, Boosted Random Forest) more or less agree to this result. 

From a granular perspective, we can somewhat deduce that there are three major factors that determine a article's popularity in Mashable. 

1. Avg. Keyword of the article
2. Shares of referenced articles
3. The topic of the article

And we can certainly start to get into the detail of these three area if we would love to investigate further about factors that are affecting Mashable article's popularity.

# Problem 3

For this problem we need to select no more than 5 predictors to include in the model. For convenience, we will only consider the model with exactly 5 predictors and not explore models with less than 5 predictors. We will use LASSO regression and random forest to select a list of top 5 most important predictors.

In the previous part, we have a cross validation object for lasso model, however, for this problem we need the original list of models because we need to observe which predictors' coefficients are regularized to 0 as $\lambda$ increases.

First, we build the large model that contains all the predictors and get the coefficients
```{r}
lasso.fit <- glmnet(trn_x, trn_y, alpha = 1)
lasso.coef <- predict(lasso.fit, type = "coefficient")
lasso.coef <- lasso.coef %>% as.matrix %>% as.data.frame()
plot(lasso.fit, xvar = "lambda")
```

```{r}
head(lasso.fit$lambda)
```

Note that `glmnet` orders $\lambda$ in descending order. Also note that once a coefficient has been shrunk to, it will stay at 0. Therefore, to get the list of predictor importance, we simply count the number of 0 values for each predictor. The predictors that have less 0 values are the ones that have their coefficients shrunk to 0 last, and therefore are the important predictors that we are looking for.

```{r}
lasso.coef[lasso.coef==0]<- NA ## set all 0 to NAs
tlasso.coef<-t(lasso.coef) %>% as.data.frame()
lasso.importance <-colSums(is.na(tlasso.coef)) %>% as.data.frame()


names(lasso.importance)<-"count"
lasso.importance<-arrange(lasso.importance, count)
```

Now we get a list of top 5 most important predictors
```{r}
lasso.select <- rownames(lasso.importance)[2:6] #do not include `intercept`

lasso.select
```

Next, we get the list of top 5 most important predictors from our previous random forest models. Here, we want to consider both impurity-based importance and permutation-based importance because impurity-based importance is computed within the training data and may not be generalization for prediction, which is our objective.

```{r}
rf.perm.select <- rf_fit2_importance %>% arrange(desc(Importance)) %>% head(5) %>% rownames()
rf.impurity.select <- rf_fit3_importance %>% arrange(desc(Importance)) %>% head(5) %>% rownames()
```

We can get a list of unique predictors across these 3 lists and get all possible combinations of these predictors.

```{r}
all.select <- unique(c(lasso.select,rf.perm.select,rf.impurity.select))
all.select
```

```{r}
list.select<- combn(all.select, 5)
```

Now we run the models. We will pick the model that has best performance by roost mean test error.

**LASSO regression**

```{r}
lasso.errors <- data.frame(index = seq(1,ncol(list.select),1),
                           rmse = vector(length = ncol(list.select)))

for (i in 1:ncol(list.select)){
  trn_x <- df_train_sample[,list.select[,i]] %>% as.matrix()
  tst_x <- df_test_sample[,list.select[,i]] %>% as.matrix()
  
  cv.lasso.fit <- cv.glmnet(trn_x, trn_y, alpha = 1)
  lasso.prediction <- predict(cv.lasso.fit, newx = tst_x, s = cv.lasso.fit$lambda.1se) %>% as.vector()
  
  lasso.errors[i,"rmse"] <- mse(pred = lasso.prediction, actual = df_test_sample$shares) %>% sqrt()
}

lasso.errors %>% arrange(rmse) %>% head()
```

We see that with lasso model, model #20 has the least root MSE. Before, storing this as the final model, we should also try other models.

**ridge regression**

```{r}
ridge.errors <- data.frame(index = seq(1,ncol(list.select),1),
                           rmse = vector(length = ncol(list.select)))

for (i in 1:ncol(list.select)){
  trn_x <- df_train_sample[,list.select[,i]] %>% as.matrix()
  tst_x <- df_test_sample[,list.select[,i]] %>% as.matrix()
  
  cv.ridge.fit <- cv.glmnet(trn_x, trn_y, alpha = 0)
  ridge.prediction <- predict(cv.ridge.fit, newx = tst_x, s = cv.ridge.fit$lambda.1se) %>% as.vector()
  
  ridge.errors[i,"rmse"] <- mse(pred = ridge.prediction, actual = df_test_sample$shares) %>% sqrt()
}

ridge.errors %>% arrange(rmse) %>% head()
```

**GAM**

```{r}
gam.errors <- data.frame(index = seq(1,ncol(list.select),1),
                           rmse = vector(length = ncol(list.select)))

for (i in 1:ncol(list.select)){
  trn <- df_train_sample[,c(list.select[,i], "shares")]
  
  tst <- df_test_sample[,c(list.select[,i], "shares")]
  
  formula <- as.formula(paste0("shares","~ ", paste(list.select[,i], collapse=" + ")))
  gam.fit <- gam(formula, data = trn)
  
  gam.prediction <- predict(gam.fit, newdata = tst, type = "response") %>% as.data.frame()
  gam.prediction <- gam.prediction[,1]
  gam.errors[i,"rmse"] <- mse(pred = gam.prediction, actual = df_test_sample$shares) %>% sqrt()
}

gam.errors %>% arrange(rmse) %>% head()
```

**GAM with spines**

Here we need to be careful because some predictors are factors

```{r}
df_train_sample[,all.select] %>% str()
```
We can see that `data_channel_is_viral` and `data_channel_is_entertainment` so we need to write an if-else statement to avoid them when applying spline to other predictors

```{r}
gam.s.errors <- data.frame(index = seq(1,ncol(list.select),1),
                           rmse = vector(length = ncol(list.select)))

for (i in 1:ncol(list.select)){
  trn <- df_train_sample[,c(list.select[,i], "shares")]
  tst <- df_test_sample[,c(list.select[,i], "shares")]
  
  if("data_channel_is_viral" %in% list.select[,i]==T & 
     "data_channel_is_entertainment" %in% list.select[,i]==T){
    
        non.factors1 <- list.select[,i] %>% str_subset("data_channel_is_viral",negate = T) %>% 
                                            str_subset("data_channel_is_entertainment",negate = T)
      
        formula <- as.formula(paste0("shares","~ ", 
                                   paste(paste0("s(",non.factors1,")"), collapse=" + "),
                                   "+data_channel_is_viral+data_channel_is_entertainment" ))
      
    } else if("data_channel_is_viral" %in% list.select[,i]==T & 
              "data_channel_is_entertainment" %in% list.select[,i]==F) {
        non.factors2 <- list.select[,i] %>% str_subset("data_channel_is_viral",negate = T)
        formula <- as.formula(paste0("shares","~ ", 
                                     paste(paste0("s(",non.factors2,")"),collapse=" + "),
                                     "+data_channel_is_viral"))
    }
      else if("data_channel_is_viral" %in% list.select[,i]==F & 
              "data_channel_is_entertainment" %in% list.select[,i]==T) {
        non.factors3 <- list.select[,i] %>% str_subset("data_channel_is_entertainment",negate = T)
        formula <- as.formula(paste0("shares","~ ", 
                                     paste(paste0("s(",non.factors3,")"),collapse=" + "),
                                     "+data_channel_is_entertainment"))
      
  }  else{
        formula <- as.formula(paste0("shares","~ ", paste(paste0("s(",list.select[,i],")"), collapse=" + ")))
  }
  
  gam.s.fit <- gam(formula, data=trn)
  
  gam.s.prediction <- predict(gam.s.fit, newdata = tst, type = "response") %>% as.data.frame()
  gam.s.prediction <- gam.s.prediction[,1]
  gam.s.errors[i,"rmse"] <- mse(pred = gam.s.prediction, actual = df_test_sample$shares) %>% sqrt()
}

gam.s.errors %>% arrange(rmse) %>% head()
```

Now we compare across all the best performing models

```{r}
model.options <- data.frame(models = c("LASSO","Ridge","GAM","GAM w/splines"),
                           rmse = c(arrange(lasso.errors, rmse)[1,2],
                                    arrange(ridge.errors, rmse)[1,2],
                                    arrange(gam.errors, rmse)[1,2],
                                    arrange(gam.s.errors, rmse)[1,2]))
model.options %>% arrange(rmse)
```

Judging by this table, GAM model with splines produces the least root MSE.

Now we trace it back to get the model. From the **GAM with spines** section, we can see that the 23th combination of the predictors gives the least root MSE

```{r}
final.select <- list.select[,23]
final.select
```

Get the model and note that `data_channel_is_entertainment` is a factor

```{r}

final.nonfactors <- final.select %>% str_subset("data_channel_is_entertainment", negate = T)
final.formula <- as.formula(paste0("shares","~ ", 
                                     paste(paste0("s(",final.nonfactors,")"),collapse=" + "),
                                     "+data_channel_is_entertainment"))
final.model <- gam(final.formula, data = df_train_sample)
```

Write the function that takes in a test data and outputs prediction

```{r}
q3_midterm1_predict <- function(train, test){
  # reshape training and test data
  library(dplyr)
  library(stringr)
  
  train$shares = log(train$shares)
  test$shares = log(test$shares)
  
  train <- subset(train, select = -c(is_weekend, LDA_04, url))
  test <- subset(test, select = -c(is_weekend, LDA_04, url))
  
  # load the 5 features that we found to be most useful
  features <- c("kw_avg_avg", "LDA_02", "data_channel_is_entertainment", "self_reference_avg_sharess", "kw_min_avg")
  
  non.fac.features <- features %>% str_subset("data_channel_is_entertainment",negate = T)
  # construct model
  library(mgcv)
  formula <- as.formula(paste0("shares","~ ", 
                                paste(paste0("s(",non.fac.features,")"),collapse=" + "),
                                "+data_channel_is_entertainment"))
  gam.model <- gam(formula, data = train)
  
  #compute predictions
  prediction <- predict(gam.model, newdata = test, type = "response") %>% as.data.frame()
  return(prediction[,1])
}
```

Test the prediction function

```{r, message=F}
q3_midterm1_predict(df_train, df_proto.test)
```

# Problem 4

```{r}
summary(final.model)
```

```{r}
plot(final.model, pages = 1)
```

The summary shows that all 5 predictors are significant at 0.001 level. The model tells us that if an article is shared on the Entertainment channel, it may get less shares. Neither `LDA_02` nor `kw_min_avg` display significant partial effects because the line is almost constant at 0. There is a peak at about 6000 for `kw_avg_avg`. For `self_reference_avg_shares`, the effect falls negative after 600000, and there appears to be a peak at 200000.

With this model, we expect an article that is not share on the Entertainment channel, has average keyword per share at around 6000, and has average shares of referenced articles on Mashable at 200000 to have a higher number of shares. Therefore, the actionable plan for Mashable is to edit and publish articles that fit the above statistics.

The change in predictor surface is a reduction in dimension. This is potentially helpful for prediction purpose because with high dimensional data, if we do not select features, there is a high chance of overfitting. In fact, the final model, by using less but also important predictors, has a `GCV` that is less than the GAM model constructed in Problem has.

```{r}
round(sqrt(summary(gam1)$sp.criterion), 5)
round(sqrt(summary(final.model)$sp.criterion), 5)
```

Finally, the desired statistics mentioned above cannot be viewed as a robust causal inference. First, we are using only 5 predictors out of 59, so immediately we might be ommiting some predictors that has a causal relationship with the number of shares. Secondly, machine learning recognizes patterns in the data, which is useful for description and prediction purposes but is insufficient to establish causality. The model does not look into what explains article sharing behavior, has a large human factor. Mashable should also listen to user feedback to improve article sharing.















