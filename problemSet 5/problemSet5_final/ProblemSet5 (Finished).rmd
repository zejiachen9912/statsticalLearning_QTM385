
---
title: 'Problem Set #5'
author: "Harry Chen, Francis Lin, Mike Lin"
date: "March 11th, 2022"
output:
  prettydoc::html_pretty:
    df_print: kable
    highlight: github
    theme: leonids
    toc: no
    toc_depth: 2
    toc_float:
      collapsed: no
  html_document:
    df_print: paged
    toc: no
    toc_depth: '2'
urlcolor: blue
---

```{r, include=FALSE}
library(ggplot2)
library(data.table)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
```

This is the fifth problem set for QTM 385 - Intro to Statistical Learning.  This homework will cover applied exercises related to classification methods. 

Please use the intro to RMarkdown posted in the Intro module and my .Rmd file as a guide for writing up your answers.  You can use any language you want, but I think that a number of the computational problems are easier in R.  Please post any questions about the content of this problem set or RMarkdown questions to the corresponding discussion board.

Your final deliverable should be two files: 1) a .Rmd/.ipynb file and 2) either a rendered HTML file or a PDF.  Students can complete this assignment in groups of up to 3.  Please identify your collaborators at the top of your document.  All students should turn in a copy of the solutions, but your solutions can be identical to those of your collaborators.

This assignment is due by March 21st, 2022 at 11:59 PM EST.  

***

## Problem 1: The Multivariate Normal Distribution (20 pts.)

The multivariate normal distribution is an important distribution for the study of multivariate statistical models.  Specifically, the multivariate normal distribution is one of just a few ways to parametrically specify a data generating process that encodes the covariance between pairs of random variables.  A random vector $\boldsymbol x \in \mathbb{R}^P$ is said to follow a multivariate normal distribution if:

$$f(\boldsymbol x \mid \boldsymbol \mu , \Sigma) \sim \mathcal{N}_P(\boldsymbol x \mid \boldsymbol \mu , \Sigma) = (2 \pi)^{-\frac{P}{2}} \text{det}(\Sigma)^{-\frac{1}{2}} \exp \left[-\frac{1}{2} (\boldsymbol x - \boldsymbol \mu )' \Sigma^{-1} (\boldsymbol{x} - \boldsymbol \mu) \right]$$

Suppose we have $N$ iid draws from an unknown multivariate normal distribution.  We can specify the joint log-likelihood as:

$$\ell \ell (\boldsymbol x \mid \boldsymbol \mu , \Sigma) = -\frac{NP}{2} \log 2 \pi - \frac{N}{2} \log \text{det}(\Sigma) - \frac{1}{2} \sum \limits_{i = 1}^N (\boldsymbol x_i - \boldsymbol \mu )' \Sigma^{-1} (\boldsymbol{x}_i - \boldsymbol \mu)$$

Show that the maximum likelihood estimates of the parameters are:

$$\hat{\boldsymbol \mu} = \frac{1}{N} \sum \limits_{i = 1}^N \boldsymbol{x}_i$$

$$\hat{\Sigma} = \frac{1}{N} \sum \limits_{i = 1}^N (\boldsymbol x_i - \hat{\boldsymbol \mu})(\boldsymbol x_i - \hat{\boldsymbol \mu})'$$

Some hints:

1. Let $\gamma = y'Ay$ such that $\gamma$ evaluates to a scalar:
  
  $$\frac{\partial \gamma}{\partial y} = 2Ay$$
  
2. Let $\gamma = y'A^{-1}y$ such that $\gamma$ evaluates to a scalar:
  
  $$\frac{\partial \gamma}{\partial A^{-1}} = yy'$$
  
This arises because of a trace trick rearrangement, $\gamma = y'A^{-1}y = \text{tr}(yy'A^{-1})$
  
3. A well-known matrix identity is:
  
  $$\frac{\partial \log \text{det}(A)}{\partial A^{-1}} = A$$
  
when $A$ is symmetric.


### Solution

We would love to find $\hat{u}$ and $\hat{\Sigma}$ that maximize the given maximum likelihood function

$$\hat{u},\hat{\Sigma}=\text{argmax}(\ell \ell (\boldsymbol x \mid \boldsymbol \mu , \Sigma))$$

**Maximizing for ** $\hat{\mu}$

We first take the derivative w.r.t $\mu$ and set it to zero.

$$\frac{\partial\ell\ell}{\partial\mu}= - \frac{1}{2} \sum \limits_{i = 1}^N (\boldsymbol x_i - \boldsymbol \mu )' \Sigma^{-1} (\boldsymbol{x}_i - \boldsymbol \mu)=0$$

Let $\gamma = y'Ay$ such that $\gamma$ evaluates to a scalar:
  
  $$\frac{\partial \gamma}{\partial y} = 2Ay$$
Thus, we get

$$\frac{\partial\ell\ell}{\partial\hat{\mu}}=-\frac{1}{2}\sum2\Sigma^{-1}(x_i-\hat{\mu})=0$$
After some arrangement, we can have
$$-\frac{1}{2}\sum2\Sigma^{-1}(x_i-\hat{\mu}) = -\sum(x_i-\hat{\mu})=-\sum{x_i}+\sum\hat{\mu}=0$$
$$=-\sum x_i+n*\hat{\mu}=0$$
$$\hat{\mu}=\frac{1}{n}\sum x_i$$

**Maximizing for ** $\hat{\Sigma}$
 
We also first take the derivative w.r.t $\Sigma$ and set it to zero.
$$\frac{\partial\ell\ell}{\partial\hat{\Sigma}}= -\frac{N}{2}\log{det(\hat{\Sigma)}}-\frac{1}{2}\sum \limits_{i = 1}^N (\boldsymbol x_i - \boldsymbol \mu )' \hat{\Sigma}^{-1} (\boldsymbol{x}_i - \boldsymbol \mu)$$

To take the partial derivative w.r.t $\hat{\Sigma}$ for the second term, we need to take the trace of both $(\boldsymbol x_i - \boldsymbol \mu )'$ and $(\boldsymbol{x}_i - \boldsymbol \mu)$ to make them communitive

$$\frac{\partial\ell\ell}{\partial\hat{\Sigma}}=-\frac{1}{2}\sum \limits_{i = 1}^N tr(\boldsymbol x_i - \boldsymbol \mu )' \hat{\Sigma}^{-1} tr(\boldsymbol{x}_i - \boldsymbol \mu)\\ =-\frac{1}{2}\sum \limits_{i = 1}^N tr\Big((\boldsymbol x_i - \boldsymbol \mu )' (\boldsymbol{x}_i - \boldsymbol \mu)\hat{\Sigma}^{-1}\Big)$$

Then $\frac{\partial\ell\ell}{\partial\hat{\Sigma}}$ will be:

$$\frac{\partial\ell\ell}{\partial\hat{\Sigma}}= -\frac{N}{2}\log{det(\hat{\Sigma)}}-\frac{1}{2}\sum \limits_{i = 1}^N tr\Big((\boldsymbol x_i - \boldsymbol \mu ) (\boldsymbol{x}_i - \boldsymbol \mu)'\hat{\Sigma}^{-1}\Big)$$
where we will denote $(\boldsymbol x_i - \boldsymbol \mu ) (\boldsymbol{x}_i - \boldsymbol \mu)'$ as $A$

According to some matrix derivative identity, we know that

$$\frac{\partial\ det(A)}{\partial A}=det(A)\cdot A^{-'}\\ \frac{\partial\ tr(AB^{-1})}{\partial B}=-(B^{-1}AB^{-1})'$$

Apply the identity to our above expression

$$\frac{\partial\ell\ell}{\partial\hat{\Sigma}}= -\frac{N}{2}\frac{1}{det(\hat{\Sigma})}\cdot det(\hat{\Sigma})\cdot \hat{\Sigma}^{-'}-\frac{1}{2}\sum-(\hat{\Sigma}^{-1} A\hat{\Sigma}^{-1})'=0$$

Some simplification
$$-\frac{N}{2}\hat{\Sigma}^{-1}+\frac{1}{2}\sum\hat{\Sigma}^{-'}A'\hat{\Sigma}^{-'}=0$$
Substitute $A$ with $(\boldsymbol x_i - \boldsymbol \mu ) (\boldsymbol{x}_i - \boldsymbol \mu)'$

$$-N\hat{\Sigma}^{-1}+\sum\hat{\Sigma}^{-1}((\boldsymbol x_i - \boldsymbol \mu ) (\boldsymbol{x}_i - \boldsymbol \mu)')'\hat{\Sigma}^{-1}=0$$
Some more simplification, then we have
$$-N\hat{\Sigma}+\sum(\boldsymbol x_i - \boldsymbol \mu ) (\boldsymbol{x}_i - \boldsymbol \mu)'=0$$
$$\hat{\Sigma}=\frac{1}{N}\sum(\boldsymbol x_i - \boldsymbol \mu ) (\boldsymbol{x}_i - \boldsymbol \mu)'$$



## Problem 2: Cancer Data (40 Points)

In a number of data sets, the predictors are a collection of ordered categorical ratings of objects.  These predictors are then used to classify objects into categories.  In class, we discussed plausibly continuous predictors (like income) and unordered categorical predictors (like manufacturing country).  However, we did not spend time discussing ordered categorical predictors.  This problem will see you work with two ordered categorical predictors - rating of uniformity of cell size (`UCellSize`: 1-10 with 10 being most irregular) and single epithelial cell size (`SECellSize`: 1-10 with 10 being largest) - trying to predict whether or not a cell is cancerous (`Malignant` is one if cancerous, 0 otherwise).

The problem is that there is no in-between: we must either treat the predictor as an unordered outcome and lose any information that comes from seeing how the predictor increases or decreases while preserving its discreteness **or** treat the predictor as continuous preserving any ordering but losing its discrete nature.  Each choice comes with some downside, so we'll see how each one works on the training data set and use it to assess predictive accuracy on the test data set.

### Part 1 (15 points)

If we choose to treat each predictor as a discrete input, then we can use a generative classifier built via Bayes' theorem to create predictions that preserve dependencies between the predictors.  For discrete predictors $X$ and $Z$, the classifier can be built using the following formula:

$$P(y = 1 | X = x , Z = z) = \frac{P(X = x,Z = z | y = 1)P(y = 1)}{P(X = x,Z = z | y = 1)P(y = 1) + P(X = x,Z = z | y = 0)P(y = 0)}$$

Using the training data, create a lookup table that encodes the probability that an observation with $X = x$ **and** $Z = z$ is malignant.  This should be a $10 \times 10$ table where each element corresponds to a possible $\{x,z\}$ pair.  For some elements of this table, there are zero elements in the training set!  These elements should be recorded as missing since we can't compute a probability using this approach.

Along with the probability lookup table, create a corresponding table that encodes the **Bayes' classifier** - for a given $\{x,z\}$ pair, which class has the highest probability of occurrence?

What is the general relationship between these predictors and the probability that a cell is cancerous?  Does it appear that we've missed some information by treating this problem in an unordered discrete fashion?

### Solution

```{r}
df_train<-read.csv("cancerTrain.csv")
df_test<-read.csv("cancerTest.csv")
```

Compute posterior given predictors x and z. (UCellSize->x, SECellSize->z)
```{r}
bayes <- function(x,z){
  # prior
  p1=nrow(df_train[df_train$Malignant==1,]) / nrow(df_train)
  # P(y=0)
  p0=1-p1
  # P(x,y|y=1)
  n=nrow(df_train[df_train$Malignant==1&df_train$UCellSize==x&df_train$SECellSize==z,])
  d=nrow(df_train[df_train$Malignant==1,])
  p_1=n/d
  # P(x,y|y=0)
  n=nrow(df_train[df_train$Malignant==0&df_train$UCellSize==x&df_train$SECellSize==z,])
  d=nrow(df_train[df_train$Malignant==0,])
  p_0=n/d
  # posterior
  p = p_1*p1/(p_1*p1+p_0*p0)
  
}
```

probability lookup table
```{r}
table_1_m1 = matrix(, nrow = 10, ncol = 10)
for (x in 1:10){
  for (z in 1:10){
    table_1_m1[x,z] = bayes(x,z)
  }
}
colnames(table_1_m1)<-c("SECellSize=1",2,3,4,5,6,7,8,9,10)
rownames(table_1_m1)<-c("UCellSize=1",2,3,4,5,6,7,8,9,10)
print(table_1_m1)
```

Bayes lookup table
```{r}
table_2_m1=matrix(, nrow = 10, ncol = 10)
for (x in 1:10){
  for (z in 1:10){
    if ((!is.nan(table_1_m1[x,z]))&table_1_m1[x,z]>0.5){
      table_2_m1[x,z]="cancer"
    } else if (!is.nan(table_1_m1[x,z])){
      table_2_m1[x,z]='no cancer'
    }
  }
}
colnames(table_2_m1)<-c("SECellSize=1",2,3,4,5,6,7,8,9,10)
rownames(table_2_m1)<-c("UCellSize=1",2,3,4,5,6,7,8,9,10)
print(table_2_m1)
```


Generally, as the values of SECellSize or UCellSize go up, we expect higher chance of cancer occurrence. By treating the predictors as discrete, we miss some information. When a given {x,z} pair is missing in training data, we cannot predict the outcome given this value pair. While in reality, we might expect the outcome given a predictor pair to be closed to the outcome when we have similar predictors (for example the outcome for {5,5} should probably be similar to the outcome for {5,6}). By treating predictors as discrete, we lose the ordering nature of predictor.

### Part 2 (15 points)

Now, build classifiers that treat the two predictors as continuous (and ordered, in turn).  Specifically, use the training data to train 1) a logistic regression classifier and 2) a QDA classifier.  Using these two classifiers, create probability and Bayes' classifier tables for each training method for each possible combination of $x$ and $z$.  

How do these tables differ from the ones computed in part 1?  Have we lost anything by treating the predictors as continuous when they are truly discrete?

### Solution

Fit a logistic regression.
```{r}
log.fit<-glm(Malignant~UCellSize+SECellSize,data=df_train,family=binomial)
```

Make prediction and the tables.
```{r}
library(tidyr)
# generate combinations
a=1:10
b=1:10
comb<-crossing(a,b)
pred<-data.frame(matrix(ncol = 0, nrow = 100))
pred$UCellSize<-comb$a
pred$SECellSize<-comb$b
# make prediction with combinations of predictor values
log.probs<-predict(log.fit,newdata=pred,type = "response")
```

Create tables for logistic regression.
```{r}
pred$prediction=log.probs
table_1_m2 = matrix(, nrow = 10, ncol = 10)
for (x in 1:10){
  for (z in 1:10){
    table_1_m2[x,z] = pred[pred$UCellSize==x & pred$SECellSize==z,]$prediction
  }
}
colnames(table_1_m2)<-c("SECellSize=1",2,3,4,5,6,7,8,9,10)
rownames(table_1_m2)<-c("UCellSize=1",2,3,4,5,6,7,8,9,10)
print("probability table")
print(table_1_m2)

# Bayes lookup table
table_2_m2=matrix(, nrow = 10, ncol = 10)
for (x in 1:10){
  for (z in 1:10){
    if ((!is.nan(table_1_m2[x,z]))&table_1_m2[x,z]>0.5){
      table_2_m2[x,z]="cancer"
    } else if (!is.nan(table_1_m2[x,z])){
      table_2_m2[x,z]='no cancer'
    }
  }
}
colnames(table_2_m2)<-c("SECellSize=1",2,3,4,5,6,7,8,9,10)
rownames(table_2_m2)<-c("UCellSize=1",2,3,4,5,6,7,8,9,10)
print(table_2_m2)
```

QDA classifier
```{r}
library(MASS)
qda.fit<-qda(Malignant~UCellSize+SECellSize,data=df_train)
```

Make prediction.
```{r}
library(tidyr)
# generate combinations
a=1:10
b=1:10
comb<-crossing(a,b)
pred<-data.frame(matrix(ncol = 0, nrow = 100))
pred$UCellSize<-comb$a
pred$SECellSize<-comb$b
# make prediction with combinations of predictor values
qda.probs<-predict(qda.fit,newdata=pred,type = "response")
```

Create tables for QDA classifier.
```{r}
pred$posterior=qda.probs$posterior[,2]
table_1_m3 = matrix(, nrow = 10, ncol = 10)
for (x in 1:10){
  for (z in 1:10){
    table_1_m3[x,z] = pred[pred$UCellSize==x & pred$SECellSize==z,]$posterior[1]
  }
}
colnames(table_1_m3)<-c("SECellSize=1",2,3,4,5,6,7,8,9,10)
rownames(table_1_m3)<-c("UCellSize=1",2,3,4,5,6,7,8,9,10)
print("probability table")
print(table_1_m3)

# Bayes lookup table
table_2_m3=matrix(, nrow = 10, ncol = 10)
for (x in 1:10){
  for (z in 1:10){
    if ((!is.nan(table_1_m3[x,z]))&table_1_m3[x,z]>0.5){
      table_2_m3[x,z]="cancer"
    } else if (!is.nan(table_1_m3[x,z])){
      table_2_m3[x,z]='no cancer'
    }
  }
}
colnames(table_2_m3)<-c("SECellSize=1",2,3,4,5,6,7,8,9,10)
rownames(table_2_m3)<-c("UCellSize=1",2,3,4,5,6,7,8,9,10)
print(table_2_m3)
```


The most salient difference between tables in part two and those in previous part is that we can now fill every cell in the lookup tables. By treating predictors as continuous, we can predict the outcome given predictor values that we have not seen in training dataset. In QDA classifier, we assume the dirtribution of predictor is Gaussian. However, this assumption could be wrong, especially for predictors that take only ten discrete values. By making wrong assumption about data distribution, the classifier is high in bias.

### Part 3 (10 points)

Now, let's use the three models to create predictions for the test set and compare the results to the true class.  For each observation in the test set, compute the probability that the cell is malignant using each of the three tables computed in parts 1 and 2.  Using these values, compute the average probability of incorrect classification and the proportion of observations incorrectly classified under the Bayes' classifier.  Which method performs best?  Worst?  Discuss which loss metric we might want to favor in this situation - think about the context of the classifier and how the predictions would likely be used.

Under what conditions might we expect the unordered discrete model to perform better than the continuous predictor one?  Under what conditions might we expect the opposite to hold?

Note: There is one big weakness of the discrete Bayes' theorem approach.  Briefly discuss this weakness and then skip any affected observations when computing the average loss.


### Solution

There are 6 tables computed in previous steps: 
discrete Bayes classifier: table_1_m1, table_2_m1; 
logistic regression: table_1_m2, table_2_m2;
QDA classifier: table_1_m3, table_2_m3.

```{r}
df_test_pred = df_test
m1_probs=c() # model 1 probability
m1_preds=c() # model 1 prediction
m2_probs=c() # model 2 probability
m2_preds=c() # model 2 prediction
m3_probs=c() # model 3 probability
m3_preds=c() # model 3 prediction

for (row in 1:nrow(df_test)){
  x=df_test[row,]$UCellSize
  z=df_test[row,]$SECellSize
  # predictions
  m1_prob=table_1_m1[x,z] # model 1 probability
  m1_pred=table_2_m1[x,z] # model 1 prediction
  m2_prob=table_1_m2[x,z] # model 2 probability
  m2_pred=table_2_m2[x,z] # model 2 prediction
  m3_prob=table_1_m3[x,z] # model 3 probability
  m3_pred=table_2_m3[x,z] # model 3 prediction
  
  m1_probs=c(m1_probs,m1_prob) # model 1 probability
  m1_preds=c(m1_preds,m1_pred) # model 1 prediction
  m2_probs=c(m2_probs,m2_prob) # model 2 probability
  m2_preds=c(m2_preds,m2_pred) # model 2 prediction
  m3_probs=c(m3_probs,m3_prob) # model 3 probability
  m3_preds=c(m3_preds,m3_pred) # model 3 prediction
}


  df_test_pred$m1_prob=m1_probs
  df_test_pred$m1_pred=m1_preds
  df_test_pred$m2_prob=m2_probs
  df_test_pred$m2_pred=m2_preds
  df_test_pred$m3_prob=m3_probs
  df_test_pred$m3_pred=m3_preds
  
```

Display the predictions. m1_prob, m2_prob and m3_prob are predicted probabilities for discrete, logistic regression and QDA, respectively. m1_pred, m2_pred, m3_pred are predicted classes.
```{r}
df_test_pred
```

Compute the proportion of observations incorrectly classified under the Bayes' classifier
```{r}
n1=0 # number of correct prediction for model 1
n2=0 # number of correct prediction for model 2
n3=0 # number of correct prediction for model 3
for (row in 1:nrow(df_test)){
  # model1 discrete
  if (!is.nan(df_test_pred[row,]$m1_prob)){
  if (df_test_pred[row,]$m1_pred=='cancer' & df_test_pred[row,]$Malignant==1 ){
    n1 = n1 + 1
  } else if (df_test_pred[row,]$m1_pred=='no cancer' & df_test_pred[row,]$Malignant==0 ){
    n1 = n1 + 1
  }
  }
  # model2 logistic regression
  if (df_test_pred[row,]$m2_pred=='cancer' & df_test_pred[row,]$Malignant==1 ){
    n2 = n2 + 1
  } else if (df_test_pred[row,]$m2_pred=='no cancer' & df_test_pred[row,]$Malignant==0 ){
    n2 = n2 + 1
  }
  # model3 QDA
  if (df_test_pred[row,]$m3_pred=='cancer' & df_test_pred[row,]$Malignant==1 ){
    n3 = n3 + 1
  } else if (df_test_pred[row,]$m3_pred=='no cancer' & df_test_pred[row,]$Malignant==0 ){
    n3 = n3 + 1
  }
}
accuracy1=n1/(nrow(df_test)-sum(is.nan(df_test_pred$m1_prob)))
accuracy2=n2/nrow(df_test)
accuracy3=n3/nrow(df_test)

print("discrete Bayes classifier, logistic, QDA")
1-accuracy1
1-accuracy2
1-accuracy3
```


Compute the average probability of incorrect classification.

```{r}
print("average probability of incorrect classification: discrete, logistic, QDA")
sum(abs(df_test_pred$Malignant-df_test_pred$m1_prob),na.rm=TRUE)/(nrow(df_test)-sum(is.na(df_test_pred$m1_prob)))
sum(abs(df_test_pred$Malignant-df_test_pred$m2_prob))/nrow(df_test)
sum(abs(df_test_pred$Malignant-df_test_pred$m3_prob))/nrow(df_test)
```

The discrete Bayes classifier has highest proportion of observations incorrectly classified under the Bayes' classifier, and the other two are the same. Discrete method is the worst and the other two are the same, in terms of proportion of observations incorrectly classified. Logistic regression performs worst in terms of average probability of incorrect classification, and the other two are similar (QDA is a bit better than discrete method).

Since our classifier would be used to identify cancers, we would like to make sure our model identifies most, if not all, positive cases. So, we use recall as our evaluation metric.

```{r}
n1=0 # number of correct prediction for model 1
n2=0 # number of correct prediction for model 2
n3=0 # number of correct prediction for model 3
for (row in 1:nrow(df_test)){
  # model1 discrete
  if (!is.nan(df_test_pred[row,]$m1_prob)){
  if (df_test_pred[row,]$m1_pred=='cancer' & df_test_pred[row,]$Malignant==1 ){
    n1 = n1 + 1
  } 
  }
  # model2 logistic regression
  if (df_test_pred[row,]$m2_pred=='cancer' & df_test_pred[row,]$Malignant==1 ){
    n2 = n2 + 1
  } 
  # model3 QDA
  if (df_test_pred[row,]$m3_pred=='cancer' & df_test_pred[row,]$Malignant==1 ){
    n3 = n3 + 1
  } 
}

recall1=n1/(nrow(df_test[df_test$Malignant==1,])-sum(is.nan(df_test_pred[df_test$Malignant==1,]$m1_prob)))
recall2=n2/nrow(df_test[df_test$Malignant==1,])
recall3=n3/nrow(df_test[df_test$Malignant==1,])

print("recalls for discrete Bayes classifier, logistic, QDA, respectively")
recall1
recall2
recall3
```

```{r}
print("accuracy: discrete Bayes classifier, logistic, QDA")
accuracy1
accuracy2
accuracy3
```

The discrete classifier has highest recall, and it successfully identifies all cancer cases. Thus, if we only care about overall accuracy, we should use either logistic regression or QDA classifier, because they perform better in terms of accuracy. If we are most concerned with identifying positive cases, we should use discrete Bayes method, because it performs better in terms of recall.

## Problem 3: Wine Data (40 points)

The Wine data set is a classic prototyping data set for classification methods.  The data set revolves around a 13 different measurements of the chemical properties of different wines that originate from three different *cultivars* (varieties of plants - in this case, grapes - that have been produced by selective breeding).  The goal of the classification task is to create a classifier for the three different *cultivars* using only the chemical properties.

The Wine data set only has 178 observations, so it is too small to split into training and test splits.  Therefore, cross-validation methods are needed to approximate expected prediction error.

### Part 1 (15 points)

Let's start with only 2 predictors: Color and OD280.  Start by creating a plot that shows the predictor values in the training data colored by their class.  Can you see approximately where the decision boundaries should be?

Using the training data, train a multinomial logistic regression classifier, a QDA classifier, and 2 Naive Bayes classifiers - one assuming normal conditional marginals and one using KDEs for the conditional marginals.  For each classifier, produce a plot that shows the Bayes' classifier as a function of Color and OD280 for the **minimum bounding box** implied by the predictors (which is just a fancy way of saying predict the class for many combinations of predictors within the minimum and maximum of each predictor).  How does the **decision boundary** differ between the 4 classifiers?  Which one appears to do the best at capturing the true decision boundary within the data?  Do any of the methods seem to overfit to the training data?

Hint: `Color` ranges from approximately 1 to 13 and `OD280` ranges from approximately 1 to 4.  There are a lot of approaches to creating the decision plots, but I think that doing a grid evaluation is easiest.

```{r}
wine.train <- read.csv("wineTrain.csv")
str(wine.train)
```

```{r}
wine.train2 <- wine.train[,c("Color","OD280","Class")]
p1 <- ggplot(wine.train2, aes(x=Color, y=OD280, color=as.factor(Class)))+geom_point()
p1
```

Firstly, the decision boundary between class 2 and 3 is clearer than that between class 1 and 2. In general, the variant with `Color` above 3.75 and `OD280` below 2.5 should be classified as class 3. Class 1 and 2 are less separable. The decision boundary between class 1 and 2 should be a downward line with its slope at about -1/2.5.

**Multinomial Logit model**

```{r}
library(nnet)

log.model <- multinom(Class ~ Color+OD280, data = wine.train2)

# create a grid to make prediction

r <- sapply(wine.train2, range, na.rm = TRUE)

xs <- seq(r[1,1], r[2,1], length.out = 100)
ys <- seq(r[1,2], r[2,2], length.out = 100)

g <- cbind(rep(xs, each=100), rep(ys, time = 100))
colnames(g) <- colnames(r[,1:2])

# make prediction with multinomial logistic regression
library(nnet)
log.model <- multinom(Class ~ Color+OD280, data = wine.train2)
logit.pred <- predict(log.model, g, type = "class")

z.logit <- matrix(as.integer(logit.pred), nrow = 100, byrow = TRUE)

# plot the boundary
contour(x=xs, y=ys, z=z.logit,
        col="grey", xlab="Color",ylab="OD280",main="Logit Model Decision Boundary",drawlabels=FALSE, lwd=1)
points(wine.train2, col=wine.train2$Class)

```

**QDA**

```{r}
# make predictions with QDA model
library(MASS)

qda.model <- qda(Class ~ Color + OD280, data = wine.train2)

qda.pred <- predict(qda.model, newdata = as.data.frame(g))$class
z.qda <- matrix(as.integer(qda.pred), nrow = 100, byrow = TRUE)

# plot decision boundary
contour(x=xs, y=ys, z=z.qda,
        col="grey", xlab="Color",ylab="OD280",main="QDA Decision Boundary",drawlabels=FALSE, lwd=1)
points(wine.train2, col=wine.train2$Class)
```

**Naive Bayes Gaussian**

```{r, message=F, warning=F}
library(naivebayes)
nb_x <- wine.train2[,-3]
nb_y <- as.character(wine.train2$Class)

# make predictions using Naive Bayes assuming normal conditional marginals

nb.gauss <- naive_bayes(x = nb_x, y = nb_y, usekernel = FALSE)

nb.gauss.pred <- predict(nb.gauss, newdata = g)

z.nb.gauss <- matrix(as.integer(nb.gauss.pred), nrow = 100, byrow = TRUE)

# plot decision boundary

contour(x=xs, y=ys, z=z.nb.gauss,
        col="grey", xlab="Color",ylab="OD280",main="NB Gaussian Decision Boundary",drawlabels=FALSE, lwd=1)
points(wine.train2, col=wine.train2$Class)

```

**Naive Bayes with Kernel Density Estimation**

```{r}
# make predictions using Naive Bayes with Kernel Density Estimation

nb.kde <- naive_bayes(x = nb_x, y = nb_y, usekernel = TRUE)

nb.kde.pred <- predict(nb.kde, newdata = g)

z.nb.kde <- matrix(as.integer(nb.kde.pred), nrow = 100, byrow = TRUE)

# plot decision boundary

contour(x=xs, y=ys, z=z.nb.kde,
        col="grey", xlab="Color",ylab="OD280",main="NB Gaussian Decision Boundary",drawlabels=FALSE, lwd=1)
points(wine.train2, col=wine.train2$Class)
```
How does the **decision boundary** differ between the 4 classifiers?  Which one appears to do the best at capturing the true decision boundary within the data?  Do any of the methods seem to overfit to the training data?

Multinomial Logistic Regression has linear decision boundaries, while the rest have nonlinear decision boundaries. The decision boundaries of the QDA model looks very much like that of Naive Bayes model with normal conditional marginals. Naive Bayes model with Kernel Density Estimation as marginals has even finer decision boundaries than QDA and Gaussian Naive Bayes. NB model with KDE also has a peculiar segment on the top right, although with my flawed drawing, I cannot see which class is in that region.

QDA and Gaussian Naive Bayes seem to capture the true decision boundary within the data the most while NB model with KDE seems to be overfitting.


### Part 2 (15 points)

Now, let's work with all 13 predictors.  The goal of this exercise is to build a model that best predicts out of sample wines, so use 5-fold cross validation to compute an estimate of the expected prediction error for three different classifiers - multinomial logistic regression, QDA, and naive Bayes (make a choice for the marginals given your observations in part 1).  For each classifier, compute the 5-fold estimate of the average probability of incorrect classification **and** the misclassification rate with respect to the Bayes' classifier.

Which model performs best?  Worst?

### Solution

Here I am choosing Naive Bayes with normal marginals because the NB with KDE has strange decision boundaries at top right

```{r}
log.model.all <- multinom(Class ~ . , data = wine.train)
qda.model.all <- qda(Class ~ . , data = wine.train)

nb_x_all <- wine.train[,-1]
nb.kde.all <- naive_bayes(x = nb_x_all, y = nb_y, usekernel = TRUE)


```

Split data in 5 equal sizes. (178 is not divisible by 5 so I minimized the difference in sizes)
```{r}
data1 <- wine.train[sample(nrow(wine.train), 36), ]
id.1 <- rownames(data1) %>% as.numeric
data.r1 <- wine.train[-id.1,]

data2 <- data.r1[sample(nrow(data.r1), 36), ]
id.2 <- rownames(data2) %>% as.numeric
data.r2 <- wine.train[-c(id.1, id.2),]

data3 <- data.r2[sample(nrow(data.r2), 36), ]
id.3 <- rownames(data3) %>% as.numeric
data.r3 <- wine.train[-c(id.1, id.2, id.3),]

data4 <- data.r3[sample(nrow(data.r3), 36), ]
id.4 <- rownames(data4) %>% as.numeric
data.r4 <- wine.train[-c(id.1, id.2, id.3, id.4),]

data5 <- data.r4

splits <- list(data1, data2, data3, data4, data5)
```

Compute 5-fold cross validation for logit model

```{r}
library(dplyr)
logit.errors <- vector(length = 5)
comb <- combn(1:5,4)
for(i in 1:5){
  ind <- as.numeric(comb[,i])
  newsplits <- splits[ind]
  train <- rbind(newsplits[[1]],newsplits[[2]],newsplits[[3]],newsplits[[4]])
  test <- splits[-ind] %>% as.data.frame()
  cv.log.model <- multinom(Class ~ . , data = train)
  cv.log.pred <- predict(cv.log.model, test, type = "class")
  logit.errors[i] <- mean(cv.log.pred!=test$Class)
}

mean(logit.errors)

```

Compute 5-fold cross validation for QDA model

```{r}
QDA.errors <- vector(length = 5)
for(i in 1:5){
  ind <- as.numeric(comb[,i])
  newsplits <- splits[ind]
  train <- rbind(newsplits[[1]],newsplits[[2]],newsplits[[3]],newsplits[[4]])
  test <- splits[-ind] %>% as.data.frame()
  cv.QDA.model <- qda(Class ~ . , data = train)
  cv.QDA.pred <- predict(cv.QDA.model, test)$class
  QDA.errors[i] <- mean(cv.QDA.pred!=test$Class)
}

mean(QDA.errors)
```

```{r}
NB.errors <- vector(length = 5)
for(i in 1:5){
  ind <- as.numeric(comb[,i])
  newsplits <- splits[ind]
  train <- rbind(newsplits[[1]],newsplits[[2]],newsplits[[3]],newsplits[[4]])
  test <- splits[-ind] %>% as.data.frame()
  train_x <- select(train, -Class)
  train_y <- as.character(train$Class)
  test_x <- select(test, -Class)
  
  cv.NB.model <- naive_bayes(train_x, train_y, usekernel = FALSE)
  cv.NB.pred <- predict(cv.NB.model, test_x)
  NB.errors[i] <- mean(cv.NB.pred!=test$Class)
}

mean(NB.errors)
```
It appears that QDA model performs best and logit model performs worst.


### Part 3 (10 points)

Broadly discuss situations where Naive Bayes is likely to outperform QDA.  Think about this from a loss perspective as well as a computational perspective.  Specifically, consider the MLE for the multivariate normal you derive above - when do you think this computation would become time and/or computer space prohibitive?

### Solution

Naive Bayes tends to outperform QDA when the true decision boundary isn't very clear, i.e. lots of noise. From a computational perspective, generative models are faster than QDA because they estimate less parameters. The MLE for the multivariate normal becomes computer space prohibitive when we need to estimate $\Sigma_k$ for each class $k=\{1,..,K\}$




